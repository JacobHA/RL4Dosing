{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_env import CellEnv\n",
    "# Use sb3 env checker:\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CellEnv()\n",
    "check_env(env)\n",
    "# use the monitor wrapper to log the results:\n",
    "env = Monitor(env)\n",
    "eval_env = CellEnv(dt=0.1)\n",
    "eval_env = Monitor(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./rl-logs/dqn_7\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 416      |\n",
      "|    exploration_rate | 0.937    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 130      |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0199   |\n",
      "|    n_updates        | 1498     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 951      |\n",
      "|    exploration_rate | 0.873    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 128      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0237   |\n",
      "|    n_updates        | 3498     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=2072.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.07e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.842    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 4498     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.01e+03 |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0369   |\n",
      "|    n_updates        | 5498     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.05e+03 |\n",
      "|    exploration_rate | 0.747    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 306      |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0556   |\n",
      "|    n_updates        | 7498     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1796.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.8e+03  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.0746   |\n",
      "|    n_updates        | 9498     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.55e+03 |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 38       |\n",
      "|    time_elapsed     | 519      |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.78e+03 |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 551      |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 11498    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.74e+03 |\n",
      "|    exploration_rate | 0.557    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 48       |\n",
      "|    time_elapsed     | 582      |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.325    |\n",
      "|    n_updates        | 13498    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=3230.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.23e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 14498    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.85e+03 |\n",
      "|    exploration_rate | 0.493    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 40       |\n",
      "|    time_elapsed     | 794      |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.536    |\n",
      "|    n_updates        | 15498    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 435      |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 826      |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 0.768    |\n",
      "|    n_updates        | 17498    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1748.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.75e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.367    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 19498    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 97.2     |\n",
      "|    exploration_rate | 0.367    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 38       |\n",
      "|    time_elapsed     | 1038     |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -378     |\n",
      "|    exploration_rate | 0.303    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 41       |\n",
      "|    time_elapsed     | 1070     |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 21498    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -979     |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 43       |\n",
      "|    time_elapsed     | 1101     |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 1.9      |\n",
      "|    n_updates        | 23498    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=2152.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.15e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.208    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 2.09     |\n",
      "|    n_updates        | 24498    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1e+03     |\n",
      "|    ep_rew_mean      | -1.19e+03 |\n",
      "|    exploration_rate | 0.177     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 39        |\n",
      "|    time_elapsed     | 1313      |\n",
      "|    total_timesteps  | 52000     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0015    |\n",
      "|    loss             | 2.58      |\n",
      "|    n_updates        | 25498     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.2e+03 |\n",
      "|    exploration_rate | 0.113    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 41       |\n",
      "|    time_elapsed     | 1344     |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0015   |\n",
      "|    loss             | 3.49     |\n",
      "|    n_updates        | 27498    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models-sde/',\n",
    "                             n_eval_episodes=25,\n",
    "                             log_path='./rl-logs/', eval_freq=10_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = DQN(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            exploration_fraction=0.2,\n",
    "            target_update_interval=2000,\n",
    "            buffer_size=10_000,\n",
    "            gradient_steps=2,\n",
    "            learning_starts=1000,\n",
    "            learning_rate=0.0015,\n",
    "            batch_size=16,\n",
    ")\n",
    "model.learn(total_timesteps=300_000, tb_log_name=\"dqn\",\n",
    "            callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(env_class, max_steps, num_episodes, model=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model over several episodes and plot the results.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained RL model to be evaluated.\n",
    "    - env_class: Environment class to create new instances of the evaluation environment.\n",
    "    - max_steps: Maximum number of steps per episode.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - avg_observations: Average observations at each step.\n",
    "    - all_observations: List of observations for all episodes.\n",
    "    \"\"\"\n",
    "    all_observations = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        eval_env = TimeLimit(env_class(), max_steps)\n",
    "        done = False\n",
    "        obs, _ = eval_env.reset()\n",
    "        episode_observations = []\n",
    "        while not done:\n",
    "            if model is not None:\n",
    "                action, _states = model.predict(obs)\n",
    "            else:\n",
    "                action = eval_env.action_space.sample()\n",
    "            obs, rewards, term, trunc, info = eval_env.step(action)\n",
    "            done = term or trunc\n",
    "            episode_observations.append(info['n_cells'])\n",
    "        \n",
    "        all_observations.append(episode_observations)\n",
    "    \n",
    "    # Compute the average observations\n",
    "    max_len = max(len(obs) for obs in all_observations)\n",
    "    avg_observations = np.zeros(max_len)\n",
    "    counts = np.zeros(max_len)\n",
    "    \n",
    "    for obs in all_observations:\n",
    "        for i, val in enumerate(obs):\n",
    "            avg_observations[i] += val[-1]\n",
    "            counts[i] += 1\n",
    "    \n",
    "    avg_observations /= counts\n",
    "\n",
    "    return avg_observations, all_observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_observations(avg_observations, all_observations, unif_obs=None, all_unif_obs=None):\n",
    "    \"\"\"\n",
    "    Plot the average observations and individual episode tracks.\n",
    "\n",
    "    Parameters:\n",
    "    - avg_observations: Average observations at each step.\n",
    "    - all_observations: List of observations for all episodes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot individual episode tracks with lower alpha\n",
    "    for obs in all_observations:\n",
    "        plt.plot(x_axis, obs, alpha=0.05, linewidth=0.5, color='black')\n",
    "\n",
    "    if unif_obs is not None:\n",
    "        for obs in all_unif_obs:\n",
    "            plt.plot(x_axis, obs, alpha=0.05, linewidth=0.5, color='red')\n",
    "    \n",
    "    # Plot average observations\n",
    "    plt.plot(x_axis, avg_observations, label='trained policy', linewidth=3, color='black')\n",
    "    if unif_obs is not None:\n",
    "        plt.plot(x_axis, unif_obs, label='random policy', linewidth=3, color='red')\n",
    "    \n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('Total cells (n_cells)')\n",
    "    plt.title('Model Evaluation: Average Observations and Individual Episode Tracks')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "avg_obs, all_obs = evaluate_model(CellEnv, 1200, 10, model)\n",
    "unif_obs, unif_all_obs = evaluate_model(CellEnv, 1200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_observations\u001b[49m(avg_obs, all_obs, unif_obs, unif_all_obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_observations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_observations(avg_obs, all_obs, unif_obs, unif_all_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
