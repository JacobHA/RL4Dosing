{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_env import CellEnv\n",
    "# Use sb3 env checker:\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.9\n",
    "env_args = {\n",
    "    \"max_timesteps\": 10000,\n",
    "    \"alpha_mem\": alpha,\n",
    "    \"dt\": 0.01,\n",
    "    \"frame_stack\": 5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CellEnv(**env_args)\n",
    "check_env(env)\n",
    "# use the monitor wrapper to log the results:\n",
    "env = Monitor(env)\n",
    "eval_env = CellEnv(**env_args)\n",
    "eval_env = Monitor(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./rl-logs/dqn_58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1        |\n",
      "|    ep_rew_mean      | -0.328   |\n",
      "|    exploration_rate | 1        |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 788      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.75     |\n",
      "|    ep_rew_mean      | -0.328   |\n",
      "|    exploration_rate | 0.999    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1665     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 14       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | -0.309   |\n",
      "|    exploration_rate | 0.992    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 3169     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 163      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.4     |\n",
      "|    ep_rew_mean      | -0.316   |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 3242     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 183      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.5      |\n",
      "|    ep_rew_mean      | -0.307   |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 3194     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 190      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.17     |\n",
      "|    ep_rew_mean      | -0.31    |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 3165     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 196      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.18     |\n",
      "|    ep_rew_mean      | -0.312   |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 3127     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 201      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.5      |\n",
      "|    ep_rew_mean      | -0.307   |\n",
      "|    exploration_rate | 0.99     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 3111     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 208      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.89     |\n",
      "|    ep_rew_mean      | -0.309   |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 3066     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 212      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.7      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.55     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 256      |\n",
      "|    ep_rew_mean      | 0.517    |\n",
      "|    exploration_rate | 0.488    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10257    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 233      |\n",
      "|    ep_rew_mean      | 0.44     |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10267    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 214      |\n",
      "|    ep_rew_mean      | 0.376    |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10276    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | 0.326    |\n",
      "|    exploration_rate | 0.486    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10287    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 184      |\n",
      "|    ep_rew_mean      | 0.284    |\n",
      "|    exploration_rate | 0.486    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10298    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 172      |\n",
      "|    ep_rew_mean      | 0.243    |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10302    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 161      |\n",
      "|    ep_rew_mean      | 0.207    |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10308    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 152      |\n",
      "|    ep_rew_mean      | 0.183    |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 10316    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.401    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.251    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.101    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 282      |\n",
      "|    ep_rew_mean      | 0.568    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20319    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 268      |\n",
      "|    ep_rew_mean      | 0.523    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20341    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 254      |\n",
      "|    ep_rew_mean      | 0.481    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20347    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 243      |\n",
      "|    ep_rew_mean      | 0.45     |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 232      |\n",
      "|    ep_rew_mean      | 0.415    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20397    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 222      |\n",
      "|    ep_rew_mean      | 0.385    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 66       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20412    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | 0.357    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20436    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 0.334    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20495    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 0.334    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20501    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 0.334    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20506    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | 0.336    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20517    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | 0.337    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 304      |\n",
      "|    total_timesteps  | 20522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | 0.336    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 20528    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | 0.339    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 20627    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 206      |\n",
      "|    ep_rew_mean      | 0.345    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 20834    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 206      |\n",
      "|    ep_rew_mean      | 0.345    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 20841    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 206      |\n",
      "|    ep_rew_mean      | 0.347    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 68       |\n",
      "|    time_elapsed     | 305      |\n",
      "|    total_timesteps  | 20859    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1        |\n",
      "|    mean_reward      | -0.328   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 217      |\n",
      "|    ep_rew_mean      | 0.0245   |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 70       |\n",
      "|    time_elapsed     | 451      |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0015   |\n",
      "|    n_updates        | 10301    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 838.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 838      |\n",
      "|    mean_reward      | -0.00583 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00141  |\n",
      "|    n_updates        | 11977    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 252      |\n",
      "|    ep_rew_mean      | 0.0354   |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 77       |\n",
      "|    time_elapsed     | 459      |\n",
      "|    total_timesteps  | 35421    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 13679    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 904.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 904      |\n",
      "|    mean_reward      | -0.0188  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00147  |\n",
      "|    n_updates        | 14560    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1046.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.05e+03 |\n",
      "|    mean_reward      | -0.0112  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00183  |\n",
      "|    n_updates        | 17418    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 290      |\n",
      "|    ep_rew_mean      | 0.0476   |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 83       |\n",
      "|    time_elapsed     | 472      |\n",
      "|    total_timesteps  | 39325    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-0.09 +/- 0.00\n",
      "Episode length: 1731.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.73e+03 |\n",
      "|    mean_reward      | -0.0873  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00263  |\n",
      "|    n_updates        | 20743    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 1901.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.9e+03  |\n",
      "|    mean_reward      | -0.0783  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00255  |\n",
      "|    n_updates        | 22474    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 349      |\n",
      "|    ep_rew_mean      | 0.0561   |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 502      |\n",
      "|    total_timesteps  | 45236    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9231.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.23e+03 |\n",
      "|    mean_reward      | -0.0241  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.003    |\n",
      "|    n_updates        | 26363    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9231.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.23e+03 |\n",
      "|    mean_reward      | -0.0241  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9231.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 9.23e+03 |\n",
      "|    mean_reward      | -0.0241  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 7450.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.45e+03 |\n",
      "|    mean_reward      | -0.0367  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00259  |\n",
      "|    n_updates        | 35522    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 7450.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.45e+03 |\n",
      "|    mean_reward      | -0.0367  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 7450.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 7.45e+03 |\n",
      "|    mean_reward      | -0.0367  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=18.41 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00296  |\n",
      "|    n_updates        | 43804    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=69000, episode_reward=18.41 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=18.41 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 18.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 644      |\n",
      "|    ep_rew_mean      | 0.249    |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 2042     |\n",
      "|    total_timesteps  | 74665    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=24.67 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00232  |\n",
      "|    n_updates        | 53804    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78000, episode_reward=24.67 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=24.67 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=24.67 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=54.24 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 54.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00302  |\n",
      "|    n_updates        | 64381    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=54.24 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 54.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=54.24 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 54.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0028   |\n",
      "|    n_updates        | 74381    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 949      |\n",
      "|    ep_rew_mean      | 1.12     |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 4233     |\n",
      "|    total_timesteps  | 105242   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=24.68 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00327  |\n",
      "|    n_updates        | 84381    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=24.68 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+04    |\n",
      "|    mean_reward      | 24.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.001    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111000   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl-models-sde_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                              n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl-logs/\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3_000\u001b[39m,\n\u001b[1;32m      4\u001b[0m                              deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                              )\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: env]), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./rl-logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m             exploration_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m             exploration_final_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m             gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.995\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_env.py:41\u001b[0m, in \u001b[0;36mCellEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m     res_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     t, tot, N, R, cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_population\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     res_fraction \u001b[38;5;241m=\u001b[39m R \u001b[38;5;241m/\u001b[39m tot\n\u001b[1;32m     43\u001b[0m     tot \u001b[38;5;241m=\u001b[39m tot[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# most recent cell count in entire simulation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:186\u001b[0m, in \u001b[0;36mCell_Population.simulate_population\u001b[0;34m(self, b, delta_t, plot)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_stop \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimulation call will exceed total time allotted and thus can not be accepted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m t, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFDE_PI12_PC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_stop\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# solving for values at next timestep using fractional integration\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# upacking\u001b[39;00m\n\u001b[1;32m    189\u001b[0m N \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:219\u001b[0m, in \u001b[0;36mCell_Population.FDE_PI12_PC\u001b[0;34m(self, T_stop)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFDE_PI12_PC\u001b[39m(\u001b[38;5;28mself\u001b[39m, T_stop):\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Main process of computation by means of the FFT algorithm\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil((T_stop \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh))\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTriangolo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzn_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzn_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProbl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeg \u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Evaluation solution in T when T is not in the mesh\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:245\u001b[0m, in \u001b[0;36mCell_Population.Triangolo\u001b[0;34m(self, nxi, nxf, t, y, fy, zn_pred, zn_corr, N, METH, Probl)\u001b[0m\n\u001b[1;32m    242\u001b[0m j_beg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j_beg, n):\n\u001b[0;32m--> 245\u001b[0m     Phi \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m METH[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m][:Probl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_length\u001b[39m\u001b[38;5;124m'\u001b[39m], n \u001b[38;5;241m-\u001b[39m j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mfy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    247\u001b[0m St \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mStartingTerm(t[n], Probl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# if no corrector application, add noise now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./rl-models-sde_{alpha}/',\n",
    "                             n_eval_episodes=1,\n",
    "                             log_path='./rl-logs/', eval_freq=3_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = DQN(\"MlpPolicy\", DummyVecEnv([lambda: env]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            exploration_fraction=0.05,\n",
    "            exploration_final_eps=0.001,\n",
    "            target_update_interval=20000,\n",
    "            buffer_size=100_000,\n",
    "            gradient_steps=-1,\n",
    "            train_freq=(1, \"episode\"),\n",
    "            learning_starts=30000,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=32,\n",
    "            gamma=0.995,\n",
    ")\n",
    "model.learn(total_timesteps=200_000, tb_log_name=\"dqn\",\n",
    "            callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
