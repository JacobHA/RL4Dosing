{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_env import CellEnv\n",
    "# Use sb3 env checker:\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = CellEnv(dt=0.05)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "# eval wrapper:\n",
    "env = TimeLimit(env, 3000)\n",
    "# use the monitor wrapper to log the results:\n",
    "env = Monitor(env)\n",
    "eval_env = TimeLimit(CellEnv(dt=0.05), 3000)\n",
    "eval_env = Monitor(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./rl-logs/ppo_12\n",
      "Eval num_timesteps=1000, episode_reward=-2.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.44    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.998    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.995    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00467  |\n",
      "|    n_updates        | 249      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-369.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -370     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.993    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00589  |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00532  |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.18    |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 384      |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.988    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-76.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -76.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.986    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00683  |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-266.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -266     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.983    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00681  |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-129.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -130     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00754  |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.06    |\n",
      "|    exploration_rate | 0.981    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-258.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -259     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.979    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00556  |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-258.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -259     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.976    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00467  |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-62.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -62.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.974    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0073   |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-52.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -52.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0068   |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.4    |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-6.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.69    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 2999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-249.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -250     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.967    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00586  |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-326.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -327     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00851  |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-239.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -240     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00961  |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.7    |\n",
      "|    exploration_rate | 0.962    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 381      |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 16000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-70.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -70.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.96     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00542  |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-61.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -61.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-365.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -366     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.955    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0069   |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-372.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -372     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00635  |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.6    |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 381      |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-368.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -368     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.95     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00696  |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-28.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -28      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.948    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00911  |\n",
      "|    n_updates        | 5249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-60.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -60.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00573  |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-347.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -347     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00622  |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.2    |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-369.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -370     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.941    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00743  |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-22.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.938    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00725  |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-16.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00537  |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-3.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.38    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.934    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00477  |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.5    |\n",
      "|    exploration_rate | 0.933    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-24.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -24.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00757  |\n",
      "|    n_updates        | 6999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-12.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.929    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00657  |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-375.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -375     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.926    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 31000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00563  |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-23.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -23      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00417  |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.5    |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=0.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.103    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.922    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00656  |\n",
      "|    n_updates        | 7999     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=-363.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -364     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.919    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.003    |\n",
      "|    n_updates        | 8249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-355.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -356     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.917    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-45.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -45.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.915    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00804  |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.6    |\n",
      "|    exploration_rate | 0.914    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-18.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.912    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00554  |\n",
      "|    n_updates        | 8999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-87.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -87.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.91     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00613  |\n",
      "|    n_updates        | 9249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-365.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -366     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.907    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00812  |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-109.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00569  |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.9    |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-372.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -373     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00605  |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-23.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -23.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.9      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00457  |\n",
      "|    n_updates        | 10249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-377.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -378     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.898    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-77.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -77      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00472  |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12      |\n",
      "|    exploration_rate | 0.895    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 44000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-184.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -184     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.893    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00533  |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-247.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -247     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00789  |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-263.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -263     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.888    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00778  |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-19.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -19.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00377  |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.9    |\n",
      "|    exploration_rate | 0.886    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 48000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-46.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -47      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.884    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 49000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00629  |\n",
      "|    n_updates        | 11999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-58.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -58.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00388  |\n",
      "|    n_updates        | 12249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-4.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.41    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.879    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00661  |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-101.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -101     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.877    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00433  |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.8    |\n",
      "|    exploration_rate | 0.876    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 52000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-367.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -367     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00505  |\n",
      "|    n_updates        | 12999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-54.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -54.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.872    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00596  |\n",
      "|    n_updates        | 13249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-195.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -195     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.869    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00419  |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-372.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -373     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00383  |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.6    |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 56000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-36.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -36.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.865    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00453  |\n",
      "|    n_updates        | 13999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-14.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00348  |\n",
      "|    n_updates        | 14249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-372.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -372     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.86     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00455  |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-65.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -65.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00568  |\n",
      "|    n_updates        | 14749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.7    |\n",
      "|    exploration_rate | 0.857    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 60000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-35.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -35.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.855    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00312  |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-27.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -27.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.853    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00403  |\n",
      "|    n_updates        | 15249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-7.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.87    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00548  |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-15.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00513  |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.8    |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total_timesteps  | 64000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-46.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -46.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.846    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00675  |\n",
      "|    n_updates        | 15999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-375.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -376     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.843    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00742  |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-374.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -375     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.841    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00449  |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-25.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -25.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.839    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00432  |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.1    |\n",
      "|    exploration_rate | 0.838    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 68000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-366.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -366     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.836    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0033   |\n",
      "|    n_updates        | 16999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-40.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -40.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00543  |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-14.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0051   |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-312.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -313     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.829    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00387  |\n",
      "|    n_updates        | 17749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.5    |\n",
      "|    exploration_rate | 0.829    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total_timesteps  | 72000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-135.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -136     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.827    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00534  |\n",
      "|    n_updates        | 17999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-193.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -193     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.824    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00432  |\n",
      "|    n_updates        | 18249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-147.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -147     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.822    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00502  |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-30.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -30.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 76000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-17.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00617  |\n",
      "|    n_updates        | 18999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-40.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -40.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.815    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00441  |\n",
      "|    n_updates        | 19249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-2.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.44    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00802  |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-17.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00849  |\n",
      "|    n_updates        | 19749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13      |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total_timesteps  | 80000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-363.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -364     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.808    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00497  |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-372.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -372     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.805    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0888   |\n",
      "|    n_updates        | 20249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-52.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -52.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00328  |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-361.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -362     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00293  |\n",
      "|    n_updates        | 20749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.3    |\n",
      "|    exploration_rate | 0.801    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 84000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-53.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -53.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.798    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00417  |\n",
      "|    n_updates        | 20999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-70.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -70.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.796    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0093   |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-124.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -125     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.793    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00642  |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-37.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -37.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 21749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.5    |\n",
      "|    exploration_rate | 0.791    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 88000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-96.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -96.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.789    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00473  |\n",
      "|    n_updates        | 21999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-25.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -25.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.786    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00604  |\n",
      "|    n_updates        | 22249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-121.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -122     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.784    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00929  |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-101.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -101     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00654  |\n",
      "|    n_updates        | 22749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.7    |\n",
      "|    exploration_rate | 0.782    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 92000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-67.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -67      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00803  |\n",
      "|    n_updates        | 22999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-57.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -57.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.777    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00677  |\n",
      "|    n_updates        | 23249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-20.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.774    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00791  |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-268.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -268     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00764  |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.9    |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 96000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-4.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.21    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.77     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0082   |\n",
      "|    n_updates        | 23999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-25.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -25.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.767    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00552  |\n",
      "|    n_updates        | 24249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-17.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.765    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00446  |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-70.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -70.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00796  |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.1    |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 265      |\n",
      "|    total_timesteps  | 100000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-373.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -373     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.76     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-365.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -365     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.758    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00657  |\n",
      "|    n_updates        | 25249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-45.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -45.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.755    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00779  |\n",
      "|    n_updates        | 25499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-15.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00531  |\n",
      "|    n_updates        | 25749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.7    |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 275      |\n",
      "|    total_timesteps  | 104000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-22.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.751    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00507  |\n",
      "|    n_updates        | 25999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-11.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00547  |\n",
      "|    n_updates        | 26249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-22.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.746    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00713  |\n",
      "|    n_updates        | 26499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-51.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -51.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 26749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15      |\n",
      "|    exploration_rate | 0.744    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total_timesteps  | 108000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-42.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -42.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.741    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00936  |\n",
      "|    n_updates        | 26999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-10.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.739    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00605  |\n",
      "|    n_updates        | 27249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-32.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -32.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.736    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00698  |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-49.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -49.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00978  |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.5    |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 112000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-375.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -376     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.732    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00512  |\n",
      "|    n_updates        | 27999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-144.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -145     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.729    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00676  |\n",
      "|    n_updates        | 28249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-47.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -47.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.727    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00574  |\n",
      "|    n_updates        | 28499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-28.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -28.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.725    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -16.1    |\n",
      "|    exploration_rate | 0.725    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 307      |\n",
      "|    total_timesteps  | 116000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-79.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -79.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.722    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 28999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-98.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -98.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00679  |\n",
      "|    n_updates        | 29249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-50.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -50.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.717    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00817  |\n",
      "|    n_updates        | 29499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-49.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -49.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 29749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -16.5    |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 318      |\n",
      "|    total_timesteps  | 120000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-26.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -26.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00849  |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-37.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -37.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.71     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00828  |\n",
      "|    n_updates        | 30249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-370.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -371     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.708    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-18.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.706    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00703  |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -17.1    |\n",
      "|    exploration_rate | 0.706    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 329      |\n",
      "|    total_timesteps  | 124000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-341.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -342     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.703    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00542  |\n",
      "|    n_updates        | 30999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-17.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00946  |\n",
      "|    n_updates        | 31249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-152.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -153     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.698    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00743  |\n",
      "|    n_updates        | 31499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-51.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -52      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00692  |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -17.6    |\n",
      "|    exploration_rate | 0.696    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 340      |\n",
      "|    total_timesteps  | 128000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-371.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -372     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.694    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00719  |\n",
      "|    n_updates        | 31999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-346.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -347     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00674  |\n",
      "|    n_updates        | 32249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-259.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -259     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.689    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00603  |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-21.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -21.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.687    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0075   |\n",
      "|    n_updates        | 32749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18      |\n",
      "|    exploration_rate | 0.686    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 132000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-4.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.38    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.684    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00493  |\n",
      "|    n_updates        | 32999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-316.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -317     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.682    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00599  |\n",
      "|    n_updates        | 33249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-323.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -324     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.679    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00684  |\n",
      "|    n_updates        | 33499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-190.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -191     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.677    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18.5    |\n",
      "|    exploration_rate | 0.677    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 361      |\n",
      "|    total_timesteps  | 136000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-51.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -51.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.675    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 33999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-13.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.672    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00846  |\n",
      "|    n_updates        | 34249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-30.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -30.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.67     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0061   |\n",
      "|    n_updates        | 34499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-30.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -30.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00863  |\n",
      "|    n_updates        | 34749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18.7    |\n",
      "|    exploration_rate | 0.667    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 372      |\n",
      "|    total_timesteps  | 140000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-58.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -58.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.665    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0095   |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-11.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.663    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00819  |\n",
      "|    n_updates        | 35249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-24.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -24.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.66     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00968  |\n",
      "|    n_updates        | 35499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-352.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -353     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00904  |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -19.2    |\n",
      "|    exploration_rate | 0.658    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 382      |\n",
      "|    total_timesteps  | 144000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-315.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -315     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.656    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00841  |\n",
      "|    n_updates        | 35999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-42.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -42.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.653    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00794  |\n",
      "|    n_updates        | 36249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-115.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -116     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00932  |\n",
      "|    n_updates        | 36499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-27.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -27.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -20.1    |\n",
      "|    exploration_rate | 0.649    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 393      |\n",
      "|    total_timesteps  | 148000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-20.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.646    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 36999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-29.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -29.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.644    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00957  |\n",
      "|    n_updates        | 37249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-170.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -171     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.641    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0084   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-27.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -27.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 37749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -20.7    |\n",
      "|    exploration_rate | 0.639    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 404      |\n",
      "|    total_timesteps  | 152000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-45.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -45.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.637    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00822  |\n",
      "|    n_updates        | 37999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-376.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -377     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.634    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 38249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-377.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -378     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.632    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00702  |\n",
      "|    n_updates        | 38499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-82.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -82.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.63     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 38749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -21.5    |\n",
      "|    exploration_rate | 0.629    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 414      |\n",
      "|    total_timesteps  | 156000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-51.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -51.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.627    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 38999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=3.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.625    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00959  |\n",
      "|    n_updates        | 39249    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=159000, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.833    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.622    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 39499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-18.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00998  |\n",
      "|    n_updates        | 39749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -21.9    |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 424      |\n",
      "|    total_timesteps  | 160000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-195.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -196     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.618    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00847  |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-47.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -47.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.615    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00808  |\n",
      "|    n_updates        | 40249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-22.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.613    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00991  |\n",
      "|    n_updates        | 40499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-378.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -379     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0162   |\n",
      "|    n_updates        | 40749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.2    |\n",
      "|    exploration_rate | 0.611    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 434      |\n",
      "|    total_timesteps  | 164000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-19.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -19.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.608    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 40999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-322.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -323     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.606    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00934  |\n",
      "|    n_updates        | 41249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-35.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -35.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.603    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 41499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-38.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -38.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00616  |\n",
      "|    n_updates        | 41749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.7    |\n",
      "|    exploration_rate | 0.601    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 445      |\n",
      "|    total_timesteps  | 168000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-320.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -320     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.599    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00985  |\n",
      "|    n_updates        | 41999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-32.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -32.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.596    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00917  |\n",
      "|    n_updates        | 42249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-311.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -312     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-361.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -361     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00865  |\n",
      "|    n_updates        | 42749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.9    |\n",
      "|    exploration_rate | 0.592    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 455      |\n",
      "|    total_timesteps  | 172000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-30.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -30.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.589    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00905  |\n",
      "|    n_updates        | 42999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-375.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -375     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00965  |\n",
      "|    n_updates        | 43249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-133.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -133     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.584    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00959  |\n",
      "|    n_updates        | 43499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-375.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -376     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 43749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.3    |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 465      |\n",
      "|    total_timesteps  | 176000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-279.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -280     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.58     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00792  |\n",
      "|    n_updates        | 43999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-201.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -201     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.577    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00877  |\n",
      "|    n_updates        | 44249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-376.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -377     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.575    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 44499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-22.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00565  |\n",
      "|    n_updates        | 44749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.6    |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 475      |\n",
      "|    total_timesteps  | 180000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-37.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -37.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.57     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 44999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-31.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -31.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.568    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0093   |\n",
      "|    n_updates        | 45249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-11.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.565    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 45499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-22.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.563    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 45749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.9    |\n",
      "|    exploration_rate | 0.563    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 486      |\n",
      "|    total_timesteps  | 184000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-125.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -126     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.561    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00975  |\n",
      "|    n_updates        | 45999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-13.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.558    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 46249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-27.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -27.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.556    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00895  |\n",
      "|    n_updates        | 46499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-375.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -376     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0096   |\n",
      "|    n_updates        | 46749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.1    |\n",
      "|    exploration_rate | 0.554    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 496      |\n",
      "|    total_timesteps  | 188000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-289.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -289     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.551    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00779  |\n",
      "|    n_updates        | 46999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-169.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -169     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.549    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00886  |\n",
      "|    n_updates        | 47249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-376.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -377     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.546    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00944  |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-7.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.34    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00835  |\n",
      "|    n_updates        | 47749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.3    |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 507      |\n",
      "|    total_timesteps  | 192000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-17.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.542    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 47999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-24.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -24.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.539    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00865  |\n",
      "|    n_updates        | 48249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-15.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.537    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 48499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-12.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00987  |\n",
      "|    n_updates        | 48749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.4    |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 518      |\n",
      "|    total_timesteps  | 196000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-12.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.532    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 48999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-379.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -380     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.53     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 49249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-342.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -343     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.527    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00934  |\n",
      "|    n_updates        | 49499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-284.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -284     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00876  |\n",
      "|    n_updates        | 49749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.5    |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 528      |\n",
      "|    total_timesteps  | 200000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-15.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.523    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 201000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0705   |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.52     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 202000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 50249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-20.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.518    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 203000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 50499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-108.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -108     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.516    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 204000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 50749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.5    |\n",
      "|    exploration_rate | 0.516    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 538      |\n",
      "|    total_timesteps  | 204000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.513    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 205000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 50999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-13.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.511    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 206000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00907  |\n",
      "|    n_updates        | 51249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=-339.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -340     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.508    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 207000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00852  |\n",
      "|    n_updates        | 51499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-3.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.36    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.506    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 208000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00999  |\n",
      "|    n_updates        | 51749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.6    |\n",
      "|    exploration_rate | 0.506    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 549      |\n",
      "|    total_timesteps  | 208000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=-54.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -54.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.504    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 209000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00932  |\n",
      "|    n_updates        | 51999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-20.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0074   |\n",
      "|    n_updates        | 52249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-9.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.79    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.499    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 211000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00878  |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=1.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 212000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 52749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -24.2    |\n",
      "|    exploration_rate | 0.497    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 560      |\n",
      "|    total_timesteps  | 212000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=2.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.494    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 213000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0836   |\n",
      "|    n_updates        | 52999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-3.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.75    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.492    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 214000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 53249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-6.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.73    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.489    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 215000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 53499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-6.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.89    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 216000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0107   |\n",
      "|    n_updates        | 53749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.7    |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 570      |\n",
      "|    total_timesteps  | 216000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-38.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -38.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.485    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 217000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 53999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-13.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.482    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 218000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 54249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-36.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -36.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 219000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 54499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-38.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -38.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0135   |\n",
      "|    n_updates        | 54749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.4    |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 581      |\n",
      "|    total_timesteps  | 220000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-62.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -62.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 221000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 54999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-55.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -55.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.473    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 222000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 55249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-64.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -64.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.47     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 223000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00841  |\n",
      "|    n_updates        | 55499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-47.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -47.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.468    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 224000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00942  |\n",
      "|    n_updates        | 55749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -23.2    |\n",
      "|    exploration_rate | 0.468    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 592      |\n",
      "|    total_timesteps  | 224000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.466    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 225000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00856  |\n",
      "|    n_updates        | 55999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-10.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.463    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 226000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0131   |\n",
      "|    n_updates        | 56249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=-64.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -64.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.461    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 227000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00822  |\n",
      "|    n_updates        | 56499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-4.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.14    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 228000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00994  |\n",
      "|    n_updates        | 56749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.8    |\n",
      "|    exploration_rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 602      |\n",
      "|    total_timesteps  | 228000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-32.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -32.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.456    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 229000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 56999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-44.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -44.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.454    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00963  |\n",
      "|    n_updates        | 57249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-78.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -78      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.451    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 231000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00622  |\n",
      "|    n_updates        | 57499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-15.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.449    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 232000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 57749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.6    |\n",
      "|    exploration_rate | 0.449    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 613      |\n",
      "|    total_timesteps  | 232000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-7.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.28    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.447    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 233000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 57999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-16.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.444    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 234000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00925  |\n",
      "|    n_updates        | 58249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-36.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -36.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.442    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 235000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 58499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-5.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.03    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.44     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 236000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 58749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.5    |\n",
      "|    exploration_rate | 0.44     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 624      |\n",
      "|    total_timesteps  | 236000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-43.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -43.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.437    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 237000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00926  |\n",
      "|    n_updates        | 58999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-19.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -19.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 238000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0112   |\n",
      "|    n_updates        | 59249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=5.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.25     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.432    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 239000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 59499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.69    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00976  |\n",
      "|    n_updates        | 59749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.3    |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 634      |\n",
      "|    total_timesteps  | 240000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-31.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -31.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.428    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 241000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00594  |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-98.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -98.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.425    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 242000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 60249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-51.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -52      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.423    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 243000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00634  |\n",
      "|    n_updates        | 60499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-12.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 244000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00534  |\n",
      "|    n_updates        | 60749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -22.1    |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 645      |\n",
      "|    total_timesteps  | 244000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-11.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.418    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 245000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00766  |\n",
      "|    n_updates        | 60999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-36.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -36.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.416    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 246000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00802  |\n",
      "|    n_updates        | 61249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.413    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 247000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.013    |\n",
      "|    n_updates        | 61499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-1.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.85    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 248000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 61749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -21.5    |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 655      |\n",
      "|    total_timesteps  | 248000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-22.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.409    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 249000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0109   |\n",
      "|    n_updates        | 61999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-18.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00887  |\n",
      "|    n_updates        | 62249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.14    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.404    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 251000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00496  |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-16.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.402    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 252000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00638  |\n",
      "|    n_updates        | 62749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -21.1    |\n",
      "|    exploration_rate | 0.402    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 665      |\n",
      "|    total_timesteps  | 252000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=2.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.399    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 253000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00941  |\n",
      "|    n_updates        | 62999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-14.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.397    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 254000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00948  |\n",
      "|    n_updates        | 63249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-20.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.394    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 255000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00828  |\n",
      "|    n_updates        | 63499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-17.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 256000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00934  |\n",
      "|    n_updates        | 63749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -20.5    |\n",
      "|    exploration_rate | 0.392    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 676      |\n",
      "|    total_timesteps  | 256000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-385.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -385     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.39     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 257000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00575  |\n",
      "|    n_updates        | 63999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-20.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.387    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 258000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00934  |\n",
      "|    n_updates        | 64249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-16.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.385    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 259000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 64499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-8.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.08    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00588  |\n",
      "|    n_updates        | 64749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -20.3    |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 686      |\n",
      "|    total_timesteps  | 260000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-17.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.38     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 261000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00991  |\n",
      "|    n_updates        | 64999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-12.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.378    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 262000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00684  |\n",
      "|    n_updates        | 65249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-24.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -24.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.375    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 263000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00777  |\n",
      "|    n_updates        | 65499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-16.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.373    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 264000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00966  |\n",
      "|    n_updates        | 65749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -19.9    |\n",
      "|    exploration_rate | 0.373    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 697      |\n",
      "|    total_timesteps  | 264000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-17.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.371    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 265000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00675  |\n",
      "|    n_updates        | 65999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.373   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.368    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 266000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00874  |\n",
      "|    n_updates        | 66249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=1.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.17     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.366    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 267000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 66499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-3.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.78    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.364    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 268000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00839  |\n",
      "|    n_updates        | 66749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -19.2    |\n",
      "|    exploration_rate | 0.364    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 707      |\n",
      "|    total_timesteps  | 268000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-15.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.361    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 269000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00714  |\n",
      "|    n_updates        | 66999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-0.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.249   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.359    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 67249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-25.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -25.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.356    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 271000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 67499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-7.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.52    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 272000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0168   |\n",
      "|    n_updates        | 67749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18.9    |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 718      |\n",
      "|    total_timesteps  | 272000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=-23.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -23.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.352    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 273000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00726  |\n",
      "|    n_updates        | 67999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=-9.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.08    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.349    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 274000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 68249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-101.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.347    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 275000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00904  |\n",
      "|    n_updates        | 68499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-102.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 276000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0108   |\n",
      "|    n_updates        | 68749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18.6    |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 728      |\n",
      "|    total_timesteps  | 276000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=-11.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.342    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 277000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 68999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=1.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.17     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 278000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00838  |\n",
      "|    n_updates        | 69249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=-21.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -21.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.337    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 279000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00797  |\n",
      "|    n_updates        | 69499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-10.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00596  |\n",
      "|    n_updates        | 69749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -18.4    |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 740      |\n",
      "|    total_timesteps  | 280000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=-2.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 281000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00553  |\n",
      "|    n_updates        | 69999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.764    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.33     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 282000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00809  |\n",
      "|    n_updates        | 70249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=-2.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.55    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.328    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 283000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00966  |\n",
      "|    n_updates        | 70499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=1.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.326    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 284000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00989  |\n",
      "|    n_updates        | 70749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -17.5    |\n",
      "|    exploration_rate | 0.325    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 751      |\n",
      "|    total_timesteps  | 284000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-4.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.48    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.323    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 285000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00495  |\n",
      "|    n_updates        | 70999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-3.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.321    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 286000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00626  |\n",
      "|    n_updates        | 71249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-4.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.76    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.318    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 287000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00595  |\n",
      "|    n_updates        | 71499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-91.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -91.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 288000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00933  |\n",
      "|    n_updates        | 71749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -17.2    |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 761      |\n",
      "|    total_timesteps  | 288000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-4.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.34    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.314    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 289000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00431  |\n",
      "|    n_updates        | 71999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-86.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -86.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.311    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00523  |\n",
      "|    n_updates        | 72249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-12.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.309    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 291000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00436  |\n",
      "|    n_updates        | 72499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-142.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -142     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.307    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 292000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00714  |\n",
      "|    n_updates        | 72749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -16.9    |\n",
      "|    exploration_rate | 0.306    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 378      |\n",
      "|    time_elapsed     | 772      |\n",
      "|    total_timesteps  | 292000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-134.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -135     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.304    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 293000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00593  |\n",
      "|    n_updates        | 72999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-133.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -134     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.302    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 294000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00558  |\n",
      "|    n_updates        | 73249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-336.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -337     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.299    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 295000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00537  |\n",
      "|    n_updates        | 73499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-3.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.11    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.297    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 296000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00832  |\n",
      "|    n_updates        | 73749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -17      |\n",
      "|    exploration_rate | 0.297    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 783      |\n",
      "|    total_timesteps  | 296000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=-2.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.89    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.295    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 297000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00795  |\n",
      "|    n_updates        | 73999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-3.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 298000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00553  |\n",
      "|    n_updates        | 74249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=-74.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -74.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.29     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 299000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0069   |\n",
      "|    n_updates        | 74499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-73.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -73.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00501  |\n",
      "|    n_updates        | 74749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -16.4    |\n",
      "|    exploration_rate | 0.287    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 794      |\n",
      "|    total_timesteps  | 300000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=-5.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.93    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.285    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 301000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0055   |\n",
      "|    n_updates        | 74999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-16.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.283    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 302000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00389  |\n",
      "|    n_updates        | 75249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-11.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.28     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 303000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00542  |\n",
      "|    n_updates        | 75499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-3.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.01    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 304000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00371  |\n",
      "|    n_updates        | 75749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -16      |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 804      |\n",
      "|    total_timesteps  | 304000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-112.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -113     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.276    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 305000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00343  |\n",
      "|    n_updates        | 75999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-10.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.273    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 306000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00652  |\n",
      "|    n_updates        | 76249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=-14.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.271    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 307000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00434  |\n",
      "|    n_updates        | 76499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-8.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.01    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.269    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 308000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00633  |\n",
      "|    n_updates        | 76749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.6    |\n",
      "|    exploration_rate | 0.268    |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 815      |\n",
      "|    total_timesteps  | 308000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-4.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.266    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 309000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00615  |\n",
      "|    n_updates        | 76999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-28.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -28.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.264    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00423  |\n",
      "|    n_updates        | 77249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.261    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 311000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00461  |\n",
      "|    n_updates        | 77499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.259    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 312000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00404  |\n",
      "|    n_updates        | 77749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.6    |\n",
      "|    exploration_rate | 0.259    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 825      |\n",
      "|    total_timesteps  | 312000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=-9.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.29    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.257    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 313000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00317  |\n",
      "|    n_updates        | 77999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-17.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.254    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 314000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0047   |\n",
      "|    n_updates        | 78249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-50.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -50.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.252    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 315000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 78499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.25     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 316000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00619  |\n",
      "|    n_updates        | 78749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.7    |\n",
      "|    exploration_rate | 0.249    |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 837      |\n",
      "|    total_timesteps  | 316000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=317000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.247    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 317000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00473  |\n",
      "|    n_updates        | 78999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 318000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00651  |\n",
      "|    n_updates        | 79249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.242    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 319000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00701  |\n",
      "|    n_updates        | 79499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00492  |\n",
      "|    n_updates        | 79749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.6    |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 377      |\n",
      "|    time_elapsed     | 848      |\n",
      "|    total_timesteps  | 320000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-26.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -26.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.238    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 321000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00473  |\n",
      "|    n_updates        | 79999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-3.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.96    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.235    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 322000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 80249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=-9.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.56    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.233    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 323000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00279  |\n",
      "|    n_updates        | 80499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=1.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.231    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 324000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00333  |\n",
      "|    n_updates        | 80749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.4    |\n",
      "|    exploration_rate | 0.23     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 859      |\n",
      "|    total_timesteps  | 324000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.228    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 325000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00624  |\n",
      "|    n_updates        | 80999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.226    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 326000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00562  |\n",
      "|    n_updates        | 81249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.223    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 327000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0033   |\n",
      "|    n_updates        | 81499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-7.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.09    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.221    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 328000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0067   |\n",
      "|    n_updates        | 81749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -15.1    |\n",
      "|    exploration_rate | 0.221    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 871      |\n",
      "|    total_timesteps  | 328000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=-19.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -19.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.219    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 329000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00843  |\n",
      "|    n_updates        | 81999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-8.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.27    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00457  |\n",
      "|    n_updates        | 82249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=-26.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -26.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.214    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 331000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00291  |\n",
      "|    n_updates        | 82499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-17.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.212    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 332000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00256  |\n",
      "|    n_updates        | 82749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.8    |\n",
      "|    exploration_rate | 0.211    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 882      |\n",
      "|    total_timesteps  | 332000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-4.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.31    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.209    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 333000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00246  |\n",
      "|    n_updates        | 82999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-381.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -381     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.207    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 334000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00229  |\n",
      "|    n_updates        | 83249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-24.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -25      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.204    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 335000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00326  |\n",
      "|    n_updates        | 83499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-14.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 336000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00401  |\n",
      "|    n_updates        | 83749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.4    |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 893      |\n",
      "|    total_timesteps  | 336000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=-1.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.62    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.2      |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 337000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00152  |\n",
      "|    n_updates        | 83999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-7.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.34    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.197    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 338000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 84249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.702    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.195    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 339000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00346  |\n",
      "|    n_updates        | 84499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-22.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -22.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00365  |\n",
      "|    n_updates        | 84749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -14.3    |\n",
      "|    exploration_rate | 0.192    |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 903      |\n",
      "|    total_timesteps  | 340000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=-18.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -18.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.19     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 341000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00203  |\n",
      "|    n_updates        | 84999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=0.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.918    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 342000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00115  |\n",
      "|    n_updates        | 85249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=-37.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -37.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.185    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 343000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0027   |\n",
      "|    n_updates        | 85499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.183    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 344000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0036   |\n",
      "|    n_updates        | 85749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.9    |\n",
      "|    exploration_rate | 0.183    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 376      |\n",
      "|    time_elapsed     | 914      |\n",
      "|    total_timesteps  | 344000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-380.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -381     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.181    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 345000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 85999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-12.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.178    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 346000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00193  |\n",
      "|    n_updates        | 86249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=-3.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.95    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.176    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 347000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00498  |\n",
      "|    n_updates        | 86499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-10.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.174    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 348000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00609  |\n",
      "|    n_updates        | 86749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.7    |\n",
      "|    exploration_rate | 0.174    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 925      |\n",
      "|    total_timesteps  | 348000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=-7.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.41    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.171    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 349000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00251  |\n",
      "|    n_updates        | 86999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-4.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.07    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.169    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00411  |\n",
      "|    n_updates        | 87249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.166    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 351000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000969 |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-2.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.57    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.164    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 352000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00317  |\n",
      "|    n_updates        | 87749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.4    |\n",
      "|    exploration_rate | 0.164    |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 936      |\n",
      "|    total_timesteps  | 352000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=-7.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.99    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 353000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00214  |\n",
      "|    n_updates        | 87999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=0.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.988    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.159    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 354000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0028   |\n",
      "|    n_updates        | 88249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=2.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.157    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 355000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0045   |\n",
      "|    n_updates        | 88499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-205.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -206     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.155    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 356000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00294  |\n",
      "|    n_updates        | 88749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -13.4    |\n",
      "|    exploration_rate | 0.155    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 947      |\n",
      "|    total_timesteps  | 356000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-17.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.152    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 357000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000952 |\n",
      "|    n_updates        | 88999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=2.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 358000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00324  |\n",
      "|    n_updates        | 89249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-2.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.18    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.147    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 359000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00573  |\n",
      "|    n_updates        | 89499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-1.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.94    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00917  |\n",
      "|    n_updates        | 89749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 958      |\n",
      "|    total_timesteps  | 360000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=-118.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.143    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 361000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00192  |\n",
      "|    n_updates        | 89999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=-131.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -131     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.14     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 362000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00191  |\n",
      "|    n_updates        | 90249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-26.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -26.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.138    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 363000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0027   |\n",
      "|    n_updates        | 90499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-23.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -23.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.136    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 364000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00572  |\n",
      "|    n_updates        | 90749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    exploration_rate | 0.136    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 968      |\n",
      "|    total_timesteps  | 364000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-159.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -159     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.133    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 365000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00282  |\n",
      "|    n_updates        | 90999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-20.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.131    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 366000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 91249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=-8.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.48    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.128    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 367000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00485  |\n",
      "|    n_updates        | 91499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-102.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.126    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 368000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0024   |\n",
      "|    n_updates        | 91749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.6    |\n",
      "|    exploration_rate | 0.126    |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 980      |\n",
      "|    total_timesteps  | 368000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=1.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.59     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 369000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00186  |\n",
      "|    n_updates        | 91999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-20.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -20.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.121    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 92249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=-10.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -10.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.119    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 371000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00641  |\n",
      "|    n_updates        | 92499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-3.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.03    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.117    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 372000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00483  |\n",
      "|    n_updates        | 92749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -12.2    |\n",
      "|    exploration_rate | 0.117    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 990      |\n",
      "|    total_timesteps  | 372000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=373000, episode_reward=-1.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.96    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.114    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 373000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00137  |\n",
      "|    n_updates        | 92999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-195.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -195     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.112    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 374000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00636  |\n",
      "|    n_updates        | 93249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-174.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -174     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 375000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00316  |\n",
      "|    n_updates        | 93499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-1.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.89    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 376000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00183  |\n",
      "|    n_updates        | 93749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.6    |\n",
      "|    exploration_rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1001     |\n",
      "|    total_timesteps  | 376000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=-2.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.89    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.105    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 377000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00205  |\n",
      "|    n_updates        | 93999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-5.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.43    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.102    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 378000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00171  |\n",
      "|    n_updates        | 94249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=2.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0999   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 379000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00323  |\n",
      "|    n_updates        | 94499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=2.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0975   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0029   |\n",
      "|    n_updates        | 94749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.1    |\n",
      "|    exploration_rate | 0.0975   |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1011     |\n",
      "|    total_timesteps  | 380000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=-86.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -86.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0951   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 381000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00281  |\n",
      "|    n_updates        | 94999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-2.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.42    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0928   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 382000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00125  |\n",
      "|    n_updates        | 95249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=-147.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -147     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0904   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 383000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00249  |\n",
      "|    n_updates        | 95499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-13.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.088    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 384000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00247  |\n",
      "|    n_updates        | 95749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11.4    |\n",
      "|    exploration_rate | 0.088    |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1021     |\n",
      "|    total_timesteps  | 384000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-101.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0856   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 385000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00304  |\n",
      "|    n_updates        | 95999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-128.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -129     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0833   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 386000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00098  |\n",
      "|    n_updates        | 96249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.00263  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0809   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 387000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00151  |\n",
      "|    n_updates        | 96499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-13.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -13.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0785   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 388000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00113  |\n",
      "|    n_updates        | 96749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -11      |\n",
      "|    exploration_rate | 0.0785   |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1032     |\n",
      "|    total_timesteps  | 388000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=-2.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.45    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0761   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 389000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00158  |\n",
      "|    n_updates        | 96999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-4.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.63    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0738   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00085  |\n",
      "|    n_updates        | 97249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=-2.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.78    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0714   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 391000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00188  |\n",
      "|    n_updates        | 97499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-8.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.92    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.069    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 392000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000956 |\n",
      "|    n_updates        | 97749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.5    |\n",
      "|    exploration_rate | 0.069    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1043     |\n",
      "|    total_timesteps  | 392000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=-2.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.27    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0666   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 393000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000224 |\n",
      "|    n_updates        | 97999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-7.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.31    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0643   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 394000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 98249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=1.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0619   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 395000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0018   |\n",
      "|    n_updates        | 98499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-16.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -16.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0595   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 396000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0017   |\n",
      "|    n_updates        | 98749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.7    |\n",
      "|    exploration_rate | 0.0595   |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1053     |\n",
      "|    total_timesteps  | 396000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=-6.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.31    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0571   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 397000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00191  |\n",
      "|    n_updates        | 98999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-5.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0548   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 398000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00116  |\n",
      "|    n_updates        | 99249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=-5.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.16    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0524   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 399000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 99499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-1.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000392 |\n",
      "|    n_updates        | 99749    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1065     |\n",
      "|    total_timesteps  | 400000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=-9.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.46    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 401000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000595 |\n",
      "|    n_updates        | 99999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=-0.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.485   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 402000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.001    |\n",
      "|    n_updates        | 100249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=-1.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.28    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 403000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.001    |\n",
      "|    n_updates        | 100499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-4.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.04    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 404000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 100749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -10.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1075     |\n",
      "|    total_timesteps  | 404000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-3.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.84    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 405000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000989 |\n",
      "|    n_updates        | 100999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=-4.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.69    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 406000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 101249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=407000, episode_reward=1.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.98     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 407000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00121  |\n",
      "|    n_updates        | 101499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-3.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.13    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 408000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 101749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.85    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1086     |\n",
      "|    total_timesteps  | 408000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=3.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 409000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000802 |\n",
      "|    n_updates        | 101999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-1.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.51    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000908 |\n",
      "|    n_updates        | 102249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=0.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.885    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 411000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00289  |\n",
      "|    n_updates        | 102499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-6.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.56    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 412000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00233  |\n",
      "|    n_updates        | 102749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -9.27    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1097     |\n",
      "|    total_timesteps  | 412000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=1.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 413000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00381  |\n",
      "|    n_updates        | 102999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=-1.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 414000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00134  |\n",
      "|    n_updates        | 103249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-2.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.96    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 415000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00193  |\n",
      "|    n_updates        | 103499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-3.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.18    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 416000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00365  |\n",
      "|    n_updates        | 103749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.77    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1108     |\n",
      "|    total_timesteps  | 416000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-2.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.06    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 417000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0021   |\n",
      "|    n_updates        | 103999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=-41.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -41.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 418000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000698 |\n",
      "|    n_updates        | 104249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=-2.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.25    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 419000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000883 |\n",
      "|    n_updates        | 104499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-14.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 104749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -8.27    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1119     |\n",
      "|    total_timesteps  | 420000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=421000, episode_reward=-1.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.63    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 421000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 104999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=-2.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 422000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00152  |\n",
      "|    n_updates        | 105249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=-3.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.46    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 423000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00215  |\n",
      "|    n_updates        | 105499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-1.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.42    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 424000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00136  |\n",
      "|    n_updates        | 105749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.95    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1129     |\n",
      "|    total_timesteps  | 424000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-59.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -59.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 425000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00225  |\n",
      "|    n_updates        | 105999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=0.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.252    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 426000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00133  |\n",
      "|    n_updates        | 106249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=0.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.342    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 427000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 106499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-0.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.732   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 428000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00127  |\n",
      "|    n_updates        | 106749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.66    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 375      |\n",
      "|    time_elapsed     | 1140     |\n",
      "|    total_timesteps  | 428000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=-6.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 429000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 106999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=-4.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00223  |\n",
      "|    n_updates        | 107249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=-32.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -32      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 431000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000702 |\n",
      "|    n_updates        | 107499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=2.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.65     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 432000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 107749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.58    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1152     |\n",
      "|    total_timesteps  | 432000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=-3.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 433000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00111  |\n",
      "|    n_updates        | 107999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=0.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.373    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 434000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 108249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=4.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 435000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000765 |\n",
      "|    n_updates        | 108499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-3.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.02    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 436000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000581 |\n",
      "|    n_updates        | 108749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.36    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1163     |\n",
      "|    total_timesteps  | 436000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=-2.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.95    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 437000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000646 |\n",
      "|    n_updates        | 108999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-0.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.911   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 438000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0009   |\n",
      "|    n_updates        | 109249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-3.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 439000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00198  |\n",
      "|    n_updates        | 109499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-1.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.45    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 440000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 109749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.81    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1175     |\n",
      "|    total_timesteps  | 440000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=-1.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.56    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 441000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000981 |\n",
      "|    n_updates        | 109999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=-8.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.58    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 442000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000713 |\n",
      "|    n_updates        | 110249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=-380.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -380     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 443000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000411 |\n",
      "|    n_updates        | 110499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=1.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.95     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 444000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00248  |\n",
      "|    n_updates        | 110749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.68    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1186     |\n",
      "|    total_timesteps  | 444000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=4.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.06     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 445000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000546 |\n",
      "|    n_updates        | 110999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=4.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 446000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000763 |\n",
      "|    n_updates        | 111249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=4.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 447000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000411 |\n",
      "|    n_updates        | 111499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=-3.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.99    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 448000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000561 |\n",
      "|    n_updates        | 111749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1197     |\n",
      "|    total_timesteps  | 448000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=3.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 449000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00135  |\n",
      "|    n_updates        | 111999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-1.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.86    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000342 |\n",
      "|    n_updates        | 112249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=0.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.893    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 451000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00156  |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=-19.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -19.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 452000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000635 |\n",
      "|    n_updates        | 112749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -7.05    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 374      |\n",
      "|    time_elapsed     | 1208     |\n",
      "|    total_timesteps  | 452000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=-3.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.48    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 453000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000932 |\n",
      "|    n_updates        | 112999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=-252.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -253     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 454000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00107  |\n",
      "|    n_updates        | 113249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-382.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -383     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 455000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000653 |\n",
      "|    n_updates        | 113499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-2.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.32    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 456000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00207  |\n",
      "|    n_updates        | 113749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.96    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 373      |\n",
      "|    time_elapsed     | 1219     |\n",
      "|    total_timesteps  | 456000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=-6.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.27    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 457000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00056  |\n",
      "|    n_updates        | 113999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=-4.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 458000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00177  |\n",
      "|    n_updates        | 114249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=2.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 459000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 114499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-8.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -8.45    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 460000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000955 |\n",
      "|    n_updates        | 114749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 373      |\n",
      "|    time_elapsed     | 1230     |\n",
      "|    total_timesteps  | 460000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=-0.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.562   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 461000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000918 |\n",
      "|    n_updates        | 114999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=4.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 462000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000534 |\n",
      "|    n_updates        | 115249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=463000, episode_reward=-1.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.75    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 463000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000353 |\n",
      "|    n_updates        | 115499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-5.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 464000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00166  |\n",
      "|    n_updates        | 115749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.87    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 373      |\n",
      "|    time_elapsed     | 1241     |\n",
      "|    total_timesteps  | 464000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-1.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 465000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000573 |\n",
      "|    n_updates        | 115999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=4.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 466000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000832 |\n",
      "|    n_updates        | 116249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=1.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 467000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000562 |\n",
      "|    n_updates        | 116499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=2.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 468000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00124  |\n",
      "|    n_updates        | 116749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.55    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 373      |\n",
      "|    time_elapsed     | 1253     |\n",
      "|    total_timesteps  | 468000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=-15.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 469000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00182  |\n",
      "|    n_updates        | 116999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=4.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 470000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00106  |\n",
      "|    n_updates        | 117249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=-3.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.67    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 471000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000718 |\n",
      "|    n_updates        | 117499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-5.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.13    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 472000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000397 |\n",
      "|    n_updates        | 117749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.38    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 373      |\n",
      "|    time_elapsed     | 1264     |\n",
      "|    total_timesteps  | 472000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=1.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 473000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000407 |\n",
      "|    n_updates        | 117999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-4.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.86    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 474000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000461 |\n",
      "|    n_updates        | 118249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=0.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.0687   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 475000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00141  |\n",
      "|    n_updates        | 118499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=1.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 476000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000239 |\n",
      "|    n_updates        | 118749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.24    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1276     |\n",
      "|    total_timesteps  | 476000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.187   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 477000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 118999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-15.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -15.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 478000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00392  |\n",
      "|    n_updates        | 119249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-1.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.99    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 479000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00249  |\n",
      "|    n_updates        | 119499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=4.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 480000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00015  |\n",
      "|    n_updates        | 119749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -6.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1287     |\n",
      "|    total_timesteps  | 480000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=3.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 481000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00549  |\n",
      "|    n_updates        | 119999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=3.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.97     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 482000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000362 |\n",
      "|    n_updates        | 120249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=2.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 483000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000508 |\n",
      "|    n_updates        | 120499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=4.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.62     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 484000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00113  |\n",
      "|    n_updates        | 120749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5.55    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1298     |\n",
      "|    total_timesteps  | 484000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=3.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.92     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 485000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00116  |\n",
      "|    n_updates        | 120999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=1.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.41     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 486000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000304 |\n",
      "|    n_updates        | 121249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=487000, episode_reward=-0.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.762   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 487000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000502 |\n",
      "|    n_updates        | 121499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-12.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 488000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000731 |\n",
      "|    n_updates        | 121749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5.45    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1310     |\n",
      "|    total_timesteps  | 488000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=-185.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -186     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 489000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000687 |\n",
      "|    n_updates        | 121999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 490000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000251 |\n",
      "|    n_updates        | 122249   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=491000, episode_reward=-6.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.06    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 491000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00067  |\n",
      "|    n_updates        | 122499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=0.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.99     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 492000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00432  |\n",
      "|    n_updates        | 122749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -5.76    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1320     |\n",
      "|    total_timesteps  | 492000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=0.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.471    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 493000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000275 |\n",
      "|    n_updates        | 122999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=0.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.128    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 494000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00114  |\n",
      "|    n_updates        | 123249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.668    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 495000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000273 |\n",
      "|    n_updates        | 123499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=6.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 496000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000384 |\n",
      "|    n_updates        | 123749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.71    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1331     |\n",
      "|    total_timesteps  | 496000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=497000, episode_reward=6.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 497000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000498 |\n",
      "|    n_updates        | 123999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=1.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.9      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 498000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000401 |\n",
      "|    n_updates        | 124249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=1.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 499000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000326 |\n",
      "|    n_updates        | 124499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-2.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.96    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.49e-05 |\n",
      "|    n_updates        | 124749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.51    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1342     |\n",
      "|    total_timesteps  | 500000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=1.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 501000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000187 |\n",
      "|    n_updates        | 124999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-5.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.09    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 502000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000143 |\n",
      "|    n_updates        | 125249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=3.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 503000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0014   |\n",
      "|    n_updates        | 125499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=4.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.72     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 504000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000387 |\n",
      "|    n_updates        | 125749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.33    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1352     |\n",
      "|    total_timesteps  | 504000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=2.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.07     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 505000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00038  |\n",
      "|    n_updates        | 125999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=1.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.26     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 506000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000433 |\n",
      "|    n_updates        | 126249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=1.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 507000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00179  |\n",
      "|    n_updates        | 126499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=-2.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.75    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 508000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000187 |\n",
      "|    n_updates        | 126749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.19    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1363     |\n",
      "|    total_timesteps  | 508000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=3.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.94     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 509000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000184 |\n",
      "|    n_updates        | 126999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=2.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.88     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 510000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000594 |\n",
      "|    n_updates        | 127249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=1.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.78     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 511000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000336 |\n",
      "|    n_updates        | 127499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=3.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 512000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000431 |\n",
      "|    n_updates        | 127749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1374     |\n",
      "|    total_timesteps  | 512000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=-12.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 513000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00164  |\n",
      "|    n_updates        | 127999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.688    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 514000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000515 |\n",
      "|    n_updates        | 128249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=4.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 515000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000343 |\n",
      "|    n_updates        | 128499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=3.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 516000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000334 |\n",
      "|    n_updates        | 128749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1385     |\n",
      "|    total_timesteps  | 516000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=4.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 517000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000614 |\n",
      "|    n_updates        | 128999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=3.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 518000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000445 |\n",
      "|    n_updates        | 129249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=3.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 519000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000188 |\n",
      "|    n_updates        | 129499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 520000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000506 |\n",
      "|    n_updates        | 129749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.91    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1395     |\n",
      "|    total_timesteps  | 520000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=3.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.06     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 521000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000276 |\n",
      "|    n_updates        | 129999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=2.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.38     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 522000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000281 |\n",
      "|    n_updates        | 130249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=-11.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 523000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000451 |\n",
      "|    n_updates        | 130499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=0.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.583    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 524000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000555 |\n",
      "|    n_updates        | 130749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1406     |\n",
      "|    total_timesteps  | 524000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=3.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 525000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000731 |\n",
      "|    n_updates        | 130999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=3.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 526000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000204 |\n",
      "|    n_updates        | 131249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=-9.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9       |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 527000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000566 |\n",
      "|    n_updates        | 131499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=2.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.17     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 528000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000165 |\n",
      "|    n_updates        | 131749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.43    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1417     |\n",
      "|    total_timesteps  | 528000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=3.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 529000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000612 |\n",
      "|    n_updates        | 131999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-3.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.74    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 530000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00163  |\n",
      "|    n_updates        | 132249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=4.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 531000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000363 |\n",
      "|    n_updates        | 132499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-0.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.263   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 532000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000419 |\n",
      "|    n_updates        | 132749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.15    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1428     |\n",
      "|    total_timesteps  | 532000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=5.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 533000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000623 |\n",
      "|    n_updates        | 132999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-4.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.55    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 534000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000199 |\n",
      "|    n_updates        | 133249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-0.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.756   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 535000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000315 |\n",
      "|    n_updates        | 133499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=2.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.74     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 536000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000379 |\n",
      "|    n_updates        | 133749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1439     |\n",
      "|    total_timesteps  | 536000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=-21.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -21.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 537000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000548 |\n",
      "|    n_updates        | 133999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=3.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 538000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000491 |\n",
      "|    n_updates        | 134249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=1.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 539000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000425 |\n",
      "|    n_updates        | 134499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 540000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00131  |\n",
      "|    n_updates        | 134749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -4       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1449     |\n",
      "|    total_timesteps  | 540000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=-0.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.248   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 541000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000298 |\n",
      "|    n_updates        | 134999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=3.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.94     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 542000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000181 |\n",
      "|    n_updates        | 135249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=0.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.866    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 543000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000284 |\n",
      "|    n_updates        | 135499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=4.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 544000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000278 |\n",
      "|    n_updates        | 135749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1460     |\n",
      "|    total_timesteps  | 544000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-4.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.76    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 545000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000526 |\n",
      "|    n_updates        | 135999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=2.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 546000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000325 |\n",
      "|    n_updates        | 136249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=4.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.01     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 547000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000362 |\n",
      "|    n_updates        | 136499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-9.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -9.76    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 548000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000193 |\n",
      "|    n_updates        | 136749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.51    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1472     |\n",
      "|    total_timesteps  | 548000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 549000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000529 |\n",
      "|    n_updates        | 136999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000308 |\n",
      "|    n_updates        | 137249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=1.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.82     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 551000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000211 |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 552000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000338 |\n",
      "|    n_updates        | 137749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.44    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1482     |\n",
      "|    total_timesteps  | 552000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=553000, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.821    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 553000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00135  |\n",
      "|    n_updates        | 137999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 554000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000593 |\n",
      "|    n_updates        | 138249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=3.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 555000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000491 |\n",
      "|    n_updates        | 138499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 556000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00057  |\n",
      "|    n_updates        | 138749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.21    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1493     |\n",
      "|    total_timesteps  | 556000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 557000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000984 |\n",
      "|    n_updates        | 138999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=2.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.93     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 558000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000298 |\n",
      "|    n_updates        | 139249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=-3.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.99    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 559000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000498 |\n",
      "|    n_updates        | 139499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 560000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000912 |\n",
      "|    n_updates        | 139749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.41    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1503     |\n",
      "|    total_timesteps  | 560000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=3.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.99     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 561000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000298 |\n",
      "|    n_updates        | 139999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=-3.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.02    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 562000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000171 |\n",
      "|    n_updates        | 140249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 563000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00132  |\n",
      "|    n_updates        | 140499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 564000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000939 |\n",
      "|    n_updates        | 140749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1514     |\n",
      "|    total_timesteps  | 564000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=-0.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.992   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 565000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000198 |\n",
      "|    n_updates        | 140999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=3.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 566000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000659 |\n",
      "|    n_updates        | 141249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=0.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.374    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 567000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000716 |\n",
      "|    n_updates        | 141499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-14.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -14.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 568000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000436 |\n",
      "|    n_updates        | 141749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1524     |\n",
      "|    total_timesteps  | 568000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=3.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.59     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 569000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000272 |\n",
      "|    n_updates        | 141999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=4.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 570000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000415 |\n",
      "|    n_updates        | 142249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=-7.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.65    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 571000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000415 |\n",
      "|    n_updates        | 142499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=2.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 572000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000201 |\n",
      "|    n_updates        | 142749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1535     |\n",
      "|    total_timesteps  | 572000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=-1.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.85    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 573000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000454 |\n",
      "|    n_updates        | 142999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=2.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 574000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000217 |\n",
      "|    n_updates        | 143249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-2.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.94    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 575000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00105  |\n",
      "|    n_updates        | 143499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=0.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.952    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 576000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000506 |\n",
      "|    n_updates        | 143749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.07    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1546     |\n",
      "|    total_timesteps  | 576000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=4.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.55     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 577000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000207 |\n",
      "|    n_updates        | 143999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.597   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 578000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000552 |\n",
      "|    n_updates        | 144249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=3.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 579000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 144499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-2.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.12    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 580000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.24e-05 |\n",
      "|    n_updates        | 144749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1557     |\n",
      "|    total_timesteps  | 580000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=4.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 581000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0035   |\n",
      "|    n_updates        | 144999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=5.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.81     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 582000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000286 |\n",
      "|    n_updates        | 145249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=-12.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -12.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 583000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.78e-05 |\n",
      "|    n_updates        | 145499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-6.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -6.69    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 584000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000277 |\n",
      "|    n_updates        | 145749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1568     |\n",
      "|    total_timesteps  | 584000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=4.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 585000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000131 |\n",
      "|    n_updates        | 145999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=3.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 586000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00113  |\n",
      "|    n_updates        | 146249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=587000, episode_reward=0.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.539    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 587000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000438 |\n",
      "|    n_updates        | 146499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.293   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 588000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000235 |\n",
      "|    n_updates        | 146749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -3.01    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1579     |\n",
      "|    total_timesteps  | 588000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=2.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 589000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000572 |\n",
      "|    n_updates        | 146999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-5.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.71    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 590000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000423 |\n",
      "|    n_updates        | 147249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=4.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.9      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 591000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.024    |\n",
      "|    n_updates        | 147499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-3.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.26    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 592000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00051  |\n",
      "|    n_updates        | 147749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.62    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 372      |\n",
      "|    time_elapsed     | 1589     |\n",
      "|    total_timesteps  | 592000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=5.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 593000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 147999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=3.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 594000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00299  |\n",
      "|    n_updates        | 148249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-1.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -1.97    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 595000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000458 |\n",
      "|    n_updates        | 148499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=4.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.96     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 596000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000543 |\n",
      "|    n_updates        | 148749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.69    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 324      |\n",
      "|    time_elapsed     | 1836     |\n",
      "|    total_timesteps  | 596000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=1.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.49     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 597000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000453 |\n",
      "|    n_updates        | 148999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=1.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 598000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000404 |\n",
      "|    n_updates        | 149249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=5.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.43     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 599000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000773 |\n",
      "|    n_updates        | 149499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=2.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00175  |\n",
      "|    n_updates        | 149749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.77    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 324      |\n",
      "|    time_elapsed     | 1846     |\n",
      "|    total_timesteps  | 600000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=0.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.268    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 601000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000501 |\n",
      "|    n_updates        | 149999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.621    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 602000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0765   |\n",
      "|    n_updates        | 150249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=3.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 603000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00115  |\n",
      "|    n_updates        | 150499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=5.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.21     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 604000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.91e-05 |\n",
      "|    n_updates        | 150749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.79    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 325      |\n",
      "|    time_elapsed     | 1856     |\n",
      "|    total_timesteps  | 604000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=4.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 605000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000382 |\n",
      "|    n_updates        | 150999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=4.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 606000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 151249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 607000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0018   |\n",
      "|    n_updates        | 151499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=4.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.38     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 608000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000193 |\n",
      "|    n_updates        | 151749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.81    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 325      |\n",
      "|    time_elapsed     | 1866     |\n",
      "|    total_timesteps  | 608000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=2.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.95     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 609000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000714 |\n",
      "|    n_updates        | 151999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=1.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 610000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000342 |\n",
      "|    n_updates        | 152249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 611000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00109  |\n",
      "|    n_updates        | 152499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=2.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 612000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 152749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.83    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 326      |\n",
      "|    time_elapsed     | 1876     |\n",
      "|    total_timesteps  | 612000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=-2.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -2.89    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 613000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00069  |\n",
      "|    n_updates        | 152999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=-3.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 614000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000314 |\n",
      "|    n_updates        | 153249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=5.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.3      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 615000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00128  |\n",
      "|    n_updates        | 153499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=5.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 616000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000197 |\n",
      "|    n_updates        | 153749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.73    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 208      |\n",
      "|    time_elapsed     | 2950     |\n",
      "|    total_timesteps  | 616000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=3.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 617000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000155 |\n",
      "|    n_updates        | 153999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=4.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.53     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 618000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 154249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=2.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 619000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 154499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-0.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.842   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 620000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000237 |\n",
      "|    n_updates        | 154749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 209      |\n",
      "|    time_elapsed     | 2960     |\n",
      "|    total_timesteps  | 620000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=2.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.72     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 621000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0012   |\n",
      "|    n_updates        | 154999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=3.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.94     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 622000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000519 |\n",
      "|    n_updates        | 155249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=-0.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.962   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 623000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000204 |\n",
      "|    n_updates        | 155499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=2.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 624000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000709 |\n",
      "|    n_updates        | 155749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 210      |\n",
      "|    time_elapsed     | 2971     |\n",
      "|    total_timesteps  | 624000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=3.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 625000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000185 |\n",
      "|    n_updates        | 155999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.997    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 626000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00053  |\n",
      "|    n_updates        | 156249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 627000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000522 |\n",
      "|    n_updates        | 156499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=3.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.52     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 628000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000506 |\n",
      "|    n_updates        | 156749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -1.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 210      |\n",
      "|    time_elapsed     | 2981     |\n",
      "|    total_timesteps  | 628000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=2.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.91     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 629000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000193 |\n",
      "|    n_updates        | 156999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=1.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 630000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000359 |\n",
      "|    n_updates        | 157249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 631000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000411 |\n",
      "|    n_updates        | 157499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.742    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 632000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00207  |\n",
      "|    n_updates        | 157749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 161      |\n",
      "|    time_elapsed     | 3911     |\n",
      "|    total_timesteps  | 632000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=3.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 633000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00456  |\n",
      "|    n_updates        | 157999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=3.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.45     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 634000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000559 |\n",
      "|    n_updates        | 158249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.0206  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 635000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00048  |\n",
      "|    n_updates        | 158499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=3.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.74     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 636000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000607 |\n",
      "|    n_updates        | 158749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 162      |\n",
      "|    time_elapsed     | 3921     |\n",
      "|    total_timesteps  | 636000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=-362.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -363     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 637000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000467 |\n",
      "|    n_updates        | 158999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 638000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00024  |\n",
      "|    n_updates        | 159249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=2.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.97     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 639000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000703 |\n",
      "|    n_updates        | 159499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=1.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 640000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000168 |\n",
      "|    n_updates        | 159749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.12    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 162      |\n",
      "|    time_elapsed     | 3931     |\n",
      "|    total_timesteps  | 640000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=3.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 641000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00048  |\n",
      "|    n_updates        | 159999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=-7.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -7.58    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 642000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000986 |\n",
      "|    n_updates        | 160249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=3.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 643000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000239 |\n",
      "|    n_updates        | 160499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.639    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 644000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000523 |\n",
      "|    n_updates        | 160749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -2.08    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 3941     |\n",
      "|    total_timesteps  | 644000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.96     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 645000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000565 |\n",
      "|    n_updates        | 160999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=-3.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.72    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 646000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00104  |\n",
      "|    n_updates        | 161249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=-5.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.68    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 647000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00174  |\n",
      "|    n_updates        | 161499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-11.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -11.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 648000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000607 |\n",
      "|    n_updates        | 161749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -1.51    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 163      |\n",
      "|    time_elapsed     | 3951     |\n",
      "|    total_timesteps  | 648000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=4.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 649000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000893 |\n",
      "|    n_updates        | 161999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=4.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 162249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=1.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 651000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000197 |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=3.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 652000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000484 |\n",
      "|    n_updates        | 162749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -1.52    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 133      |\n",
      "|    time_elapsed     | 4878     |\n",
      "|    total_timesteps  | 652000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=1.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 653000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.32e-05 |\n",
      "|    n_updates        | 162999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=3.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 654000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00174  |\n",
      "|    n_updates        | 163249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=3.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 655000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000141 |\n",
      "|    n_updates        | 163499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-17.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -17.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 656000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000979 |\n",
      "|    n_updates        | 163749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 4891     |\n",
      "|    total_timesteps  | 656000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=5.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.61     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 657000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000699 |\n",
      "|    n_updates        | 163999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=3.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 658000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000293 |\n",
      "|    n_updates        | 164249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=2.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 659000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000681 |\n",
      "|    n_updates        | 164499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=3.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 660000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000374 |\n",
      "|    n_updates        | 164749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -0.775   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 134      |\n",
      "|    time_elapsed     | 4902     |\n",
      "|    total_timesteps  | 660000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=5.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 661000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00296  |\n",
      "|    n_updates        | 164999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-4.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.14    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 662000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0078   |\n",
      "|    n_updates        | 165249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=4.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 663000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000475 |\n",
      "|    n_updates        | 165499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=2.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 664000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00277  |\n",
      "|    n_updates        | 165749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -0.652   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4914     |\n",
      "|    total_timesteps  | 664000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=2.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.01     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 665000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0022   |\n",
      "|    n_updates        | 165999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=3.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.49     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 666000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00038  |\n",
      "|    n_updates        | 166249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=4.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 667000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000831 |\n",
      "|    n_updates        | 166499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=3.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 668000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000542 |\n",
      "|    n_updates        | 166749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -0.496   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 135      |\n",
      "|    time_elapsed     | 4925     |\n",
      "|    total_timesteps  | 668000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 669000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000344 |\n",
      "|    n_updates        | 166999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=5.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.23     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 670000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000281 |\n",
      "|    n_updates        | 167249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=3.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.52     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 671000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00014  |\n",
      "|    n_updates        | 167499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=3.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.03     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 672000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000404 |\n",
      "|    n_updates        | 167749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -0.245   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 4936     |\n",
      "|    total_timesteps  | 672000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=4.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.67     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 673000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000125 |\n",
      "|    n_updates        | 167999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=5.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 674000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00168  |\n",
      "|    n_updates        | 168249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=5.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 675000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000169 |\n",
      "|    n_updates        | 168499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=5.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.41     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 676000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000394 |\n",
      "|    n_updates        | 168749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.0176   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 123      |\n",
      "|    time_elapsed     | 5456     |\n",
      "|    total_timesteps  | 676000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=677000, episode_reward=4.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 677000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00042  |\n",
      "|    n_updates        | 168999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=4.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 678000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.96e-05 |\n",
      "|    n_updates        | 169249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=4.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 679000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000414 |\n",
      "|    n_updates        | 169499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=3.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 680000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000227 |\n",
      "|    n_updates        | 169749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.167    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 124      |\n",
      "|    time_elapsed     | 5467     |\n",
      "|    total_timesteps  | 680000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=5.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.43     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 681000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000316 |\n",
      "|    n_updates        | 169999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=4.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 682000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 170249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=4.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.49     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 683000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 170499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=3.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 684000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000206 |\n",
      "|    n_updates        | 170749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.328    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 124      |\n",
      "|    time_elapsed     | 5477     |\n",
      "|    total_timesteps  | 684000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=5.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.3      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 685000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000422 |\n",
      "|    n_updates        | 170999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=5.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 686000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000643 |\n",
      "|    n_updates        | 171249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=2.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.65     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 687000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00123  |\n",
      "|    n_updates        | 171499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=2.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.15     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 688000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.7e-05  |\n",
      "|    n_updates        | 171749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.527    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 125      |\n",
      "|    time_elapsed     | 5487     |\n",
      "|    total_timesteps  | 688000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=5.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 689000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 171999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=4.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.98     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 690000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000715 |\n",
      "|    n_updates        | 172249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=4.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 691000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000398 |\n",
      "|    n_updates        | 172499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=5.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.42     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 692000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00247  |\n",
      "|    n_updates        | 172749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 6537     |\n",
      "|    total_timesteps  | 692000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=1.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 693000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000561 |\n",
      "|    n_updates        | 172999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=6.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 694000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000129 |\n",
      "|    n_updates        | 173249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=5.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.38     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 695000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000958 |\n",
      "|    n_updates        | 173499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=5.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 696000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00238  |\n",
      "|    n_updates        | 173749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.928    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 6548     |\n",
      "|    total_timesteps  | 696000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=4.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 697000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000287 |\n",
      "|    n_updates        | 173999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=4.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.38     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 698000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000265 |\n",
      "|    n_updates        | 174249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=2.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 699000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000724 |\n",
      "|    n_updates        | 174499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00056  |\n",
      "|    n_updates        | 174749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.15     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 6558     |\n",
      "|    total_timesteps  | 700000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.33     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 701000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000872 |\n",
      "|    n_updates        | 174999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=5.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 702000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 175249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=703000, episode_reward=5.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 703000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00204  |\n",
      "|    n_updates        | 175499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=5.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.78     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 704000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.34e-05 |\n",
      "|    n_updates        | 175749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.31     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 107      |\n",
      "|    time_elapsed     | 6568     |\n",
      "|    total_timesteps  | 704000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=3.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 705000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.51e-05 |\n",
      "|    n_updates        | 175999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.26     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 706000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000771 |\n",
      "|    n_updates        | 176249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 707000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00138  |\n",
      "|    n_updates        | 176499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=5.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.53     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 708000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0002   |\n",
      "|    n_updates        | 176749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.44     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 107      |\n",
      "|    time_elapsed     | 6579     |\n",
      "|    total_timesteps  | 708000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=4.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 709000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000678 |\n",
      "|    n_updates        | 176999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=0.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.905    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 710000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000332 |\n",
      "|    n_updates        | 177249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=5.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 711000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000213 |\n",
      "|    n_updates        | 177499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 712000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000186 |\n",
      "|    n_updates        | 177749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.6      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 6589     |\n",
      "|    total_timesteps  | 712000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=5.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.06     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 713000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000171 |\n",
      "|    n_updates        | 177999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 714000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000169 |\n",
      "|    n_updates        | 178249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=5.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.02     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 715000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000203 |\n",
      "|    n_updates        | 178499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=4.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.93     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 716000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000147 |\n",
      "|    n_updates        | 178749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.73     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 6599     |\n",
      "|    total_timesteps  | 716000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=5.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 717000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000131 |\n",
      "|    n_updates        | 178999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=3.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 718000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00162  |\n",
      "|    n_updates        | 179249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=-378.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -379     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 719000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000288 |\n",
      "|    n_updates        | 179499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=5.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 720000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00029  |\n",
      "|    n_updates        | 179749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.899    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 108      |\n",
      "|    time_elapsed     | 6609     |\n",
      "|    total_timesteps  | 720000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=2.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.91     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 721000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000276 |\n",
      "|    n_updates        | 179999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=5.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 722000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000187 |\n",
      "|    n_updates        | 180249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=5.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 723000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000414 |\n",
      "|    n_updates        | 180499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=5.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.62     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 724000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000782 |\n",
      "|    n_updates        | 180749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 109      |\n",
      "|    time_elapsed     | 6620     |\n",
      "|    total_timesteps  | 724000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=5.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 725000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000177 |\n",
      "|    n_updates        | 180999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=5.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.97     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 726000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00025  |\n",
      "|    n_updates        | 181249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=3.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 727000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000333 |\n",
      "|    n_updates        | 181499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 728000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000241 |\n",
      "|    n_updates        | 181749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.792    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 109      |\n",
      "|    time_elapsed     | 6630     |\n",
      "|    total_timesteps  | 728000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=5.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.06     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 729000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.7e-05  |\n",
      "|    n_updates        | 181999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=4.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 730000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000616 |\n",
      "|    n_updates        | 182249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=4.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.06     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 731000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.94e-05 |\n",
      "|    n_updates        | 182499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=5.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 732000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000219 |\n",
      "|    n_updates        | 182749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.735    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 6640     |\n",
      "|    total_timesteps  | 732000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=5.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 733000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000548 |\n",
      "|    n_updates        | 182999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=5.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 734000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000907 |\n",
      "|    n_updates        | 183249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=4.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 735000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.1e-05  |\n",
      "|    n_updates        | 183499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 736000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000245 |\n",
      "|    n_updates        | 183749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.944    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 6650     |\n",
      "|    total_timesteps  | 736000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=5.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 737000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000235 |\n",
      "|    n_updates        | 183999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=5.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 738000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000527 |\n",
      "|    n_updates        | 184249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=1.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.61     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 739000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.54e-05 |\n",
      "|    n_updates        | 184499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=4.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.52     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 740000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 184749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 111      |\n",
      "|    time_elapsed     | 6661     |\n",
      "|    total_timesteps  | 740000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=3.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 741000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.46e-05 |\n",
      "|    n_updates        | 184999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=4.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.04     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 742000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.6e-05  |\n",
      "|    n_updates        | 185249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 743000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.65e-05 |\n",
      "|    n_updates        | 185499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=5.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 744000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00699  |\n",
      "|    n_updates        | 185749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.48     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 111      |\n",
      "|    time_elapsed     | 6671     |\n",
      "|    total_timesteps  | 744000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=3.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 745000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00017  |\n",
      "|    n_updates        | 185999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 746000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000765 |\n",
      "|    n_updates        | 186249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=5.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 747000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.06e-05 |\n",
      "|    n_updates        | 186499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=5.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 748000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000342 |\n",
      "|    n_updates        | 186749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.66     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 111      |\n",
      "|    time_elapsed     | 6681     |\n",
      "|    total_timesteps  | 748000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=1.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 749000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000433 |\n",
      "|    n_updates        | 186999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=5.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.84     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.73e-05 |\n",
      "|    n_updates        | 187249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=5.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 751000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=4.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 752000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000264 |\n",
      "|    n_updates        | 187749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.77     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 112      |\n",
      "|    time_elapsed     | 6692     |\n",
      "|    total_timesteps  | 752000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.67     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 753000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 187999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=3.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.39     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 754000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000164 |\n",
      "|    n_updates        | 188249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=2.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 755000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00138  |\n",
      "|    n_updates        | 188499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=5.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 756000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.34e-05 |\n",
      "|    n_updates        | 188749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.89     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 112      |\n",
      "|    time_elapsed     | 6703     |\n",
      "|    total_timesteps  | 756000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=757000, episode_reward=5.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 757000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00101  |\n",
      "|    n_updates        | 188999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=5.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.73     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 758000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000204 |\n",
      "|    n_updates        | 189249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 759000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.67e-05 |\n",
      "|    n_updates        | 189499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=5.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.41     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 760000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.16e-05 |\n",
      "|    n_updates        | 189749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.91     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 113      |\n",
      "|    time_elapsed     | 6714     |\n",
      "|    total_timesteps  | 760000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=5.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 761000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.56e-05 |\n",
      "|    n_updates        | 189999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=4.26 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.26     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 762000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000154 |\n",
      "|    n_updates        | 190249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=3.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 763000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000142 |\n",
      "|    n_updates        | 190499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=4.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.43     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 764000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00146  |\n",
      "|    n_updates        | 190749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.92     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 113      |\n",
      "|    time_elapsed     | 6725     |\n",
      "|    total_timesteps  | 764000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=4.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 765000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.33e-05 |\n",
      "|    n_updates        | 190999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=5.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.03     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 766000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.74e-05 |\n",
      "|    n_updates        | 191249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=5.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.43     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 767000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000235 |\n",
      "|    n_updates        | 191499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=4.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 768000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.9e-05  |\n",
      "|    n_updates        | 191749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.94     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 114      |\n",
      "|    time_elapsed     | 6736     |\n",
      "|    total_timesteps  | 768000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=769000, episode_reward=-0.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.587   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 769000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000119 |\n",
      "|    n_updates        | 191999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=4.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 770000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.09e-05 |\n",
      "|    n_updates        | 192249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=1.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 771000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00222  |\n",
      "|    n_updates        | 192499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=3.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 772000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.95e-05 |\n",
      "|    n_updates        | 192749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.327    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 114      |\n",
      "|    time_elapsed     | 6747     |\n",
      "|    total_timesteps  | 772000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=3.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 773000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00027  |\n",
      "|    n_updates        | 192999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=3.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.38     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 774000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00014  |\n",
      "|    n_updates        | 193249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=3.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 775000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000465 |\n",
      "|    n_updates        | 193499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 776000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.7e-05  |\n",
      "|    n_updates        | 193749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.303    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 114      |\n",
      "|    time_elapsed     | 6757     |\n",
      "|    total_timesteps  | 776000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=4.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.95     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 777000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000281 |\n",
      "|    n_updates        | 193999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=4.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 778000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000145 |\n",
      "|    n_updates        | 194249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=5.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 779000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.03e-05 |\n",
      "|    n_updates        | 194499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=3.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.81     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 780000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 194749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.282    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 115      |\n",
      "|    time_elapsed     | 6767     |\n",
      "|    total_timesteps  | 780000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=5.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 781000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000253 |\n",
      "|    n_updates        | 194999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=4.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.88     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 782000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000495 |\n",
      "|    n_updates        | 195249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=-4.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -4.36    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 783000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.12e-05 |\n",
      "|    n_updates        | 195499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=3.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 784000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000399 |\n",
      "|    n_updates        | 195749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.273    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 115      |\n",
      "|    time_elapsed     | 6778     |\n",
      "|    total_timesteps  | 784000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-3.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -3.69    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 785000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000263 |\n",
      "|    n_updates        | 195999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=5.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.45     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 786000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.28e-05 |\n",
      "|    n_updates        | 196249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=5.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 787000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000478 |\n",
      "|    n_updates        | 196499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=5.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 788000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000853 |\n",
      "|    n_updates        | 196749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.284    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 116      |\n",
      "|    time_elapsed     | 6788     |\n",
      "|    total_timesteps  | 788000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=5.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 789000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00069  |\n",
      "|    n_updates        | 196999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=4.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.95     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 790000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000806 |\n",
      "|    n_updates        | 197249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=2.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.33     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 791000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00286  |\n",
      "|    n_updates        | 197499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 792000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000117 |\n",
      "|    n_updates        | 197749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.292    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 116      |\n",
      "|    time_elapsed     | 6798     |\n",
      "|    total_timesteps  | 792000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=5.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 793000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000216 |\n",
      "|    n_updates        | 197999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=5.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 794000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.16e-05 |\n",
      "|    n_updates        | 198249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=4.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 795000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.45e-05 |\n",
      "|    n_updates        | 198499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.93     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 796000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.14e-05 |\n",
      "|    n_updates        | 198749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.303    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 116      |\n",
      "|    time_elapsed     | 6808     |\n",
      "|    total_timesteps  | 796000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=5.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 797000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00154  |\n",
      "|    n_updates        | 198999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=4.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 798000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000591 |\n",
      "|    n_updates        | 199249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=5.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 799000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000229 |\n",
      "|    n_updates        | 199499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=3.40 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.4      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000308 |\n",
      "|    n_updates        | 199749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.251    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 7823     |\n",
      "|    total_timesteps  | 800000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=4.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 801000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.73e-05 |\n",
      "|    n_updates        | 199999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 802000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000135 |\n",
      "|    n_updates        | 200249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=803000, episode_reward=1.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 803000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000129 |\n",
      "|    n_updates        | 200499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 804000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000249 |\n",
      "|    n_updates        | 200749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.196    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 102      |\n",
      "|    time_elapsed     | 7834     |\n",
      "|    total_timesteps  | 804000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 805000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.68e-05 |\n",
      "|    n_updates        | 200999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=4.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.56     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 806000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000189 |\n",
      "|    n_updates        | 201249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=5.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.31     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 807000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000712 |\n",
      "|    n_updates        | 201499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=5.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 808000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00624  |\n",
      "|    n_updates        | 201749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.197    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 7844     |\n",
      "|    total_timesteps  | 808000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=5.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.73     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 809000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.64e-05 |\n",
      "|    n_updates        | 201999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=5.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 810000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00012  |\n",
      "|    n_updates        | 202249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=5.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.74     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 811000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.95e-05 |\n",
      "|    n_updates        | 202499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=5.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.78     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 812000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000149 |\n",
      "|    n_updates        | 202749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.217    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 7854     |\n",
      "|    total_timesteps  | 812000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=5.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 813000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000362 |\n",
      "|    n_updates        | 202999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=4.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 814000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 203249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=4.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.65     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 815000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000264 |\n",
      "|    n_updates        | 203499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=2.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 816000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000321 |\n",
      "|    n_updates        | 203749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.172    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 103      |\n",
      "|    time_elapsed     | 7865     |\n",
      "|    total_timesteps  | 816000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=5.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 817000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 203999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=2.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 818000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.22e-05 |\n",
      "|    n_updates        | 204249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=-381.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 819000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.55e-05 |\n",
      "|    n_updates        | 204499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=5.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 820000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000119 |\n",
      "|    n_updates        | 204749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.145    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 104      |\n",
      "|    time_elapsed     | 7875     |\n",
      "|    total_timesteps  | 820000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 821000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000824 |\n",
      "|    n_updates        | 204999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 822000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.54e-05 |\n",
      "|    n_updates        | 205249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 823000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00515  |\n",
      "|    n_updates        | 205499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 824000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000584 |\n",
      "|    n_updates        | 205749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.129    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 104      |\n",
      "|    time_elapsed     | 7885     |\n",
      "|    total_timesteps  | 824000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=5.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 825000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000139 |\n",
      "|    n_updates        | 205999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=5.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.84     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 826000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000203 |\n",
      "|    n_updates        | 206249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 827000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.53e-05 |\n",
      "|    n_updates        | 206499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 828000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.46e-05 |\n",
      "|    n_updates        | 206749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.577    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 104      |\n",
      "|    time_elapsed     | 7896     |\n",
      "|    total_timesteps  | 828000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.99     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 829000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000142 |\n",
      "|    n_updates        | 206999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=5.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.92     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 830000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 207249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=6.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 831000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.07e-05 |\n",
      "|    n_updates        | 207499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=5.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 832000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 207749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 0.997    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 7906     |\n",
      "|    total_timesteps  | 832000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=5.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 833000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00343  |\n",
      "|    n_updates        | 207999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=6.10 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 834000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000551 |\n",
      "|    n_updates        | 208249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=5.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 835000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000255 |\n",
      "|    n_updates        | 208499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=4.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.79     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 836000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.43e-05 |\n",
      "|    n_updates        | 208749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 7916     |\n",
      "|    total_timesteps  | 836000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=5.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.53     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 837000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000976 |\n",
      "|    n_updates        | 208999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=5.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 838000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00072  |\n",
      "|    n_updates        | 209249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=5.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.9      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 839000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 209499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 840000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.7e-05  |\n",
      "|    n_updates        | 209749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.12     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 105      |\n",
      "|    time_elapsed     | 7926     |\n",
      "|    total_timesteps  | 840000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=5.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.73     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 841000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000528 |\n",
      "|    n_updates        | 209999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-381.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -382     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 842000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00024  |\n",
      "|    n_updates        | 210249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=5.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 843000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000517 |\n",
      "|    n_updates        | 210499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=5.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.61     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 844000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.77e-05 |\n",
      "|    n_updates        | 210749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.06     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 7937     |\n",
      "|    total_timesteps  | 844000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 845000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 210999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 846000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00159  |\n",
      "|    n_updates        | 211249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=5.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 847000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000121 |\n",
      "|    n_updates        | 211499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=5.54 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.54     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 848000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.08e-05 |\n",
      "|    n_updates        | 211749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 7947     |\n",
      "|    total_timesteps  | 848000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=5.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.44     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 849000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000333 |\n",
      "|    n_updates        | 211999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=5.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.62     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.75e-05 |\n",
      "|    n_updates        | 212249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 851000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000136 |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=5.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 852000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.92e-05 |\n",
      "|    n_updates        | 212749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 96       |\n",
      "|    time_elapsed     | 8856     |\n",
      "|    total_timesteps  | 852000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=5.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 853000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.71e-05 |\n",
      "|    n_updates        | 212999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=5.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 854000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000394 |\n",
      "|    n_updates        | 213249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=5.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 855000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000314 |\n",
      "|    n_updates        | 213499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=5.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.62     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 856000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000163 |\n",
      "|    n_updates        | 213749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.21     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 96       |\n",
      "|    time_elapsed     | 8867     |\n",
      "|    total_timesteps  | 856000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=5.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 857000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.77e-05 |\n",
      "|    n_updates        | 213999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=5.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.47     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 858000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00137  |\n",
      "|    n_updates        | 214249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=5.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 859000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 214499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=5.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.78     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 860000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8e-05    |\n",
      "|    n_updates        | 214749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.23     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 96       |\n",
      "|    time_elapsed     | 8877     |\n",
      "|    total_timesteps  | 860000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.7      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 861000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.41e-05 |\n",
      "|    n_updates        | 214999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=5.50 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 862000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.31e-05 |\n",
      "|    n_updates        | 215249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=5.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.88     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 863000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.69e-05 |\n",
      "|    n_updates        | 215499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=5.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 864000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.69e-05 |\n",
      "|    n_updates        | 215749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.27     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 8887     |\n",
      "|    total_timesteps  | 864000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 865000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.16e-05 |\n",
      "|    n_updates        | 215999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=5.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.41     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 866000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000151 |\n",
      "|    n_updates        | 216249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=5.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.82     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 867000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000472 |\n",
      "|    n_updates        | 216499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.33     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 868000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000935 |\n",
      "|    n_updates        | 216749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 1.33     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 8897     |\n",
      "|    total_timesteps  | 868000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=5.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 869000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000231 |\n",
      "|    n_updates        | 216999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=5.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 870000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.46e-05 |\n",
      "|    n_updates        | 217249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=871000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.99     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 871000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00043  |\n",
      "|    n_updates        | 217499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=5.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.81     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 872000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000237 |\n",
      "|    n_updates        | 217749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 2.98     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 8908     |\n",
      "|    total_timesteps  | 872000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 873000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.79e-05 |\n",
      "|    n_updates        | 217999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=5.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 874000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0001   |\n",
      "|    n_updates        | 218249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=5.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 875000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000105 |\n",
      "|    n_updates        | 218499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=5.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 876000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000285 |\n",
      "|    n_updates        | 218749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 8918     |\n",
      "|    total_timesteps  | 876000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=5.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.76     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 877000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 218999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=5.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 878000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.87e-05 |\n",
      "|    n_updates        | 219249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 879000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000103 |\n",
      "|    n_updates        | 219499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.67     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 880000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.46e-05 |\n",
      "|    n_updates        | 219749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 8928     |\n",
      "|    total_timesteps  | 880000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 881000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.71e-05 |\n",
      "|    n_updates        | 219999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=5.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.9      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 882000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000205 |\n",
      "|    n_updates        | 220249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=5.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.72     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 883000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00264  |\n",
      "|    n_updates        | 220499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.93     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 884000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00135  |\n",
      "|    n_updates        | 220749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 97       |\n",
      "|    time_elapsed     | 9042     |\n",
      "|    total_timesteps  | 884000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=5.86 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.86     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 885000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.43e-05 |\n",
      "|    n_updates        | 220999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 886000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.88e-05 |\n",
      "|    n_updates        | 221249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.8      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 887000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.2e-05  |\n",
      "|    n_updates        | 221499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=-381.02 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -381     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 888000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.3e-05  |\n",
      "|    n_updates        | 221749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.2      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 9052     |\n",
      "|    total_timesteps  | 888000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 889000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.09e-05 |\n",
      "|    n_updates        | 221999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=5.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 890000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.68e-05 |\n",
      "|    n_updates        | 222249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=1.78 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.78     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 891000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.71e-05 |\n",
      "|    n_updates        | 222499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.67     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 892000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 222749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.26     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 9063     |\n",
      "|    total_timesteps  | 892000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=893000, episode_reward=4.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.19     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 893000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 222999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=6.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 894000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000652 |\n",
      "|    n_updates        | 223249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=5.65 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.65     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 895000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.53e-05 |\n",
      "|    n_updates        | 223499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=4.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.85     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 896000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00191  |\n",
      "|    n_updates        | 223749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.3      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 98       |\n",
      "|    time_elapsed     | 9073     |\n",
      "|    total_timesteps  | 896000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=4.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 897000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.01e-05 |\n",
      "|    n_updates        | 223999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=4.59 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.59     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 898000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00026  |\n",
      "|    n_updates        | 224249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=4.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.42     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 899000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.48e-05 |\n",
      "|    n_updates        | 224499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=2.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.82     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000106 |\n",
      "|    n_updates        | 224749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.28     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 89       |\n",
      "|    time_elapsed     | 10060    |\n",
      "|    total_timesteps  | 900000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.67     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 901000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.13e-05 |\n",
      "|    n_updates        | 224999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=5.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.95     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 902000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00123  |\n",
      "|    n_updates        | 225249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 903000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.82e-05 |\n",
      "|    n_updates        | 225499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=5.88 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.88     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 904000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 225749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.37     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 89       |\n",
      "|    time_elapsed     | 10070    |\n",
      "|    total_timesteps  | 904000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=5.87 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.87     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 905000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.002    |\n",
      "|    n_updates        | 225999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.6      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 906000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000151 |\n",
      "|    n_updates        | 226249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=5.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 907000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.91e-05 |\n",
      "|    n_updates        | 226499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=4.93 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.93     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 908000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000344 |\n",
      "|    n_updates        | 226749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.42     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 10080    |\n",
      "|    total_timesteps  | 908000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=6.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.21     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 909000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.85e-05 |\n",
      "|    n_updates        | 226999   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=910000, episode_reward=6.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 910000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.37e-05 |\n",
      "|    n_updates        | 227249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=6.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 911000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00535  |\n",
      "|    n_updates        | 227499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 912000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000983 |\n",
      "|    n_updates        | 227749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.43     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 10090    |\n",
      "|    total_timesteps  | 912000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=5.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.82     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 913000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.41e-05 |\n",
      "|    n_updates        | 227999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=5.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 914000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.05e-05 |\n",
      "|    n_updates        | 228249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 915000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00018  |\n",
      "|    n_updates        | 228499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=4.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.9      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 916000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.55e-05 |\n",
      "|    n_updates        | 228749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 3.52     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 10101    |\n",
      "|    total_timesteps  | 916000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=5.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 917000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.38e-05 |\n",
      "|    n_updates        | 228999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=5.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.72     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 918000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000108 |\n",
      "|    n_updates        | 229249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=2.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 919000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.62e-05 |\n",
      "|    n_updates        | 229499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=4.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 920000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000168 |\n",
      "|    n_updates        | 229749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.53     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 90       |\n",
      "|    time_elapsed     | 10111    |\n",
      "|    total_timesteps  | 920000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 921000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.72e-05 |\n",
      "|    n_updates        | 229999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=5.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.84     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 922000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.74e-05 |\n",
      "|    n_updates        | 230249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=-5.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -5.03    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 923000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000132 |\n",
      "|    n_updates        | 230499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -0.0734  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 924000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000222 |\n",
      "|    n_updates        | 230749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.42     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 91       |\n",
      "|    time_elapsed     | 10121    |\n",
      "|    total_timesteps  | 924000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=1.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 1.63     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 925000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000128 |\n",
      "|    n_updates        | 230999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=5.61 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.61     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 926000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000107 |\n",
      "|    n_updates        | 231249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=927000, episode_reward=5.77 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.77     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 927000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.75e-05 |\n",
      "|    n_updates        | 231499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=3.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.31     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 928000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.97e-05 |\n",
      "|    n_updates        | 231749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.41     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 84       |\n",
      "|    time_elapsed     | 11035    |\n",
      "|    total_timesteps  | 928000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.661    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 929000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.1e-05  |\n",
      "|    n_updates        | 231999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=4.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.66     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 930000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.42e-05 |\n",
      "|    n_updates        | 232249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=0.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.374    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 931000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.1e-05  |\n",
      "|    n_updates        | 232499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 932000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.05e-05 |\n",
      "|    n_updates        | 232749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 84       |\n",
      "|    time_elapsed     | 11045    |\n",
      "|    total_timesteps  | 932000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.98     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 933000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.07e-05 |\n",
      "|    n_updates        | 232999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=5.94 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.94     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 934000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.2e-05  |\n",
      "|    n_updates        | 233249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=4.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 935000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.68e-05 |\n",
      "|    n_updates        | 233499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=2.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 2.91     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 936000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.29e-05 |\n",
      "|    n_updates        | 233749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 84       |\n",
      "|    time_elapsed     | 11056    |\n",
      "|    total_timesteps  | 936000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 937000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.27e-05 |\n",
      "|    n_updates        | 233999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=5.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 938000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000244 |\n",
      "|    n_updates        | 234249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=5.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.41     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 939000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.01e-05 |\n",
      "|    n_updates        | 234499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=6.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 940000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.21e-05 |\n",
      "|    n_updates        | 234749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.05     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 84       |\n",
      "|    time_elapsed     | 11066    |\n",
      "|    total_timesteps  | 940000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=6.19 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.19     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 941000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000255 |\n",
      "|    n_updates        | 234999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=5.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 942000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.68e-05 |\n",
      "|    n_updates        | 235249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=6.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 943000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.14e-05 |\n",
      "|    n_updates        | 235499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=6.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.2      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 944000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00211  |\n",
      "|    n_updates        | 235749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.2      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 85       |\n",
      "|    time_elapsed     | 11076    |\n",
      "|    total_timesteps  | 944000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=4.01 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.01     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 945000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.34e-05 |\n",
      "|    n_updates        | 235999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=4.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 946000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.55e-05 |\n",
      "|    n_updates        | 236249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=4.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 947000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000268 |\n",
      "|    n_updates        | 236499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=5.68 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 948000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.8e-05  |\n",
      "|    n_updates        | 236749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.16     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 85       |\n",
      "|    time_elapsed     | 11086    |\n",
      "|    total_timesteps  | 948000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.96     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 949000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000307 |\n",
      "|    n_updates        | 236999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.17     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.13e-05 |\n",
      "|    n_updates        | 237249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=5.81 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.81     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 951000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000122 |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=6.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 952000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000134 |\n",
      "|    n_updates        | 237749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 85       |\n",
      "|    time_elapsed     | 11096    |\n",
      "|    total_timesteps  | 952000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=6.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 953000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 237999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=4.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.33     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 954000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000192 |\n",
      "|    n_updates        | 238249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=5.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.83     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 955000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.67e-05 |\n",
      "|    n_updates        | 238499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=6.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 956000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.89e-05 |\n",
      "|    n_updates        | 238749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.17     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 86       |\n",
      "|    time_elapsed     | 11107    |\n",
      "|    total_timesteps  | 956000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=6.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 957000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000101 |\n",
      "|    n_updates        | 238999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 958000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 239249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 959000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.28e-05 |\n",
      "|    n_updates        | 239499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=6.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.25     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 960000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.07e-05 |\n",
      "|    n_updates        | 239749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.19     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 86       |\n",
      "|    time_elapsed     | 11117    |\n",
      "|    total_timesteps  | 960000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=5.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 961000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000115 |\n",
      "|    n_updates        | 239999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=5.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.97     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 962000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.56e-05 |\n",
      "|    n_updates        | 240249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=5.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.53     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 963000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.44e-05 |\n",
      "|    n_updates        | 240499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=5.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 964000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.85e-05 |\n",
      "|    n_updates        | 240749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.2      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 86       |\n",
      "|    time_elapsed     | 11127    |\n",
      "|    total_timesteps  | 964000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=3.48 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 3.48     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 965000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.39e-05 |\n",
      "|    n_updates        | 240999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=6.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 966000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.8e-05  |\n",
      "|    n_updates        | 241249   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=967000, episode_reward=6.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 967000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 241499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=6.28 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.28     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 968000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.61e-05 |\n",
      "|    n_updates        | 241749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 86       |\n",
      "|    time_elapsed     | 11138    |\n",
      "|    total_timesteps  | 968000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=6.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 969000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.84e-05 |\n",
      "|    n_updates        | 241999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=6.05 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 970000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.79e-05 |\n",
      "|    n_updates        | 242249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.14     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 971000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 242499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=6.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.04     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 972000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000121 |\n",
      "|    n_updates        | 242749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.16     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 87       |\n",
      "|    time_elapsed     | 11148    |\n",
      "|    total_timesteps  | 972000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=4.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.92     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 973000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000325 |\n",
      "|    n_updates        | 242999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=6.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 974000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.98e-05 |\n",
      "|    n_updates        | 243249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=6.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.23     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 975000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 243499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=6.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.22     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 976000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000319 |\n",
      "|    n_updates        | 243749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 87       |\n",
      "|    time_elapsed     | 11158    |\n",
      "|    total_timesteps  | 976000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.89     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 977000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 9.21e-05 |\n",
      "|    n_updates        | 243999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=5.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 978000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.28e-05 |\n",
      "|    n_updates        | 244249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=5.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.84     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 979000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.86e-05 |\n",
      "|    n_updates        | 244499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=4.04 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.04     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 980000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.2e-05  |\n",
      "|    n_updates        | 244749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 87       |\n",
      "|    time_elapsed     | 11168    |\n",
      "|    total_timesteps  | 980000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=6.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 981000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00019  |\n",
      "|    n_updates        | 244999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=6.15 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.15     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 982000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000269 |\n",
      "|    n_updates        | 245249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=6.25 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.25     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 983000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00081  |\n",
      "|    n_updates        | 245499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=6.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 984000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000442 |\n",
      "|    n_updates        | 245749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 80       |\n",
      "|    time_elapsed     | 12171    |\n",
      "|    total_timesteps  | 984000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=5.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 985000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.34e-05 |\n",
      "|    n_updates        | 245999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=6.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.18     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 986000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000145 |\n",
      "|    n_updates        | 246249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=6.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.17     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 987000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000557 |\n",
      "|    n_updates        | 246499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=6.31 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.31     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 988000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000111 |\n",
      "|    n_updates        | 246749   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 12182    |\n",
      "|    total_timesteps  | 988000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=6.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.07     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 989000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.24e-05 |\n",
      "|    n_updates        | 246999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=6.13 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 990000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 247249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=4.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 4.55     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 991000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000582 |\n",
      "|    n_updates        | 247499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=6.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.21     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 992000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.94e-05 |\n",
      "|    n_updates        | 247749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.1      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 12192    |\n",
      "|    total_timesteps  | 992000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=5.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.71     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 993000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.54e-06 |\n",
      "|    n_updates        | 247999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.99     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 994000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000301 |\n",
      "|    n_updates        | 248249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=5.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.64     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 995000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000407 |\n",
      "|    n_updates        | 248499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=6.08 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.08     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 996000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.91e-05 |\n",
      "|    n_updates        | 248749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 12202    |\n",
      "|    total_timesteps  | 996000   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=997000, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.12     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 997000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000116 |\n",
      "|    n_updates        | 248999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=6.29 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 6.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 998000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.37e-05 |\n",
      "|    n_updates        | 249249   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 5.75     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 999000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.74e-05 |\n",
      "|    n_updates        | 249499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=0.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | 0.475    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 5.78e-05 |\n",
      "|    n_updates        | 249749   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | 4.18     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 81       |\n",
      "|    time_elapsed     | 12213    |\n",
      "|    total_timesteps  | 1000000  |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x177c1d090>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models/',\n",
    "                             log_path='./rl-logs/', eval_freq=1 000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=4,\n",
    "#             tensorboard_log=\"./rl-logs/\")\n",
    "model = DQN(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            exploration_fraction=0.3,\n",
    "            target_update_interval=5000,\n",
    "            buffer_size=100000,\n",
    "            learning_starts=1000,\n",
    "            learning_rate=0.001\n",
    ")\n",
    "model.learn(total_timesteps=3000000, tb_log_name=\"ppo\",\n",
    "            callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(env_class, max_steps, num_episodes, model=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model over several episodes and plot the results.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained RL model to be evaluated.\n",
    "    - env_class: Environment class to create new instances of the evaluation environment.\n",
    "    - max_steps: Maximum number of steps per episode.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    - avg_observations: Average observations at each step.\n",
    "    - all_observations: List of observations for all episodes.\n",
    "    \"\"\"\n",
    "    all_observations = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        eval_env = TimeLimit(env_class(), max_steps)\n",
    "        done = False\n",
    "        obs, _ = eval_env.reset()\n",
    "        episode_observations = []\n",
    "        while not done:\n",
    "            if model is not None:\n",
    "                action, _states = model.predict(obs)\n",
    "            else:\n",
    "                action = eval_env.action_space.sample()\n",
    "            obs, rewards, term, trunc, info = eval_env.step(action)\n",
    "            done = term or trunc\n",
    "            episode_observations.append(info['n_cells'])\n",
    "        \n",
    "        all_observations.append(episode_observations)\n",
    "    \n",
    "    # Compute the average observations\n",
    "    max_len = max(len(obs) for obs in all_observations)\n",
    "    avg_observations = np.zeros(max_len)\n",
    "    counts = np.zeros(max_len)\n",
    "    \n",
    "    for obs in all_observations:\n",
    "        for i, val in enumerate(obs):\n",
    "            avg_observations[i] += val[-1]\n",
    "            counts[i] += 1\n",
    "    \n",
    "    avg_observations /= counts\n",
    "\n",
    "    return avg_observations, all_observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_observations(avg_observations, all_observations, unif_obs=None, all_unif_obs=None):\n",
    "    \"\"\"\n",
    "    Plot the average observations and individual episode tracks.\n",
    "\n",
    "    Parameters:\n",
    "    - avg_observations: Average observations at each step.\n",
    "    - all_observations: List of observations for all episodes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot individual episode tracks with lower alpha\n",
    "    for obs in all_observations:\n",
    "        plt.plot(x_axis, obs, alpha=0.05, linewidth=0.5, color='black')\n",
    "\n",
    "    if unif_obs is not None:\n",
    "        for obs in all_unif_obs:\n",
    "            plt.plot(x_axis, obs, alpha=0.05, linewidth=0.5, color='red')\n",
    "    \n",
    "    # Plot average observations\n",
    "    plt.plot(x_axis, avg_observations, label='trained policy', linewidth=3, color='black')\n",
    "    if unif_obs is not None:\n",
    "        plt.plot(x_axis, unif_obs, label='random policy', linewidth=3, color='red')\n",
    "    \n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('Total cells (n_cells)')\n",
    "    plt.title('Model Evaluation: Average Observations and Individual Episode Tracks')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "avg_obs, all_obs = evaluate_model(CellEnv, 1200, 10, model)\n",
    "unif_obs, unif_all_obs = evaluate_model(CellEnv, 1200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_observations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_observations\u001b[49m(avg_obs, all_obs, unif_obs, unif_all_obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_observations' is not defined"
     ]
    }
   ],
   "source": [
    "plot_observations(avg_obs, all_obs, unif_obs, unif_all_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
