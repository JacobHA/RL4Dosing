{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_env import CellEnv\n",
    "# Use sb3 env checker:\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    'dt': 0.01,\n",
    "    'alpha_mem': 0.7,\n",
    "    'max_timesteps': 10000,\n",
    "    'frame_stack': 20\n",
    "}    \n",
    "\n",
    "env = CellEnv(**env_args)\n",
    "# Use vectorized environments:\n",
    "# use the monitor wrapper to log the results:\n",
    "env = Monitor(env)\n",
    "eval_env = CellEnv(**env_args)\n",
    "eval_env = Monitor(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./rl-logs/ppo_10\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.68     |\n",
      "|    ep_rew_mean     | -0.762   |\n",
      "| time/              |          |\n",
      "|    fps             | 2248     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020210523 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | -0.0902     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.791       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.85     |\n",
      "|    ep_rew_mean     | -0.716   |\n",
      "| time/              |          |\n",
      "|    fps             | 425      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023093056 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.165       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.429       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.2     |\n",
      "|    ep_rew_mean     | -0.715   |\n",
      "| time/              |          |\n",
      "|    fps             | 213      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.2        |\n",
      "|    ep_rew_mean          | -0.715      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011925528 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0326      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018889686 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.036       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.6     |\n",
      "|    ep_rew_mean     | -0.699   |\n",
      "| time/              |          |\n",
      "|    fps             | 192      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006721338 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00944    |\n",
      "|    value_loss           | 0.36        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | -0.682   |\n",
      "| time/              |          |\n",
      "|    fps             | 186      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -0.682      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006336157 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.076       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006232618 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.146       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00856    |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -0.674   |\n",
      "| time/              |          |\n",
      "|    fps             | 192      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003939665 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    value_loss           | 0.364       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 153      |\n",
      "|    ep_rew_mean     | -0.665   |\n",
      "| time/              |          |\n",
      "|    fps             | 194      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | -0.656       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 198          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032447418 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.389        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.192        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    value_loss           | 0.8          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 1166.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.17e+03    |\n",
      "|    mean_reward          | -0.107      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003974907 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.196       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    value_loss           | 0.59        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.648   |\n",
      "| time/              |          |\n",
      "|    fps             | 195      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 1429.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.43e+03     |\n",
      "|    mean_reward          | -0.0424      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068783592 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.491       |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.654        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00554     |\n",
      "|    value_loss           | 0.809        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.648   |\n",
      "| time/              |          |\n",
      "|    fps             | 189      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | -0.636      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008037293 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | -0.882      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1527.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.53e+03    |\n",
      "|    mean_reward          | -0.00802    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005271367 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00818    |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 265      |\n",
      "|    ep_rew_mean     | -0.629   |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 2068.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.07e+03    |\n",
      "|    mean_reward          | -0.0152     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008168458 |\n",
      "|    clip_fraction        | 0.0873      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.362       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    value_loss           | 0.917       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 265      |\n",
      "|    ep_rew_mean     | -0.629   |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 297          |\n",
      "|    ep_rew_mean          | -0.63        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 178          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053322087 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.301        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.245        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 0.709        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 1548.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.55e+03   |\n",
      "|    mean_reward          | -0.0767    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00607194 |\n",
      "|    clip_fraction        | 0.0605     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.433     |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.496      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.00808   |\n",
      "|    value_loss           | 1.3        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 325      |\n",
      "|    ep_rew_mean     | -0.622   |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.14 +/- 0.00\n",
      "Episode length: 1769.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.77e+03    |\n",
      "|    mean_reward          | -0.145      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009868228 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    value_loss           | 0.687       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 325      |\n",
      "|    ep_rew_mean     | -0.622   |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 358          |\n",
      "|    ep_rew_mean          | -0.614       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 172          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042434935 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.505       |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00665     |\n",
      "|    value_loss           | 0.471        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1963.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.96e+03    |\n",
      "|    mean_reward          | -0.0196     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003461002 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.936       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 0.85        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 389      |\n",
      "|    ep_rew_mean     | -0.607   |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 3713.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.71e+03    |\n",
      "|    mean_reward          | -0.0335     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014711495 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.519      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 389      |\n",
      "|    ep_rew_mean     | -0.607   |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 2238.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.24e+03   |\n",
      "|    mean_reward          | -0.0244    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 45000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01062278 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.488     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.214      |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    value_loss           | 1.05       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 428      |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 428         |\n",
      "|    ep_rew_mean          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009509372 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    value_loss           | 0.526       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1841.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.84e+03    |\n",
      "|    mean_reward          | -0.0181     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009322811 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.343       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 465      |\n",
      "|    ep_rew_mean     | -0.591   |\n",
      "| time/              |          |\n",
      "|    fps             | 148      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 2435.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.44e+03     |\n",
      "|    mean_reward          | -0.0643      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 51000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069045657 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.723        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.181        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00762     |\n",
      "|    value_loss           | 0.585        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 465      |\n",
      "|    ep_rew_mean     | -0.591   |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 511         |\n",
      "|    ep_rew_mean          | -0.586      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014217083 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.0267      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 688.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 688         |\n",
      "|    mean_reward          | -0.0768     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016593872 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.372       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | -0.586   |\n",
      "| time/              |          |\n",
      "|    fps             | 144      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 2350.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.35e+03    |\n",
      "|    mean_reward          | -0.0324     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012450694 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.219       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.97        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | -0.586   |\n",
      "| time/              |          |\n",
      "|    fps             | 138      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 414      |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 511      |\n",
      "|    ep_rew_mean          | -0.586   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 132      |\n",
      "|    iterations           | 29       |\n",
      "|    time_elapsed         | 448      |\n",
      "|    total_timesteps      | 59392    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.017823 |\n",
      "|    clip_fraction        | 0.265    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.608   |\n",
      "|    explained_variance   | 0.24     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.00882 |\n",
      "|    n_updates            | 280      |\n",
      "|    policy_gradient_loss | -0.0233  |\n",
      "|    value_loss           | 0.0231   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017913733 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00673     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.0223      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | -0.586   |\n",
      "| time/              |          |\n",
      "|    fps             | 123      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 63000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013243489 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.00537     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | -0.538   |\n",
      "| time/              |          |\n",
      "|    fps             | 117      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 542      |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 614         |\n",
      "|    ep_rew_mean          | -0.538      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 550         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017149653 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.848       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.758       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 716.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 716         |\n",
      "|    mean_reward          | -0.172      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008770492 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.813       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | -0.538   |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 572      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 752.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 752          |\n",
      "|    mean_reward          | -0.0362      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 69000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066649877 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.0944       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0121       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00755     |\n",
      "|    value_loss           | 0.0286       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | -0.538   |\n",
      "| time/              |          |\n",
      "|    fps             | 114      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 607      |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 614        |\n",
      "|    ep_rew_mean          | -0.538     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 654        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01822175 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.548     |\n",
      "|    explained_variance   | -0.0222    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 0.0446     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018832834 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    value_loss           | 0.0176      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | -0.451   |\n",
      "| time/              |          |\n",
      "|    fps             | 104      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 703      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 627.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 627         |\n",
      "|    mean_reward          | -0.202      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011174227 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0084     |\n",
      "|    value_loss           | 0.576       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | -0.451   |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 714      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 714         |\n",
      "|    ep_rew_mean          | -0.451      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 736         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011379091 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.513       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 673.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 673         |\n",
      "|    mean_reward          | -0.00783    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010811215 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0059      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.00531     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | -0.451   |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 773      |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 81000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017556764 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.00966     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | -0.451   |\n",
      "| time/              |          |\n",
      "|    fps             | 99       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 821      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 814       |\n",
      "|    ep_rew_mean          | -0.309    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 97        |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 864       |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0154507 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.664    |\n",
      "|    explained_variance   | 0.541     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0137   |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.0133   |\n",
      "|    value_loss           | 0.00171   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 617.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 617         |\n",
      "|    mean_reward          | -0.103      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013092528 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.416       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 876      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 748.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 748         |\n",
      "|    mean_reward          | -0.168      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011922148 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0532      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 97       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 901      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 683.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 683         |\n",
      "|    mean_reward          | -0.0257     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012514332 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0195      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | -0.309   |\n",
      "| time/              |          |\n",
      "|    fps             | 95       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 939      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 814         |\n",
      "|    ep_rew_mean          | -0.309      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 989         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017777547 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    value_loss           | 0.0142      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 641.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 641         |\n",
      "|    mean_reward          | -0.134      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018430855 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.0139      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | -0.193   |\n",
      "| time/              |          |\n",
      "|    fps             | 91       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1027     |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 738.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 738         |\n",
      "|    mean_reward          | -0.0209     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012831197 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.342       |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.424       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | -0.193   |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1041     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 914         |\n",
      "|    ep_rew_mean          | -0.193      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1066        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014021857 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.0553      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 706.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 706         |\n",
      "|    mean_reward          | -0.0375     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 99000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018825864 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.0297      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | -0.193   |\n",
      "| time/              |          |\n",
      "|    fps             | 90       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 1106     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 102000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131142875 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0202      |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    value_loss           | 0.0189       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | -0.193   |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 1157     |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.01e+03    |\n",
      "|    ep_rew_mean          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1188        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005244879 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00258     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 673.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 673         |\n",
      "|    mean_reward          | -0.00783    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013482945 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.434       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 1203     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 6.9         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 108000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015654478 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00709     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.0424      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 1384     |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.01e+03    |\n",
      "|    ep_rew_mean          | -0.102      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1424        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014677339 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0139      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 111000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01935137 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.446     |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00589   |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0318    |\n",
      "|    value_loss           | 0.00719    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.01e+03 |\n",
      "|    ep_rew_mean     | -0.102   |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 1477     |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=24.00 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 114000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012351383 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00638    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+03 |\n",
      "|    ep_rew_mean     | 0.0461   |\n",
      "| time/              |          |\n",
      "|    fps             | 69       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 1657     |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.11e+03    |\n",
      "|    ep_rew_mean          | 0.0461      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1672        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013933747 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.326       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=9.58 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 9.58        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005632761 |\n",
      "|    clip_fraction        | 0.0531      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00021    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00799    |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+03 |\n",
      "|    ep_rew_mean     | 0.0461   |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 1855     |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=18.99 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+04     |\n",
      "|    mean_reward          | 19        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 120000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0190522 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.369    |\n",
      "|    explained_variance   | 0.55      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0104   |\n",
      "|    n_updates            | 580       |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    value_loss           | 0.0114    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.11e+03 |\n",
      "|    ep_rew_mean     | 0.0461   |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 2051     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.11e+03    |\n",
      "|    ep_rew_mean          | 0.0461      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 2106        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013179269 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.446      |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018246504 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.00754     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.21e+03 |\n",
      "|    ep_rew_mean     | 0.193    |\n",
      "| time/              |          |\n",
      "|    fps             | 58       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 2126     |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=13.28 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+04      |\n",
      "|    mean_reward          | 13.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 126000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01493858 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0446     |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 0.238      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.21e+03 |\n",
      "|    ep_rew_mean     | 0.193    |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 2298     |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=17.97 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 18           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 129000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049637696 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0194      |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.21e+03 |\n",
      "|    ep_rew_mean     | 0.193    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 2483     |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.21e+03    |\n",
      "|    ep_rew_mean          | 0.193       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 2526        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017925087 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00652    |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=19.72 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 19.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011903805 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.00332     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.21e+03 |\n",
      "|    ep_rew_mean     | 0.193    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 2737     |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 135000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019100742 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+03 |\n",
      "|    ep_rew_mean     | 0.349    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 2751     |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24e+03    |\n",
      "|    ep_rew_mean          | 0.349       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2770        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010975847 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0858      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=13.68 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 13.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 138000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014557927 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 0.00803     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+03 |\n",
      "|    ep_rew_mean     | 0.349    |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 2956     |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 141000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022145253 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00568     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.24e+03 |\n",
      "|    ep_rew_mean     | 0.349    |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 3000     |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24e+03    |\n",
      "|    ep_rew_mean          | 0.349       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 3058        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011288651 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0316     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.00159     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016335342 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00901    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 3066     |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 147000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01177349 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.291     |\n",
      "|    explained_variance   | 0.6        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0312     |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    value_loss           | 0.26       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 3085     |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 402         |\n",
      "|    ep_rew_mean          | -0.0129     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 3117        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013201577 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.022      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020887218 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | -0.0129  |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 3163     |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 153000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015885886 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00247    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.179    |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 3219     |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 502        |\n",
      "|    ep_rew_mean          | 0.179      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 48         |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 3227       |\n",
      "|    total_timesteps      | 155648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01869645 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    value_loss           | 0.0884     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013593346 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.0678      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.179    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 3248     |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 159000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013935308 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.00215     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.179    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 3281     |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 502        |\n",
      "|    ep_rew_mean          | 0.179      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 48         |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 3328       |\n",
      "|    total_timesteps      | 161792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01770095 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.474     |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0285    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0344    |\n",
      "|    value_loss           | 0.00168    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 162000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018030185 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.381    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 3379     |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01337743 |\n",
      "|    clip_fraction        | 0.0851     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.558     |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0217     |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0086    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.381    |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 3388     |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.381       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 3410        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010245728 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0053      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.0361      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014132822 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0327     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.381    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 3445     |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 171000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022456255 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.00171     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.381    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 3493     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 174000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067685517 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | 0.761        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0131      |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00674     |\n",
      "|    value_loss           | 0.000402     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 3539     |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 702         |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 3550        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014882693 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.383       |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 177000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008981475 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0414     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.0136      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 3574     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017704958 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00932    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 3610     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 702         |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 3660        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014293801 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 183000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01344743 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.336     |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00855    |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.194    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 3699     |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 186000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015656823 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.368      |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.289       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.194    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 3711     |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 402         |\n",
      "|    ep_rew_mean          | 0.194       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 3735        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008945594 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00804    |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00936    |\n",
      "|    value_loss           | 0.00635     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 189000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019826338 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.599       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.00178     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.194    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 3773     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 192000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015642686 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.000896    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.194    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 502        |\n",
      "|    ep_rew_mean          | 0.385      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 50         |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 3859       |\n",
      "|    total_timesteps      | 194560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01739063 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.548     |\n",
      "|    explained_variance   | 0.584      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 0.00228    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003894913 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0359      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.385    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 3873     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 198000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009753708 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00826    |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.385    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 3899     |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 502         |\n",
      "|    ep_rew_mean          | 0.385       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 3938        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017903801 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017272526 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.00039     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.385    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 3990     |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 204000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014160792 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.602    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 4020     |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.602       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 4034        |\n",
      "|    total_timesteps      | 206848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011395926 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000121    |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 207000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010143705 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00474     |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.602    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 4062     |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 210000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016823735 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.00092     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.602    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 4103     |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.602       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 4156        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010596992 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.000405    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 213000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01731046 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.492     |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00702   |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    value_loss           | 0.00211    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.799    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 4180     |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 216000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007964049 |\n",
      "|    clip_fraction        | 0.0764      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.799    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 4197     |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 219000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014422595 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.00204     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.799    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 4225     |\n",
      "|    total_timesteps | 219136   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 702          |\n",
      "|    ep_rew_mean          | 0.799        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 51           |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 4268         |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072865793 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.587       |\n",
      "|    explained_variance   | 0.627        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0213      |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.00758     |\n",
      "|    value_loss           | 0.000462     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 222000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009721986 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.00969    |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 0.799    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 4322     |\n",
      "|    total_timesteps | 223232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 225000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00877454 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0198    |\n",
      "|    n_updates            | 1090       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    value_loss           | 0.000449   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 4340     |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 802         |\n",
      "|    ep_rew_mean          | 1.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 4357        |\n",
      "|    total_timesteps      | 227328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009646783 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00305    |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069655306 |\n",
      "|    clip_fraction        | 0.0889       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.478       |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0016      |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.00984     |\n",
      "|    value_loss           | 0.00144      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 4388     |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 231000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060744165 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.52        |\n",
      "|    explained_variance   | 0.609        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0139      |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 0.00053      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 4431     |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 802         |\n",
      "|    ep_rew_mean          | 1.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 4487        |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010847369 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.000304    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 234000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01278158 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.517     |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 0.000634   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | 0.591    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 4500     |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 237000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00593108 |\n",
      "|    clip_fraction        | 0.0537     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0946    |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 1150       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | 0.591    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 4518     |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 503         |\n",
      "|    ep_rew_mean          | 0.591       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 4549        |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008577276 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0019     |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00132     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.000427   |\n",
      "|    value_loss           | 0.00679     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | -0.00977   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 240000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06571584 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 1170       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    value_loss           | 0.00414    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | 0.591    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 4592     |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 243000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012842121 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.000416    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | 0.591    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 4649     |\n",
      "|    total_timesteps | 243712   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 403          |\n",
      "|    ep_rew_mean          | 0.341        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 4660         |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041251853 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.661        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00691      |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 0.000264     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 246000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009249828 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00702     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 403      |\n",
      "|    ep_rew_mean     | 0.341    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 247808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 249000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009600569 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000315    |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    value_loss           | 0.000637    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 403      |\n",
      "|    ep_rew_mean     | 0.341    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 4712     |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 403        |\n",
      "|    ep_rew_mean          | 0.341      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 52         |\n",
      "|    iterations           | 123        |\n",
      "|    time_elapsed         | 4756       |\n",
      "|    total_timesteps      | 251904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01219943 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.386     |\n",
      "|    explained_variance   | 0.481      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00879   |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012407575 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.000617    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 303      |\n",
      "|    ep_rew_mean     | 0.151    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 4813     |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 255000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008859019 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0055     |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 303      |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 4820     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 258000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078750895 |\n",
      "|    clip_fraction        | 0.0965       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00511      |\n",
      "|    n_updates            | 1250         |\n",
      "|    policy_gradient_loss | -0.00695     |\n",
      "|    value_loss           | 0.153        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 303      |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 4840     |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 303         |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 127         |\n",
      "|    time_elapsed         | 4873        |\n",
      "|    total_timesteps      | 260096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013226179 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00777     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0005      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 261000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066347835 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.568        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0187      |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.00869     |\n",
      "|    value_loss           | 0.000233     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 303      |\n",
      "|    ep_rew_mean     | 0.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 4919     |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 264000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009347146 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.000555    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.112    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 4971     |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 302         |\n",
      "|    ep_rew_mean          | 0.112       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 4979        |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013869995 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 267000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033897543 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.976        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0194       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 0.0312       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.112    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 5000     |\n",
      "|    total_timesteps | 268288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006591581 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.85e-05    |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 0.000527    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.112    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 5033     |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 302         |\n",
      "|    ep_rew_mean          | 0.112       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 5080        |\n",
      "|    total_timesteps      | 272384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003177274 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00406    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 0.000191    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 273000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004106552 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00816    |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 0.000168    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.0508  |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 5129     |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=19.96 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 276000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018616116 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00216    |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.0508  |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 5293     |\n",
      "|    total_timesteps | 276480   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 202         |\n",
      "|    ep_rew_mean          | -0.0508     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 5314        |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005673418 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0029     |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    value_loss           | 0.0296      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=20.43 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 279000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010924603 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.000387    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.0508  |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 5496     |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 282000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006950766 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.000224    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -0.0508  |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 5542     |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 302         |\n",
      "|    ep_rew_mean          | 0.142       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 5588        |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007091865 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 0.000346    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 285000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024080958 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.211       |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.0942      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.142    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 5598     |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004331558 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000299   |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.142    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 5620     |\n",
      "|    total_timesteps | 288768   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 302         |\n",
      "|    ep_rew_mean          | 0.142       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 5655        |\n",
      "|    total_timesteps      | 290816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944458 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00486     |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.000259    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 291000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044085467 |\n",
      "|    clip_fraction        | 0.0743       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0291      |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.007       |\n",
      "|    value_loss           | 0.000134     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 0.142    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 5702     |\n",
      "|    total_timesteps | 292864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 294000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021330942 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00974     |\n",
      "|    n_updates            | 1430         |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    value_loss           | 0.000208     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.363    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 5741     |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 402         |\n",
      "|    ep_rew_mean          | 0.363       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 5753        |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007769868 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 297000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008001078 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000539    |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.00745    |\n",
      "|    value_loss           | 0.0114      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.363    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 5777     |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005689706 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00847    |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    value_loss           | 0.000312    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.363    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 5814     |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 303000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023396122 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.363       |\n",
      "|    explained_variance   | 0.619        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00431      |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 0.000189     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.363    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 5863     |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 502          |\n",
      "|    ep_rew_mean          | 0.557        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 51           |\n",
      "|    iterations           | 149          |\n",
      "|    time_elapsed         | 5896         |\n",
      "|    total_timesteps      | 305152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034431978 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.328       |\n",
      "|    explained_variance   | 0.588        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00893     |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    value_loss           | 0.000171     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 306000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010098196 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00169    |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.557    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 5909     |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 309000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005430557 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00457    |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.557    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 5935     |\n",
      "|    total_timesteps | 309248   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 502          |\n",
      "|    ep_rew_mean          | 0.557        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 5972         |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018881662 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | 0.59         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00965     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 0.000256     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 312000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037019863 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00316      |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    value_loss           | 0.000147     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.557    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 6022     |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 315000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007884628 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000893   |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.00996    |\n",
      "|    value_loss           | 0.000172    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.327    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 6050     |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 402         |\n",
      "|    ep_rew_mean          | 0.327       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 6063        |\n",
      "|    total_timesteps      | 317440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022482641 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00769     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 318000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003829963 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | -0.109      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0092     |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.327    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 6090     |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 321000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006152002 |\n",
      "|    clip_fraction        | 0.0894      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.328      |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    value_loss           | 0.000167    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | 0.327    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 6128     |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 402          |\n",
      "|    ep_rew_mean          | 0.327        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 6179         |\n",
      "|    total_timesteps      | 323584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066290163 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.308       |\n",
      "|    explained_variance   | 0.664        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0204      |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 0.000114     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054260315 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | 0.585        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00639     |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.00374     |\n",
      "|    value_loss           | 0.000198     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 6205     |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 327000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027359799 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0139      |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 6219     |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 502          |\n",
      "|    ep_rew_mean          | 0.55         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 161          |\n",
      "|    time_elapsed         | 6246         |\n",
      "|    total_timesteps      | 329728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034518542 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.0383       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 0.00128      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 330000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036038805 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.605        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00426     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    value_loss           | 0.000101     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 6285     |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 333000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004971643 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00465    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 0.000135    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 6338     |\n",
      "|    total_timesteps | 333824   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 602          |\n",
      "|    ep_rew_mean          | 0.79         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 52           |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 6360         |\n",
      "|    total_timesteps      | 335872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020600706 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.651        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 0.000107     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 336000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022471659 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.126      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00663     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.00596    |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.79     |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 6375     |\n",
      "|    total_timesteps | 337920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=339000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 339000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033152793 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.0889       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00181     |\n",
      "|    n_updates            | 1650         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 0.00065      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.79     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 6404     |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 342000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005403136 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000821   |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 0.000129    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.79     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 6445     |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 6498        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002051074 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000392   |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    value_loss           | 9.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 345000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032326938 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.65         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00294     |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000441    |\n",
      "|    value_loss           | 9.13e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.812    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 6515     |\n",
      "|    total_timesteps | 346112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 348000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015406369 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.812    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 6532     |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 602          |\n",
      "|    ep_rew_mean          | 0.812        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 53           |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 6562         |\n",
      "|    total_timesteps      | 350208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028025496 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.169        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00251     |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 0.000622     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=351000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 351000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024968395 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.577        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0086      |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 0.000161     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.812    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 6604     |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 354000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016687365 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.575        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00102     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 9.86e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.812    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 6659     |\n",
      "|    total_timesteps | 354304   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 702         |\n",
      "|    ep_rew_mean          | 1.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 174         |\n",
      "|    time_elapsed         | 6670        |\n",
      "|    total_timesteps      | 356352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002077336 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 7.82e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 357000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009803958 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0947     |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 6688     |\n",
      "|    total_timesteps | 358400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011259917 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.358        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00393     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000381    |\n",
      "|    value_loss           | 0.000345     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 6720     |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 702        |\n",
      "|    ep_rew_mean          | 1.02       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 6763       |\n",
      "|    total_timesteps      | 362496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00204665 |\n",
      "|    clip_fraction        | 0.0276     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.224     |\n",
      "|    explained_variance   | 0.585      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000522  |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.00163   |\n",
      "|    value_loss           | 0.000121   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 363000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026523718 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.603        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0096       |\n",
      "|    n_updates            | 1770         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 9.1e-05      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.613    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 6817     |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | -0.00977    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 366000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011141423 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.613    |\n",
      "| time/              |          |\n",
      "|    fps             | 53       |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 6824     |\n",
      "|    total_timesteps | 366592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 502         |\n",
      "|    ep_rew_mean          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 6844        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007905944 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0608     |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00809     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    value_loss           | 0.0452      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=20.74 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 369000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048618433 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 0.00031      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.613    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 7026     |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | -0.00977     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 372000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023254766 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.657        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00123     |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 7.97e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 502      |\n",
      "|    ep_rew_mean     | 0.613    |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 7071     |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.869       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 7121        |\n",
      "|    total_timesteps      | 374784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002572171 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.000887   |\n",
      "|    value_loss           | 8.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=20.62 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 375000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071363817 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 1830         |\n",
      "|    policy_gradient_loss | -0.00972     |\n",
      "|    value_loss           | 0.176        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.869    |\n",
      "| time/              |          |\n",
      "|    fps             | 51       |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 7279     |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=20.69 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 378000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053753313 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0803      |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00509     |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    value_loss           | 0.00995      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.869    |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 7451     |\n",
      "|    total_timesteps | 378880   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 602         |\n",
      "|    ep_rew_mean          | 0.869       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 186         |\n",
      "|    time_elapsed         | 7484        |\n",
      "|    total_timesteps      | 380928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003541559 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0238      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    value_loss           | 0.000225    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=20.81 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 381000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043421295 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0145      |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 7.95e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 0.869    |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 7681     |\n",
      "|    total_timesteps | 382976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 384000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025377823 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.189       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00268     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 9.03e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 7726     |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=21.33 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 387000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097580515 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0121       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 0.179        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 49       |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 7886     |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 702          |\n",
      "|    ep_rew_mean          | 1.04         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 49           |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 7908         |\n",
      "|    total_timesteps      | 389120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052518323 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0872      |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00491      |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -0.00523     |\n",
      "|    value_loss           | 0.0106       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=20.66 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+04         |\n",
      "|    mean_reward          | 20.7          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 390000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068321207 |\n",
      "|    clip_fraction        | 0.00806       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0163       |\n",
      "|    explained_variance   | 0.966         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.43e-05      |\n",
      "|    n_updates            | 1900          |\n",
      "|    policy_gradient_loss | -0.000422     |\n",
      "|    value_loss           | 0.000744      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 8092     |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=20.73 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+04     |\n",
      "|    mean_reward          | 20.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 393000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.69e-06 |\n",
      "|    explained_variance   | 0.972     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0004    |\n",
      "|    n_updates            | 1910      |\n",
      "|    policy_gradient_loss | 7.28e-09  |\n",
      "|    value_loss           | 0.000731  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 1.04     |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 8290     |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 802       |\n",
      "|    ep_rew_mean          | 1.26      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 47        |\n",
      "|    iterations           | 193       |\n",
      "|    time_elapsed         | 8329      |\n",
      "|    total_timesteps      | 395264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.1e-05  |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.000213  |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -3.79e-09 |\n",
      "|    value_loss           | 0.00049   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=20.36 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 396000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063083963 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0184      |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0137       |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.00717     |\n",
      "|    value_loss           | 0.146        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 8491     |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=19.09 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 19.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 399000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011150746 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00768    |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.00519    |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 8665     |\n",
      "|    total_timesteps | 399360   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 802        |\n",
      "|    ep_rew_mean          | 1.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 46         |\n",
      "|    iterations           | 196        |\n",
      "|    time_elapsed         | 8701       |\n",
      "|    total_timesteps      | 401408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01040386 |\n",
      "|    clip_fraction        | 0.0752     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.116     |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000893   |\n",
      "|    n_updates            | 1950       |\n",
      "|    policy_gradient_loss | -0.00772   |\n",
      "|    value_loss           | 0.000312   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=20.26 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 402000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073798625 |\n",
      "|    clip_fraction        | 0.0833       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.142       |\n",
      "|    explained_variance   | 0.551        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0133      |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.00705     |\n",
      "|    value_loss           | 0.000133     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 1.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 8899     |\n",
      "|    total_timesteps | 403456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=20.35 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 405000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005699181 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00187    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 8.82e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 9083     |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 902        |\n",
      "|    ep_rew_mean          | 1.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 199        |\n",
      "|    time_elapsed         | 9095       |\n",
      "|    total_timesteps      | 407552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01343677 |\n",
      "|    clip_fraction        | 0.0403     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0194     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0043    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=21.20 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013494336 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.182       |\n",
      "|    explained_variance   | 0.0222       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000567     |\n",
      "|    n_updates            | 1990         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 0.00211      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 9270     |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=20.98 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 411000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037311064 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0146       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    value_loss           | 0.000165     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 9457     |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 902          |\n",
      "|    ep_rew_mean          | 1.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 202          |\n",
      "|    time_elapsed         | 9508         |\n",
      "|    total_timesteps      | 413696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036534085 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.492        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00456      |\n",
      "|    n_updates            | 2010         |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 0.000182     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=21.86 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 21.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 414000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002249951 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 0.000122    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 9685     |\n",
      "|    total_timesteps | 415744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=20.68 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+04      |\n",
      "|    mean_reward          | 20.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 417000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01046863 |\n",
      "|    clip_fraction        | 0.038      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0252     |\n",
      "|    n_updates            | 2030       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 9849     |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 902          |\n",
      "|    ep_rew_mean          | 1.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 9876         |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043313312 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00401     |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00332     |\n",
      "|    value_loss           | 0.00449      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=20.59 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035964255 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00178      |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | 0.000264     |\n",
      "|    value_loss           | 0.000117     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 10065    |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=20.52 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 423000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003415523 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000981    |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 7.9e-05     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 10266    |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 1.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 10289        |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026518588 |\n",
      "|    clip_fraction        | 0.0463       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.563        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00442     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 9e-05        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=21.30 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 21.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 426000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619237 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0922     |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00347     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 10454    |\n",
      "|    total_timesteps | 428032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=21.49 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 21.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 429000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008190222 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 10631    |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=21.36 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 432000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014133258 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00669     |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.000144    |\n",
      "|    value_loss           | 9.06e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 10822    |\n",
      "|    total_timesteps | 432128   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 10875       |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002538631 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.197      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00194    |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 6.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=21.33 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 435000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027274485 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000615    |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 3.56e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 11042    |\n",
      "|    total_timesteps | 436224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=20.71 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+04      |\n",
      "|    mean_reward          | 20.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 438000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02235783 |\n",
      "|    clip_fraction        | 0.0568     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0855    |\n",
      "|    explained_variance   | 0.825      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 2130       |\n",
      "|    policy_gradient_loss | -0.00705   |\n",
      "|    value_loss           | 0.205      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 11209    |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.1e+03      |\n",
      "|    ep_rew_mean          | 1.88         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 215          |\n",
      "|    time_elapsed         | 11238        |\n",
      "|    total_timesteps      | 440320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056669083 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.767        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000895     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 0.000936     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=441000, episode_reward=20.42 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 441000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061705885 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.627        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00817      |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 6.76e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 11430    |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 444000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042859414 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.542        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00815      |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 9.5e-05      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 11484    |\n",
      "|    total_timesteps | 444416   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.2e+03      |\n",
      "|    ep_rew_mean          | 2.07         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 218          |\n",
      "|    time_elapsed         | 11496        |\n",
      "|    total_timesteps      | 446464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035659913 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0174      |\n",
      "|    n_updates            | 2170         |\n",
      "|    policy_gradient_loss | -0.000822    |\n",
      "|    value_loss           | 5.25e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=20.54 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 447000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009763391 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0796     |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00297    |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 2.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 11664    |\n",
      "|    total_timesteps | 448512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=21.48 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 450000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029417565 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.32         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00213      |\n",
      "|    n_updates            | 2190         |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 0.000395     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 2.07     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 11845    |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.2e+03      |\n",
      "|    ep_rew_mean          | 2.07         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 221          |\n",
      "|    time_elapsed         | 11888        |\n",
      "|    total_timesteps      | 452608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020639352 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.494        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00167      |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 0.000106     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=21.84 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 453000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022418802 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.644        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00395     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 6.85e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 2.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 12092    |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 456000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015741489 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0011      |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | 0.000174     |\n",
      "|    value_loss           | 0.0803       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 2.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 12099    |\n",
      "|    total_timesteps | 456704   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.3e+03     |\n",
      "|    ep_rew_mean          | 2.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 12119       |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014590393 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0656     |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00553     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 0.084       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=20.63 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 459000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003677962 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000382    |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 0.000322    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 2.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 12301    |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 462000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046422193 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.328        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00753     |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -0.00699     |\n",
      "|    value_loss           | 0.000177     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 2.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 12346    |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.4e+03      |\n",
      "|    ep_rew_mean          | 2.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 227          |\n",
      "|    time_elapsed         | 12395        |\n",
      "|    total_timesteps      | 464896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034387615 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00768     |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 6.47e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 465000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027693585 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00913     |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.00942    |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 12404    |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 468000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011187237 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0996     |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00852     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 0.0601      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 12426    |\n",
      "|    total_timesteps | 468992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 471000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040128827 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 0.000203     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 12460    |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.4e+03      |\n",
      "|    ep_rew_mean          | 2.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 231          |\n",
      "|    time_elapsed         | 12506        |\n",
      "|    total_timesteps      | 473088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021513226 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00594     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 0.000126     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 474000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003246498 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    value_loss           | 0.00011     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 12549    |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 477000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046305135 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.153       |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0684       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.000844    |\n",
      "|    value_loss           | 0.106        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 12560    |\n",
      "|    total_timesteps | 477184   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 2.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 12582        |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050476696 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00234      |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 0.00511      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011912987 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.49         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00176     |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.000297    |\n",
      "|    value_loss           | 0.000154     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 12618    |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 483000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020544438 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 2350         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 0.000124     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 12666    |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.88         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 237          |\n",
      "|    time_elapsed         | 12703        |\n",
      "|    total_timesteps      | 485376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030858696 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.583        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00606      |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.000557    |\n",
      "|    value_loss           | 0.000102     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=20.78 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 486000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008114373 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 12865    |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=20.66 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 489000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008277882 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 13040    |\n",
      "|    total_timesteps | 489472   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.6e+03     |\n",
      "|    ep_rew_mean          | 2.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 240         |\n",
      "|    time_elapsed         | 13077       |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008113262 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.193      |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00888    |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 0.000126    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=20.73 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+04      |\n",
      "|    mean_reward          | 20.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 492000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00521785 |\n",
      "|    clip_fraction        | 0.0299     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.197     |\n",
      "|    explained_variance   | 0.594      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00127   |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.00112   |\n",
      "|    value_loss           | 6.99e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 13276    |\n",
      "|    total_timesteps | 493568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=20.68 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 495000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002555512 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00209     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    value_loss           | 7.42e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 3.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 13458    |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.7e+03     |\n",
      "|    ep_rew_mean          | 3.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 13471       |\n",
      "|    total_timesteps      | 497664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010140089 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=20.66 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 498000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038698707 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | -0.0389      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0035      |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 0.00229      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 3.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 13648    |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=20.52 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 20.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 501000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044504777 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0031       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    value_loss           | 0.000144     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 3.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 13836    |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.7e+03     |\n",
      "|    ep_rew_mean          | 3.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 13886       |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003077076 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.179      |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.000789   |\n",
      "|    value_loss           | 9.91e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 504000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027201357 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.435        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00267     |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 9.06e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 13913    |\n",
      "|    total_timesteps | 505856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=21.03 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 21          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 507000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007429486 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0992     |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.63        |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 14077    |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.8e+03      |\n",
      "|    ep_rew_mean          | 3.29         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 249          |\n",
      "|    time_elapsed         | 14104        |\n",
      "|    total_timesteps      | 509952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031614236 |\n",
      "|    clip_fraction        | 0.0361       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.187        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00265      |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 0.00106      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 510000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028193023 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00761     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 0.000108     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 14144    |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 513000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023201285 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00561     |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 6.14e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.29     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 14196    |\n",
      "|    total_timesteps | 514048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 516000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023134942 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.622        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00659      |\n",
      "|    n_updates            | 2510         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 7.79e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 14217    |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.9e+03      |\n",
      "|    ep_rew_mean          | 3.48         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 14233        |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062234355 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0897      |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0164       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00468     |\n",
      "|    value_loss           | 0.109        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 519000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004500507 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00332    |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.00179    |\n",
      "|    value_loss           | 0.000485    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 14262    |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=21.20 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 522000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024701515 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.565        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00912     |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.000975    |\n",
      "|    value_loss           | 9.97e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 14453    |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.9e+03     |\n",
      "|    ep_rew_mean          | 3.48        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 14506       |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001939493 |\n",
      "|    clip_fraction        | 0.0274      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0028     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    value_loss           | 7.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 525000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032947625 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00665     |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 0.000138     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 14521    |\n",
      "|    total_timesteps | 526336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=20.69 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011532793 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0884     |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00177     |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 14689    |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 3.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 259         |\n",
      "|    time_elapsed         | 14719       |\n",
      "|    total_timesteps      | 530432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002167378 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00677     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 0.000332    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=531000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 531000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024671592 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00976     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.000105     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 14762    |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 534000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016401686 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00354     |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 8.75e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 14817    |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2e+03        |\n",
      "|    ep_rew_mean          | 3.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 262          |\n",
      "|    time_elapsed         | 14826        |\n",
      "|    total_timesteps      | 536576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021116363 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00575     |\n",
      "|    n_updates            | 2610         |\n",
      "|    policy_gradient_loss | 0.000251     |\n",
      "|    value_loss           | 9.07e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 537000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009593928 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0835     |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 14845    |\n",
      "|    total_timesteps | 538624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 540000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025083101 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00576     |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 0.000252     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 14877    |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2e+03        |\n",
      "|    ep_rew_mean          | 3.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 36           |\n",
      "|    iterations           | 265          |\n",
      "|    time_elapsed         | 14921        |\n",
      "|    total_timesteps      | 542720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041866964 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00193      |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.000916    |\n",
      "|    value_loss           | 0.000118     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 543000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015365176 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00805     |\n",
      "|    n_updates            | 2650         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 8.13e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 14973    |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 546000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026536838 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.585        |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 0.0846       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 14981    |\n",
      "|    total_timesteps | 546816   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.1e+03     |\n",
      "|    ep_rew_mean          | 3.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 15002       |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196912 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0953     |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    value_loss           | 0.0158      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 549000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00392243 |\n",
      "|    clip_fraction        | 0.0408     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.157     |\n",
      "|    explained_variance   | 0.429      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00212    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.000849  |\n",
      "|    value_loss           | 0.00023    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 15035    |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 552000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002850194 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 0.00011     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 15081    |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 555000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004678945 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000994    |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | 0.00034     |\n",
      "|    value_loss           | 0.00011     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 15127    |\n",
      "|    total_timesteps | 555008   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.9e+03     |\n",
      "|    ep_rew_mean          | 3.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 15136       |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005365093 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.00578    |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 558000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034292033 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.002        |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.00271     |\n",
      "|    value_loss           | 0.00762      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 15158    |\n",
      "|    total_timesteps | 559104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 561000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054928595 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.153       |\n",
      "|    explained_variance   | 0.383        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00959      |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 0.000217     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 15193    |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.9e+03     |\n",
      "|    ep_rew_mean          | 3.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 15240       |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006713328 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 564000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031358285 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.549        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00483      |\n",
      "|    n_updates            | 2750         |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    value_loss           | 8.57e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 15281    |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 567000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014353004 |\n",
      "|    clip_fraction        | 0.0547      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00538     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 0.0975      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 15292    |\n",
      "|    total_timesteps | 567296   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 3.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 278         |\n",
      "|    time_elapsed         | 15316       |\n",
      "|    total_timesteps      | 569344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012527032 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00837    |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 570000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048189145 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00265     |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 0.000142     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 15352    |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 573000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003044104 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 8.34e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 15400    |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.1e+03      |\n",
      "|    ep_rew_mean          | 3.85         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 281          |\n",
      "|    time_elapsed         | 15436        |\n",
      "|    total_timesteps      | 575488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040450534 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00346      |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 0.00012      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 576000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008356537 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.103      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00532    |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 0.095       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 15448    |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 579000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026472416 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.0343       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00436      |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 0.00248      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 15473    |\n",
      "|    total_timesteps | 579584   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.1e+03      |\n",
      "|    ep_rew_mean          | 3.85         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 284          |\n",
      "|    time_elapsed         | 15511        |\n",
      "|    total_timesteps      | 581632       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032034027 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.239        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0117       |\n",
      "|    n_updates            | 2830         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 0.000166     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 582000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026078639 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00913      |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.000838    |\n",
      "|    value_loss           | 0.000173     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.1e+03  |\n",
      "|    ep_rew_mean     | 3.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 15560    |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 585000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021273117 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.579        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00366     |\n",
      "|    n_updates            | 2850         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 8.36e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.2e+03  |\n",
      "|    ep_rew_mean     | 4.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 15590    |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.2e+03     |\n",
      "|    ep_rew_mean          | 4.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 37          |\n",
      "|    iterations           | 287         |\n",
      "|    time_elapsed         | 15604       |\n",
      "|    total_timesteps      | 587776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016681228 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0191      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 588000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052507794 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0.0233       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0212      |\n",
      "|    n_updates            | 2870         |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    value_loss           | 0.00144      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.2e+03  |\n",
      "|    ep_rew_mean     | 4.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 15630    |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 591000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050849244 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | -0.0901      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00111      |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0046      |\n",
      "|    value_loss           | 0.000297     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.2e+03  |\n",
      "|    ep_rew_mean     | 4.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 15669    |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.2e+03      |\n",
      "|    ep_rew_mean          | 4.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 37           |\n",
      "|    iterations           | 290          |\n",
      "|    time_elapsed         | 15720        |\n",
      "|    total_timesteps      | 593920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025127418 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.469        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00546      |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | 0.000544     |\n",
      "|    value_loss           | 9.18e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 594000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019782453 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.523        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | 0.000408     |\n",
      "|    value_loss           | 8.77e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.3e+03  |\n",
      "|    ep_rew_mean     | 4.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 15745    |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 597000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014637778 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.099      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.3e+03  |\n",
      "|    ep_rew_mean     | 4.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 37       |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 15760    |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 600000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048657265 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00898     |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 0.000768     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.3e+03  |\n",
      "|    ep_rew_mean     | 4.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 15788    |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.3e+03     |\n",
      "|    ep_rew_mean          | 4.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 38          |\n",
      "|    iterations           | 294         |\n",
      "|    time_elapsed         | 15828       |\n",
      "|    total_timesteps      | 602112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003583835 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00694    |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 0.000166    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 603000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017942071 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0011      |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | 0.000332     |\n",
      "|    value_loss           | 0.000116     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.3e+03  |\n",
      "|    ep_rew_mean     | 4.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 15881    |\n",
      "|    total_timesteps | 604160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 606000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014333199 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.415        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000728     |\n",
      "|    n_updates            | 2950         |\n",
      "|    policy_gradient_loss | -0.000641    |\n",
      "|    value_loss           | 0.000106     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 15900    |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.4e+03     |\n",
      "|    ep_rew_mean          | 4.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 38          |\n",
      "|    iterations           | 297         |\n",
      "|    time_elapsed         | 15917       |\n",
      "|    total_timesteps      | 608256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008198398 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.103      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000416   |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 609000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043298095 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00179     |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    value_loss           | 0.000508     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 15946    |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 612000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003894947 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 0.000144    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 15988    |\n",
      "|    total_timesteps | 612352   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.4e+03      |\n",
      "|    ep_rew_mean          | 4.42         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 16042        |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018105265 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.255        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00764      |\n",
      "|    n_updates            | 2990         |\n",
      "|    policy_gradient_loss | -0.000587    |\n",
      "|    value_loss           | 0.000151     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 615000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031687664 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.319        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00967      |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    value_loss           | 0.000104     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 16055    |\n",
      "|    total_timesteps | 616448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 618000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00873596 |\n",
      "|    clip_fraction        | 0.0591     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.114     |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 3010       |\n",
      "|    policy_gradient_loss | -0.00642   |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 16074    |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.4e+03      |\n",
      "|    ep_rew_mean          | 4.34         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 303          |\n",
      "|    time_elapsed         | 16104        |\n",
      "|    total_timesteps      | 620544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065161353 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.0816       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00924     |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 0.000333     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=621000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 621000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002750418 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00593    |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 0.000367    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 16147    |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060247323 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.25         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0028       |\n",
      "|    n_updates            | 3040         |\n",
      "|    policy_gradient_loss | -0.000152    |\n",
      "|    value_loss           | 0.000138     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 16203    |\n",
      "|    total_timesteps | 624640   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.4e+03      |\n",
      "|    ep_rew_mean          | 4.33         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 16210        |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035571635 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00402     |\n",
      "|    n_updates            | 3050         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 9.02e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 627000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076602763 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00411      |\n",
      "|    n_updates            | 3060         |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    value_loss           | 0.115        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 16230    |\n",
      "|    total_timesteps | 628736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 630000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043863594 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.0398       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0222      |\n",
      "|    n_updates            | 3070         |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 0.000349     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 16262    |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.4e+03      |\n",
      "|    ep_rew_mean          | 4.33         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 309          |\n",
      "|    time_elapsed         | 16307        |\n",
      "|    total_timesteps      | 632832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021119171 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.303        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00667     |\n",
      "|    n_updates            | 3080         |\n",
      "|    policy_gradient_loss | -0.000938    |\n",
      "|    value_loss           | 0.000158     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 633000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016971829 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.45         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00176     |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | -0.000512    |\n",
      "|    value_loss           | 0.000123     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 16357    |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 636000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004954339 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00295     |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    value_loss           | 0.0981      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 38       |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 16365    |\n",
      "|    total_timesteps | 636928   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.4e+03     |\n",
      "|    ep_rew_mean          | 4.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 38          |\n",
      "|    iterations           | 312         |\n",
      "|    time_elapsed         | 16387       |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005687089 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00163    |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 639000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033907422 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000258     |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 0.000298     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 16420    |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 642000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009780336 |\n",
      "|    clip_fraction        | 0.0807      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.0949      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 0.00021     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2.4e+03  |\n",
      "|    ep_rew_mean     | 4.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 16466    |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 645000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049349377 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.529        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000537    |\n",
      "|    n_updates            | 3140         |\n",
      "|    policy_gradient_loss | -0.000766    |\n",
      "|    value_loss           | 0.000104     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 16511    |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 3.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 16521       |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010369383 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.00614    |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 648000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041183196 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00414      |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    value_loss           | 0.00604      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 16543    |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 651000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014898741 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.0942       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 3170         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 0.000303     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 16578    |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2e+03        |\n",
      "|    ep_rew_mean          | 3.44         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 319          |\n",
      "|    time_elapsed         | 16625        |\n",
      "|    total_timesteps      | 653312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020703545 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000183    |\n",
      "|    n_updates            | 3180         |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 0.000189     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 654000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003833026 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00511    |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.000393   |\n",
      "|    value_loss           | 0.000168    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 16665    |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 657000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007470313 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00751     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 16676    |\n",
      "|    total_timesteps | 657408   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 3.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 16700       |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004926211 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030587977 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.466        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00373      |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 0.000214     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 16736    |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 663000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002826157 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.126      |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 0.000122    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 16785    |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.9e+03      |\n",
      "|    ep_rew_mean          | 3.23         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 325          |\n",
      "|    time_elapsed         | 16819        |\n",
      "|    total_timesteps      | 665600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017826672 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.596        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00633     |\n",
      "|    n_updates            | 3240         |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 8.68e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 666000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015235629 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00826    |\n",
      "|    n_updates            | 3250        |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 16832    |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 669000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028317649 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | 0.000949    |\n",
      "|    value_loss           | 0.00217     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 16857    |\n",
      "|    total_timesteps | 669696   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.9e+03      |\n",
      "|    ep_rew_mean          | 3.23         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 328          |\n",
      "|    time_elapsed         | 16895        |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029792306 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.092        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000309    |\n",
      "|    n_updates            | 3270         |\n",
      "|    policy_gradient_loss | -0.000881    |\n",
      "|    value_loss           | 0.000246     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038986024 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.264        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00395     |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 0.000163     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.9e+03  |\n",
      "|    ep_rew_mean     | 3.23     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 16945    |\n",
      "|    total_timesteps | 673792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 675000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031605803 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00276     |\n",
      "|    n_updates            | 3290         |\n",
      "|    policy_gradient_loss | -0.000448    |\n",
      "|    value_loss           | 0.000133     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 16973    |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | 3.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 16987       |\n",
      "|    total_timesteps      | 677888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007937487 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00856     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 678000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036405316 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.306        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 3310         |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 0.000927     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 17014    |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 681000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002076841 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00591     |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    value_loss           | 0.000307    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 17053    |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 684000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036158338 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.174       |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 0.000346     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 39       |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 17105    |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.8e+03      |\n",
      "|    ep_rew_mean          | 3.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 335          |\n",
      "|    time_elapsed         | 17128        |\n",
      "|    total_timesteps      | 686080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029329401 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0013       |\n",
      "|    n_updates            | 3340         |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 9.84e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 687000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005953258 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 17143    |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 690000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015302054 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.344        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00241     |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.000911    |\n",
      "|    value_loss           | 0.000603     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 17172    |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.8e+03      |\n",
      "|    ep_rew_mean          | 3.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 338          |\n",
      "|    time_elapsed         | 17213        |\n",
      "|    total_timesteps      | 692224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029139062 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.462        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00659     |\n",
      "|    n_updates            | 3370         |\n",
      "|    policy_gradient_loss | -0.000367    |\n",
      "|    value_loss           | 0.00014      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 693000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033302594 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.148       |\n",
      "|    explained_variance   | 0.197        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0102       |\n",
      "|    n_updates            | 3380         |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 0.000297     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.8e+03  |\n",
      "|    ep_rew_mean     | 3.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 339      |\n",
      "|    time_elapsed    | 17266    |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 696000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019105959 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.594        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00772     |\n",
      "|    n_updates            | 3390         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 8.51e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 17283    |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.4e+03      |\n",
      "|    ep_rew_mean          | 2.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 341          |\n",
      "|    time_elapsed         | 17300        |\n",
      "|    total_timesteps      | 698368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048836125 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0049       |\n",
      "|    n_updates            | 3400         |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 699000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050708726 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0144       |\n",
      "|    n_updates            | 3410         |\n",
      "|    policy_gradient_loss | -0.00366     |\n",
      "|    value_loss           | 0.000417     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 17329    |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 702000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004300605 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00729     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00284    |\n",
      "|    value_loss           | 0.000199    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 343      |\n",
      "|    time_elapsed    | 17371    |\n",
      "|    total_timesteps | 702464   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.4e+03   |\n",
      "|    ep_rew_mean          | 2.16      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 40        |\n",
      "|    iterations           | 344       |\n",
      "|    time_elapsed         | 17426     |\n",
      "|    total_timesteps      | 704512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0055664 |\n",
      "|    clip_fraction        | 0.0309    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0634   |\n",
      "|    explained_variance   | -0.899    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00308  |\n",
      "|    n_updates            | 3430      |\n",
      "|    policy_gradient_loss | 0.00039   |\n",
      "|    value_loss           | 0.00303   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 705000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034361356 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0314      |\n",
      "|    explained_variance   | 0.729        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00544      |\n",
      "|    n_updates            | 3440         |\n",
      "|    policy_gradient_loss | -0.000956    |\n",
      "|    value_loss           | 0.00565      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 345      |\n",
      "|    time_elapsed    | 17437    |\n",
      "|    total_timesteps | 706560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 708000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010777523 |\n",
      "|    clip_fraction        | 0.0544      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0942     |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00406     |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 17455    |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.2e+03      |\n",
      "|    ep_rew_mean          | 1.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 347          |\n",
      "|    time_elapsed         | 17486        |\n",
      "|    total_timesteps      | 710656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048699966 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.464        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00962     |\n",
      "|    n_updates            | 3460         |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    value_loss           | 0.000217     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 711000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042891735 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00464     |\n",
      "|    n_updates            | 3470         |\n",
      "|    policy_gradient_loss | -0.005       |\n",
      "|    value_loss           | 0.00117      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 17530    |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 714000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016814079 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0367      |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00389     |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 0.0015       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 349      |\n",
      "|    time_elapsed    | 17585    |\n",
      "|    total_timesteps | 714752   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.3e+03     |\n",
      "|    ep_rew_mean          | 1.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 17593       |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004174745 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0424     |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00448     |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 717000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006535424 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    value_loss           | 0.019       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 351      |\n",
      "|    time_elapsed    | 17612    |\n",
      "|    total_timesteps | 718848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063158567 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.308        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0272      |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -0.00905     |\n",
      "|    value_loss           | 0.000395     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 17645    |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.3e+03      |\n",
      "|    ep_rew_mean          | 1.86         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 353          |\n",
      "|    time_elapsed         | 17690        |\n",
      "|    total_timesteps      | 722944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035591074 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.335        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0073      |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 0.000339     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=723000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 723000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003081717 |\n",
      "|    clip_fraction        | 0.0211      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00699     |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | -0.000806   |\n",
      "|    value_loss           | 9.24e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 17739    |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 726000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988686 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0714     |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00431    |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 355      |\n",
      "|    time_elapsed    | 17747    |\n",
      "|    total_timesteps | 727040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 729000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039873435 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00201      |\n",
      "|    n_updates            | 3550         |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    value_loss           | 0.0121       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 17769    |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.2e+03      |\n",
      "|    ep_rew_mean          | 1.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 357          |\n",
      "|    time_elapsed         | 17803        |\n",
      "|    total_timesteps      | 731136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014145436 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.542        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00331     |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -0.000731    |\n",
      "|    value_loss           | 0.000152     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 732000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019266688 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0277       |\n",
      "|    n_updates            | 3570         |\n",
      "|    policy_gradient_loss | 9.13e-05     |\n",
      "|    value_loss           | 9.2e-05      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.2e+03  |\n",
      "|    ep_rew_mean     | 1.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 17849    |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 735000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034537227 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.705        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00971     |\n",
      "|    n_updates            | 3580         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 7.62e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 359      |\n",
      "|    time_elapsed    | 17892    |\n",
      "|    total_timesteps | 735232   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.3e+03     |\n",
      "|    ep_rew_mean          | 1.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 360         |\n",
      "|    time_elapsed         | 17902       |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011990787 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00123     |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 738000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026394373 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00429      |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 361      |\n",
      "|    time_elapsed    | 17925    |\n",
      "|    total_timesteps | 739328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 741000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004088597 |\n",
      "|    clip_fraction        | 0.0506      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.186      |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000589   |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.000885   |\n",
      "|    value_loss           | 0.000156    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 17961    |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.3e+03      |\n",
      "|    ep_rew_mean          | 1.85         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 363          |\n",
      "|    time_elapsed         | 18009        |\n",
      "|    total_timesteps      | 743424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059441337 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.295        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00215     |\n",
      "|    n_updates            | 3620         |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 0.000476     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 744000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003927268 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00162    |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    value_loss           | 0.000228    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 18047    |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 747000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006347684 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0176      |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 365      |\n",
      "|    time_elapsed    | 18059    |\n",
      "|    total_timesteps | 747520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.3e+03   |\n",
      "|    ep_rew_mean          | 1.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 41        |\n",
      "|    iterations           | 366       |\n",
      "|    time_elapsed         | 18083     |\n",
      "|    total_timesteps      | 749568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0040392 |\n",
      "|    clip_fraction        | 0.0409    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.173    |\n",
      "|    explained_variance   | 0.157     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0147   |\n",
      "|    n_updates            | 3650      |\n",
      "|    policy_gradient_loss | -0.0034   |\n",
      "|    value_loss           | 0.00362   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 750000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034408718 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.188       |\n",
      "|    explained_variance   | 0.131        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000802     |\n",
      "|    n_updates            | 3660         |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 0.000269     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 18119    |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 753000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015170495 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.349        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0023      |\n",
      "|    n_updates            | 3670         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 0.000153     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 18168    |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.4e+03     |\n",
      "|    ep_rew_mean          | 2.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 369         |\n",
      "|    time_elapsed         | 18201       |\n",
      "|    total_timesteps      | 755712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004580272 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.504       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00294    |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 0.000111    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 756000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051438916 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.15       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 18214    |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 759000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003493587 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00413     |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    value_loss           | 0.00228     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 371      |\n",
      "|    time_elapsed    | 18240    |\n",
      "|    total_timesteps | 759808   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.4e+03      |\n",
      "|    ep_rew_mean          | 2.06         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 372          |\n",
      "|    time_elapsed         | 18278        |\n",
      "|    total_timesteps      | 761856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028865493 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00445      |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | -0.000499    |\n",
      "|    value_loss           | 0.000194     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003130726 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000449   |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    value_loss           | 0.000388    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 373      |\n",
      "|    time_elapsed    | 18329    |\n",
      "|    total_timesteps | 763904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 765000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031083778 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000395     |\n",
      "|    n_updates            | 3730         |\n",
      "|    policy_gradient_loss | -0.00392     |\n",
      "|    value_loss           | 0.000879     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 18356    |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010959288 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0048     |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 18370    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 2.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 376         |\n",
      "|    time_elapsed         | 18397       |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002635949 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.183      |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00963    |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 0.00083     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 771000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017100037 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00307     |\n",
      "|    n_updates            | 3760         |\n",
      "|    policy_gradient_loss | -5.65e-05    |\n",
      "|    value_loss           | 0.000149     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 18436    |\n",
      "|    total_timesteps | 772096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 774000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004008763 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000906   |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 0.000101    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 41       |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 18488    |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 41           |\n",
      "|    iterations           | 379          |\n",
      "|    time_elapsed         | 18510        |\n",
      "|    total_timesteps      | 776192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018125746 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000226    |\n",
      "|    n_updates            | 3780         |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 8.53e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 777000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066481777 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00837      |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 18526    |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 780000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032617073 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00853     |\n",
      "|    n_updates            | 3800         |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 0.000515     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 18554    |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 382          |\n",
      "|    time_elapsed         | 18595        |\n",
      "|    total_timesteps      | 782336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014220136 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.005        |\n",
      "|    n_updates            | 3810         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.00014      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 783000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013392906 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | 0.559        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000958    |\n",
      "|    n_updates            | 3820         |\n",
      "|    policy_gradient_loss | -0.000313    |\n",
      "|    value_loss           | 9.33e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 18649    |\n",
      "|    total_timesteps | 784384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 786000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048462017 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.646        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00244      |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -0.000643    |\n",
      "|    value_loss           | 7.84e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 18665    |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.6e+03     |\n",
      "|    ep_rew_mean          | 2.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 18682       |\n",
      "|    total_timesteps      | 788480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010366453 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.103      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.00434    |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 789000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012172298 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00263      |\n",
      "|    n_updates            | 3850         |\n",
      "|    policy_gradient_loss | -0.000326    |\n",
      "|    value_loss           | 0.000353     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 18712    |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 792000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023368148 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.556        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000427     |\n",
      "|    n_updates            | 3860         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 0.000123     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 18755    |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 388          |\n",
      "|    time_elapsed         | 18810        |\n",
      "|    total_timesteps      | 794624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026777708 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0433      |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00405     |\n",
      "|    n_updates            | 3870         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1             |\n",
      "|    mean_reward          | -0.867        |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 795000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00079228665 |\n",
      "|    clip_fraction        | 0.00332       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00838      |\n",
      "|    explained_variance   | 0.781         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00163      |\n",
      "|    n_updates            | 3880          |\n",
      "|    policy_gradient_loss | -0.000936     |\n",
      "|    value_loss           | 0.0046        |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 18820    |\n",
      "|    total_timesteps | 796672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 798000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010130542 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0941     |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.925       |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 18840    |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.45         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 391          |\n",
      "|    time_elapsed         | 18871        |\n",
      "|    total_timesteps      | 800768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037029197 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.519        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0274      |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 0.000261     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 801000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006210931 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00109     |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.00353    |\n",
      "|    value_loss           | 0.000408    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 18915    |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 804000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034679933 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.417        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00146     |\n",
      "|    n_updates            | 3920         |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 0.000578     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 18967    |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.4e+03     |\n",
      "|    ep_rew_mean          | 2.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 394         |\n",
      "|    time_elapsed         | 18975       |\n",
      "|    total_timesteps      | 806912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020683128 |\n",
      "|    clip_fraction        | 0.0871      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00108    |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 807000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00713758 |\n",
      "|    clip_fraction        | 0.0386     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.107     |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00716    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0055    |\n",
      "|    value_loss           | 0.0247     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 18995    |\n",
      "|    total_timesteps | 808960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 810000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022428068 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.146        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0118      |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 0.000511     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 19028    |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 813000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045967204 |\n",
      "|    clip_fraction        | 0.0249       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00556     |\n",
      "|    n_updates            | 3960         |\n",
      "|    policy_gradient_loss | -0.000366    |\n",
      "|    value_loss           | 0.000115     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 2.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 19073    |\n",
      "|    total_timesteps | 813056   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.3e+03      |\n",
      "|    ep_rew_mean          | 1.76         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 398          |\n",
      "|    time_elapsed         | 19121        |\n",
      "|    total_timesteps      | 815104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055229617 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0564      |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0025       |\n",
      "|    n_updates            | 3970         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005084951 |\n",
      "|    clip_fraction        | 0.0218      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0437     |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 19130    |\n",
      "|    total_timesteps | 817152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 819000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034114255 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00467      |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 0.00593      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 19152    |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.3e+03      |\n",
      "|    ep_rew_mean          | 1.76         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 42           |\n",
      "|    iterations           | 401          |\n",
      "|    time_elapsed         | 19186        |\n",
      "|    total_timesteps      | 821248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053063715 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0831      |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0158      |\n",
      "|    n_updates            | 4000         |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 822000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008094032 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0487     |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00451    |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 19233    |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 825000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009812763 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0458     |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.007      |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 19275    |\n",
      "|    total_timesteps | 825344   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.3e+03     |\n",
      "|    ep_rew_mean          | 1.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 404         |\n",
      "|    time_elapsed         | 19286       |\n",
      "|    total_timesteps      | 827392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008331528 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0542     |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0269      |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | -0.0071     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 828000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031967694 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0146      |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -0.0034      |\n",
      "|    value_loss           | 0.0054       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 19309    |\n",
      "|    total_timesteps | 829440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 831000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046669836 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00254     |\n",
      "|    n_updates            | 4050         |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    value_loss           | 0.00011      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.3e+03  |\n",
      "|    ep_rew_mean     | 1.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 42       |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 19345    |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.3e+03     |\n",
      "|    ep_rew_mean          | 1.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 407         |\n",
      "|    time_elapsed         | 19393       |\n",
      "|    total_timesteps      | 833536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008966152 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00729    |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 7.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 834000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004232888 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 5.65e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 1.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 19429    |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=837000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 837000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085853515 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.188       |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0125       |\n",
      "|    n_updates            | 4080         |\n",
      "|    policy_gradient_loss | -0.00678     |\n",
      "|    value_loss           | 0.142        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 1.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 19441    |\n",
      "|    total_timesteps | 837632   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.4e+03      |\n",
      "|    ep_rew_mean          | 1.94         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 410          |\n",
      "|    time_elapsed         | 19466        |\n",
      "|    total_timesteps      | 839680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057211258 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.504        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00333     |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046974234 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00257     |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    value_loss           | 0.000113     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 1.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 19503    |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 843000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026130257 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00916      |\n",
      "|    n_updates            | 4110         |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 8.11e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.4e+03  |\n",
      "|    ep_rew_mean     | 1.94     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 19553    |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 2.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 413          |\n",
      "|    time_elapsed         | 19584        |\n",
      "|    total_timesteps      | 845824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060059507 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00379      |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 6.61e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 846000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051224725 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.142       |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000508    |\n",
      "|    n_updates            | 4130         |\n",
      "|    policy_gradient_loss | -0.00588     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 19597    |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 849000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033373283 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.274        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0105      |\n",
      "|    n_updates            | 4140         |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 0.00125      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 19623    |\n",
      "|    total_timesteps | 849920   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.5e+03     |\n",
      "|    ep_rew_mean          | 2.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 416         |\n",
      "|    time_elapsed         | 19662       |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002231452 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00379    |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 0.000152    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 852000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002843591 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00165     |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 19713    |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 855000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03027049 |\n",
      "|    clip_fraction        | 0.0363     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0425    |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00219   |\n",
      "|    n_updates            | 4170       |\n",
      "|    policy_gradient_loss | -0.00218   |\n",
      "|    value_loss           | 0.00554    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 19738    |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 858000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071395384 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0573      |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00555     |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -0.00432     |\n",
      "|    value_loss           | 0.144        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 419      |\n",
      "|    time_elapsed    | 19753    |\n",
      "|    total_timesteps | 858112   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.5e+03      |\n",
      "|    ep_rew_mean          | 2.18         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 420          |\n",
      "|    time_elapsed         | 19781        |\n",
      "|    total_timesteps      | 860160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023730504 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00657     |\n",
      "|    n_updates            | 4190         |\n",
      "|    policy_gradient_loss | -0.000876    |\n",
      "|    value_loss           | 0.000573     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 861000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029317879 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.209       |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00715     |\n",
      "|    n_updates            | 4200         |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    value_loss           | 0.000126     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 421      |\n",
      "|    time_elapsed    | 19821    |\n",
      "|    total_timesteps | 862208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 864000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004390794 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 9.27e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.5e+03  |\n",
      "|    ep_rew_mean     | 2.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 19874    |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 423          |\n",
      "|    time_elapsed         | 19894        |\n",
      "|    total_timesteps      | 866304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040447563 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00332      |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 7.16e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 867000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043447064 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00689      |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | -0.00381     |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 19910    |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 870000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025125383 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00575     |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.00298     |\n",
      "|    value_loss           | 0.000412     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 19939    |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6e+03      |\n",
      "|    ep_rew_mean          | 2.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 426          |\n",
      "|    time_elapsed         | 19981        |\n",
      "|    total_timesteps      | 872448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017200785 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.307        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.016       |\n",
      "|    n_updates            | 4250         |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    value_loss           | 0.000587     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 873000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020586683 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00153      |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 9.03e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.6e+03  |\n",
      "|    ep_rew_mean     | 2.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 20035    |\n",
      "|    total_timesteps | 874496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 876000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020421823 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0131      |\n",
      "|    n_updates            | 4270         |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 6.41e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 2.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 20050    |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.7e+03     |\n",
      "|    ep_rew_mean          | 2.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 429         |\n",
      "|    time_elapsed         | 20068       |\n",
      "|    total_timesteps      | 878592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010321703 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00227     |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 879000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030890498 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.243        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00146      |\n",
      "|    n_updates            | 4290         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 0.000322     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 2.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 20098    |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 882000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022604144 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.245        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000695     |\n",
      "|    n_updates            | 4300         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 0.00109      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.7e+03  |\n",
      "|    ep_rew_mean     | 2.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 431      |\n",
      "|    time_elapsed    | 20141    |\n",
      "|    total_timesteps | 882688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.7e+03    |\n",
      "|    ep_rew_mean          | 2.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 43         |\n",
      "|    iterations           | 432        |\n",
      "|    time_elapsed         | 20196      |\n",
      "|    total_timesteps      | 884736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14851677 |\n",
      "|    clip_fraction        | 0.0412     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0368    |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 4310       |\n",
      "|    policy_gradient_loss | -0.00596   |\n",
      "|    value_loss           | 0.00623    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 885000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004707824 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0442     |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00522    |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 0.0018      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.724    |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 433      |\n",
      "|    time_elapsed    | 20205    |\n",
      "|    total_timesteps | 886784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 888000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037894562 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.095      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00241     |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.724    |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 20224    |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 801          |\n",
      "|    ep_rew_mean          | 0.724        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 43           |\n",
      "|    iterations           | 435          |\n",
      "|    time_elapsed         | 20255        |\n",
      "|    total_timesteps      | 890880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035433923 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0038      |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -0.00366     |\n",
      "|    value_loss           | 0.000199     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 891000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034819758 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 4350         |\n",
      "|    policy_gradient_loss | -0.000773    |\n",
      "|    value_loss           | 7.58e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.724    |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 20300    |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 894000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011232546 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0597     |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00701    |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | -0.00506    |\n",
      "|    value_loss           | 0.00232     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.909    |\n",
      "| time/              |          |\n",
      "|    fps             | 43       |\n",
      "|    iterations      | 437      |\n",
      "|    time_elapsed    | 20352    |\n",
      "|    total_timesteps | 894976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 897000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040275306 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0431      |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0119      |\n",
      "|    n_updates            | 4370         |\n",
      "|    policy_gradient_loss | -0.008       |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.909    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 20360    |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 901         |\n",
      "|    ep_rew_mean          | 0.909       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 20380       |\n",
      "|    total_timesteps      | 899072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005140246 |\n",
      "|    clip_fraction        | 0.019       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00504     |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009684796 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.909    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 20413    |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 903000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019469332 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0692     |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00831    |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.00811    |\n",
      "|    value_loss           | 0.000826    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.909    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 441      |\n",
      "|    time_elapsed    | 20458    |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 901          |\n",
      "|    ep_rew_mean          | 0.939        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 442          |\n",
      "|    time_elapsed         | 20505        |\n",
      "|    total_timesteps      | 905216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028169518 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0107      |\n",
      "|    n_updates            | 4410         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 0.000167     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 906000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012704397 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00526    |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.00639    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.939    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 443      |\n",
      "|    time_elapsed    | 20515    |\n",
      "|    total_timesteps | 907264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 909000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009605646 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    value_loss           | 0.0201      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.939    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 20536    |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 901          |\n",
      "|    ep_rew_mean          | 0.939        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 445          |\n",
      "|    time_elapsed         | 20571        |\n",
      "|    total_timesteps      | 911360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057524825 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000708    |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 0.000147     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 912000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013958093 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.193      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 0.939    |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 20618    |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 915000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00387446 |\n",
      "|    clip_fraction        | 0.0521     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.216     |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00265   |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00467   |\n",
      "|    value_loss           | 0.000182   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 447      |\n",
      "|    time_elapsed    | 20659    |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 1.16         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 448          |\n",
      "|    time_elapsed         | 20670        |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085727265 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00106     |\n",
      "|    n_updates            | 4470         |\n",
      "|    policy_gradient_loss | -0.00971     |\n",
      "|    value_loss           | 0.166        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 918000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036588076 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.012       |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    value_loss           | 0.00511      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 449      |\n",
      "|    time_elapsed    | 20693    |\n",
      "|    total_timesteps | 919552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 921000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00477421 |\n",
      "|    clip_fraction        | 0.056      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.233     |\n",
      "|    explained_variance   | 0.457      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 4490       |\n",
      "|    policy_gradient_loss | -0.00506   |\n",
      "|    value_loss           | 0.000317   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 20729    |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 451         |\n",
      "|    time_elapsed         | 20778       |\n",
      "|    total_timesteps      | 923648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013174561 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    value_loss           | 8.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 924000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012333432 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 20814    |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=927000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 927000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005922118 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.565       |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.00957    |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 453      |\n",
      "|    time_elapsed    | 20826    |\n",
      "|    total_timesteps | 927744   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 1.18         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 454          |\n",
      "|    time_elapsed         | 20851        |\n",
      "|    total_timesteps      | 929792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052411426 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.0725       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00858     |\n",
      "|    n_updates            | 4530         |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 0.00292      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013391366 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.614        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0011       |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 0.000167     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 20888    |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 933000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002351204 |\n",
      "|    clip_fraction        | 0.0249      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000472   |\n",
      "|    n_updates            | 4550        |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 0.000127    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 20938    |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.1e+03      |\n",
      "|    ep_rew_mean          | 1.4          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 457          |\n",
      "|    time_elapsed         | 20968        |\n",
      "|    total_timesteps      | 935936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031773592 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -0.00415     |\n",
      "|    value_loss           | 7.85e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 936000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008056835 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0269      |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 20982    |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=939000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 939000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003149927 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00437     |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 21008    |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 942000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002410981 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00867    |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 21047    |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.1e+03    |\n",
      "|    ep_rew_mean          | 1.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 461        |\n",
      "|    time_elapsed         | 21098      |\n",
      "|    total_timesteps      | 944128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00203327 |\n",
      "|    clip_fraction        | 0.0275     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.226     |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00536   |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00315   |\n",
      "|    value_loss           | 9.01e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 945000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003932772 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000232    |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 7.17e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 21123    |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 948000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007951349 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.336       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 21138    |\n",
      "|    total_timesteps | 948224   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 1.18         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 464          |\n",
      "|    time_elapsed         | 21165        |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041763624 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.234       |\n",
      "|    explained_variance   | 0.292        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00181     |\n",
      "|    n_updates            | 4630         |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 0.00103      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1         |\n",
      "|    mean_reward          | -0.867    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 951000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0072253 |\n",
      "|    clip_fraction        | 0.0874    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.26     |\n",
      "|    explained_variance   | 0.432     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00507  |\n",
      "|    n_updates            | 4640      |\n",
      "|    policy_gradient_loss | -0.00826  |\n",
      "|    value_loss           | 0.000726  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 21205    |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 954000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051535373 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.483        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00119      |\n",
      "|    n_updates            | 4650         |\n",
      "|    policy_gradient_loss | -0.00502     |\n",
      "|    value_loss           | 0.000456     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 21258    |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 467         |\n",
      "|    time_elapsed         | 21278       |\n",
      "|    total_timesteps      | 956416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003357071 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00452    |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.00386    |\n",
      "|    value_loss           | 0.000102    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 957000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008432893 |\n",
      "|    clip_fraction        | 0.0455      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 21294    |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 960000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019440851 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.234       |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0118      |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -0.000916    |\n",
      "|    value_loss           | 0.000446     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 21323    |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 470         |\n",
      "|    time_elapsed         | 21364       |\n",
      "|    total_timesteps      | 962560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002710861 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00374    |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 0.000121    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 963000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026486625 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000599     |\n",
      "|    n_updates            | 4700         |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 9.33e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 21418    |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 966000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007086369 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00183    |\n",
      "|    n_updates            | 4710        |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.807    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 21432    |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 801        |\n",
      "|    ep_rew_mean          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 473        |\n",
      "|    time_elapsed         | 21450      |\n",
      "|    total_timesteps      | 968704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01100887 |\n",
      "|    clip_fraction        | 0.0641     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.124     |\n",
      "|    explained_variance   | 0.935      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0076     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 969000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003575175 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.001      |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 0.000276    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.807    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 21480    |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 972000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007043259 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.000588    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 0.807    |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 21523    |\n",
      "|    total_timesteps | 972800   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 801          |\n",
      "|    ep_rew_mean          | 0.807        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 476          |\n",
      "|    time_elapsed         | 21578        |\n",
      "|    total_timesteps      | 974848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023300662 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00299      |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    value_loss           | 0.000169     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 975000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063692573 |\n",
      "|    clip_fraction        | 0.0899       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.591        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0223      |\n",
      "|    n_updates            | 4760         |\n",
      "|    policy_gradient_loss | -0.0088      |\n",
      "|    value_loss           | 0.00108      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 21587    |\n",
      "|    total_timesteps | 976896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 978000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010022949 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00461     |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 21606    |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 901          |\n",
      "|    ep_rew_mean          | 1.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 479          |\n",
      "|    time_elapsed         | 21638        |\n",
      "|    total_timesteps      | 980992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039284164 |\n",
      "|    clip_fraction        | 0.0579       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 4780         |\n",
      "|    policy_gradient_loss | -0.00581     |\n",
      "|    value_loss           | 0.000475     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 981000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027710393 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00289     |\n",
      "|    n_updates            | 4790         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 5.93e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 1.02     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 21682    |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 984000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010016705 |\n",
      "|    clip_fraction        | 0.0829      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00975    |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.00688    |\n",
      "|    value_loss           | 0.000807    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 21734    |\n",
      "|    total_timesteps | 985088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 987000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010132104 |\n",
      "|    clip_fraction        | 0.0905      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -1.58e-05   |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    value_loss           | 0.0754      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 21742    |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 483         |\n",
      "|    time_elapsed         | 21763       |\n",
      "|    total_timesteps      | 989184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004696734 |\n",
      "|    clip_fraction        | 0.0261      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00387     |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    value_loss           | 0.0206      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 990000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027987752 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.68         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    value_loss           | 0.00017      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 21796    |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | -0.867       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 993000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036559254 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00383      |\n",
      "|    n_updates            | 4840         |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 7.27e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 21842    |\n",
      "|    total_timesteps | 993280   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.1e+03      |\n",
      "|    ep_rew_mean          | 1.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 486          |\n",
      "|    time_elapsed         | 21888        |\n",
      "|    total_timesteps      | 995328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017010923 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00165      |\n",
      "|    n_updates            | 4850         |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 6.15e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 996000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012832623 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.21       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000394    |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0085     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 21897    |\n",
      "|    total_timesteps | 997376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 999000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004025737 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000766    |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    value_loss           | 0.008       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1e+03  |\n",
      "|    ep_rew_mean     | 1.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 21920    |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.1e+03      |\n",
      "|    ep_rew_mean          | 1.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 489          |\n",
      "|    time_elapsed         | 21954        |\n",
      "|    total_timesteps      | 1001472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027737743 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00236      |\n",
      "|    n_updates            | 4880         |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 0.00031      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f82498c2220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models-sde/',\n",
    "                             n_eval_episodes=1,\n",
    "                             log_path='./rl-logs/', eval_freq=3_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = PPO(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            device=\"cpu\",\n",
    "            # n_steps=128,\n",
    "            # learning_rate=0.0015,\n",
    "            # batch_size=16,\n",
    ")\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name=\"ppo\",\n",
    "            callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./rl-logs/ppo_11\n",
      "Eval num_timesteps=1000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.69     |\n",
      "|    ep_rew_mean     | -0.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 2186     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019040428 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.792       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.69     |\n",
      "|    ep_rew_mean     | -0.73    |\n",
      "| time/              |          |\n",
      "|    fps             | 201      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.08e+03   |\n",
      "|    mean_reward          | -0.0195    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02149458 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.631     |\n",
      "|    explained_variance   | 0.301      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0824     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 0.351      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 44.4     |\n",
      "|    ep_rew_mean     | -0.708   |\n",
      "| time/              |          |\n",
      "|    fps             | 121      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008829663 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.098       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 44.4     |\n",
      "|    ep_rew_mean     | -0.708   |\n",
      "| time/              |          |\n",
      "|    fps             | 117      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019348484 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.29        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.9     |\n",
      "|    ep_rew_mean     | -0.699   |\n",
      "| time/              |          |\n",
      "|    fps             | 109      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007941926 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.369       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -0.694   |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1.08e+03   |\n",
      "|    mean_reward          | -0.0195    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01243849 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.583     |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0711     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    value_loss           | 0.357      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -0.694   |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.08e+03     |\n",
      "|    mean_reward          | -0.0195      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072518764 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0.194        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.07         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0129      |\n",
      "|    value_loss           | 0.271        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 133      |\n",
      "|    ep_rew_mean     | -0.695   |\n",
      "| time/              |          |\n",
      "|    fps             | 104      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.08e+03     |\n",
      "|    mean_reward          | -0.0195      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033258367 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.192        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0046      |\n",
      "|    value_loss           | 0.541        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | -0.691   |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1231.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.23e+03     |\n",
      "|    mean_reward          | -0.00636     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060577705 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0.345        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.262        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00628     |\n",
      "|    value_loss           | 0.732        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 1231.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.23e+03 |\n",
      "|    mean_reward     | -0.00636 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | -0.691   |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 203      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 2028.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.03e+03    |\n",
      "|    mean_reward          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006892693 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 2028.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.03e+03 |\n",
      "|    mean_reward     | -0.065   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 190      |\n",
      "|    ep_rew_mean     | -0.683   |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.08e+03    |\n",
      "|    mean_reward          | -0.0195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007877817 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.44        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 0.896       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1075.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.08e+03 |\n",
      "|    mean_reward     | -0.0195  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | -0.683   |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 1473.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1.47e+03     |\n",
      "|    mean_reward          | -0.161       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065583745 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.207        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00643     |\n",
      "|    value_loss           | 0.761        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 1473.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.47e+03 |\n",
      "|    mean_reward     | -0.161   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 246      |\n",
      "|    ep_rew_mean     | -0.675   |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 1903.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.9e+03     |\n",
      "|    mean_reward          | -0.0547     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012965975 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.441       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 1903.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.9e+03  |\n",
      "|    mean_reward     | -0.0547  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 246      |\n",
      "|    ep_rew_mean     | -0.675   |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1741.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1.74e+03    |\n",
      "|    mean_reward          | -0.0242     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006338424 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.361       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.725       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 1741.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1.74e+03 |\n",
      "|    mean_reward     | -0.0242  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | -0.671   |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 2182.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.18e+03   |\n",
      "|    mean_reward          | -0.0212    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 31000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00550538 |\n",
      "|    clip_fraction        | 0.0513     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.423     |\n",
      "|    explained_variance   | 0.583      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.248      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.00593   |\n",
      "|    value_loss           | 0.739      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 2182.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.18e+03 |\n",
      "|    mean_reward     | -0.0212  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | -0.671   |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 446      |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 3275.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.28e+03    |\n",
      "|    mean_reward          | -0.0462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019235862 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | -1.06       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00775     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 3275.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.28e+03 |\n",
      "|    mean_reward     | -0.0462  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 313      |\n",
      "|    ep_rew_mean     | -0.663   |\n",
      "| time/              |          |\n",
      "|    fps             | 62       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 554      |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 2675.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.68e+03    |\n",
      "|    mean_reward          | -0.0646     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009640408 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.489       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 2675.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.68e+03 |\n",
      "|    mean_reward     | -0.0646  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 313      |\n",
      "|    ep_rew_mean     | -0.663   |\n",
      "| time/              |          |\n",
      "|    fps             | 57       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 3352.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.35e+03    |\n",
      "|    mean_reward          | -0.00252    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015842285 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.0719      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.0444      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 3352.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.35e+03 |\n",
      "|    mean_reward     | -0.00252 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 50       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 763      |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 770.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 770        |\n",
      "|    mean_reward          | -0.0563    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 39000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00543497 |\n",
      "|    clip_fraction        | 0.0666     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.384     |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0891     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00785   |\n",
      "|    value_loss           | 0.372      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 770.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 770      |\n",
      "|    mean_reward     | -0.0563  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 52       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 780      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 3203.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2e+03     |\n",
      "|    mean_reward          | -0.0676     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013111189 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0015     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0401      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 3203.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2e+03  |\n",
      "|    mean_reward     | -0.0676  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 3203.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2e+03  |\n",
      "|    mean_reward     | -0.0676  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 946      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 720.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 720       |\n",
      "|    mean_reward          | -0.0298   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0223159 |\n",
      "|    clip_fraction        | 0.312     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.511    |\n",
      "|    explained_variance   | 0.6       |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0191   |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | -0.0332   |\n",
      "|    value_loss           | 0.0238    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 720.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 720      |\n",
      "|    mean_reward     | -0.0298  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 988      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 591.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 591         |\n",
      "|    mean_reward          | -0.175      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013825965 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00651    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.023       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 591.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 591      |\n",
      "|    mean_reward     | -0.175   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 364      |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017747324 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 464      |\n",
      "|    ep_rew_mean     | -0.552   |\n",
      "| time/              |          |\n",
      "|    fps             | 45       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 1075     |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 627.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 627        |\n",
      "|    mean_reward          | -0.202     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01772719 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.54       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.124      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    value_loss           | 0.409      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 627.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 627      |\n",
      "|    mean_reward     | -0.202   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 464      |\n",
      "|    ep_rew_mean     | -0.552   |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 1092     |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 752.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 752        |\n",
      "|    mean_reward          | -0.0362    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01262612 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.364     |\n",
      "|    explained_variance   | 0.497      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    value_loss           | 0.0882     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 752.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | -0.0362  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 464      |\n",
      "|    ep_rew_mean     | -0.552   |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 1123     |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 655.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 655         |\n",
      "|    mean_reward          | -0.0782     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016648445 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.00855     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 655.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 655      |\n",
      "|    mean_reward     | -0.0782  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 464      |\n",
      "|    ep_rew_mean     | -0.552   |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 1165     |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016682092 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.0097      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 464      |\n",
      "|    ep_rew_mean     | -0.552   |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 1215     |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 631.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 631         |\n",
      "|    mean_reward          | -0.0292     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014075169 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00279    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 631.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 631      |\n",
      "|    mean_reward     | -0.0292  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | -0.402   |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 1248     |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 688.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 688         |\n",
      "|    mean_reward          | -0.0768     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011018738 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.287       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 688.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 688      |\n",
      "|    mean_reward     | -0.0768  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | -0.402   |\n",
      "| time/              |          |\n",
      "|    fps             | 48       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 1267     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=10.89 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 10.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012989355 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0028      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.0205      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=63000, episode_reward=10.89 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 10.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | -0.402   |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 2193     |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 675.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 675         |\n",
      "|    mean_reward          | -0.0724     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018908072 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.0116      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 675.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 675      |\n",
      "|    mean_reward     | -0.0724  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | -0.402   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 2236     |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 598.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 598         |\n",
      "|    mean_reward          | -0.126      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017216882 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    value_loss           | 0.00739     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-0.13 +/- 0.00\n",
      "Episode length: 598.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 598      |\n",
      "|    mean_reward     | -0.126   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | -0.402   |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 2291     |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017813582 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00961    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | -0.262   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 2315     |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 610.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 610         |\n",
      "|    mean_reward          | -0.00433    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014832451 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.277       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 610.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 610      |\n",
      "|    mean_reward     | -0.00433 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | -0.262   |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 2335     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 688.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 688         |\n",
      "|    mean_reward          | -0.0768     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013999081 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00201     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0231      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 688.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 688      |\n",
      "|    mean_reward     | -0.0768  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | -0.262   |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 2367     |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 603.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 603         |\n",
      "|    mean_reward          | -0.0479     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014844173 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.00783     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 603.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 603      |\n",
      "|    mean_reward     | -0.0479  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | -0.262   |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 2411     |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018219266 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0572     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | -0.262   |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 2464     |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156111 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | -0.0762  |\n",
      "| time/              |          |\n",
      "|    fps             | 32       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 2483     |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 665.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 665         |\n",
      "|    mean_reward          | -0.0832     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006363432 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.287       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 665.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 665      |\n",
      "|    mean_reward     | -0.0832  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | -0.0762  |\n",
      "| time/              |          |\n",
      "|    fps             | 32       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 2504     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=16.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 16.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012051939 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.00591     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=16.85 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 16.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | -0.0762  |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 3429     |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 665.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 665         |\n",
      "|    mean_reward          | -0.0832     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020400409 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.00767     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 665.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 665      |\n",
      "|    mean_reward     | -0.0832  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 665.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 665      |\n",
      "|    mean_reward     | -0.0832  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | -0.0762  |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 3478     |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012744541 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | -0.0762  |\n",
      "| time/              |          |\n",
      "|    fps             | 24       |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 3532     |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 620.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 620         |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 89000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009755056 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00669    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 0.00227     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 620.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 620      |\n",
      "|    mean_reward     | -0.101   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 3549     |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 686.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 686          |\n",
      "|    mean_reward          | -0.0178      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 91000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069482713 |\n",
      "|    clip_fraction        | 0.0931       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0108       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00832     |\n",
      "|    value_loss           | 0.235        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 686.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 686      |\n",
      "|    mean_reward     | -0.0178  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 25       |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 3572     |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 607.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 607         |\n",
      "|    mean_reward          | -0.00765    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015838163 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 607.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 607      |\n",
      "|    mean_reward     | -0.00765 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 3607     |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=20.97 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 21           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062252944 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.622       |\n",
      "|    explained_variance   | 0.458        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00469      |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    value_loss           | 0.00142      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=20.97 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 21       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 4546     |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 97000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006411277 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0025      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 4602     |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 99000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014281094 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | -3.31       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.0339      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 21       |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 4609     |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2            |\n",
      "|    mean_reward          | -0.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 101000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033991965 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.401        |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 0.0873       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 4629     |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=19.25 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 19.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 103000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011184808 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=19.25 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 19.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 18       |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 5557     |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2          |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 105000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01956616 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.467     |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 19       |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 5602     |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 107000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012683496 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0316     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.06e+03 |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 19       |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 5652     |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2            |\n",
      "|    mean_reward          | -0.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 109000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082174335 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.761        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00443      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.06e+03 |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 19       |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 5661     |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=20.12 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009379656 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00533    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00791    |\n",
      "|    value_loss           | 0.0372      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=20.12 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.06e+03 |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 17       |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 6578     |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=18.98 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 19           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 113000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049281046 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00563      |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00598     |\n",
      "|    value_loss           | 0.00292      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=18.98 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 19       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.06e+03 |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 15       |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 7508     |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=19.97 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015902188 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.00184     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=19.97 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.06e+03 |\n",
      "|    ep_rew_mean     | 0.508    |\n",
      "| time/              |          |\n",
      "|    fps             | 13       |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 8449     |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 117000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018115725 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00248     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 0.719    |\n",
      "| time/              |          |\n",
      "|    fps             | 13       |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 8493     |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=20.10 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018795889 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0108      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=20.10 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 0.719    |\n",
      "| time/              |          |\n",
      "|    fps             | 12       |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 9399     |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 683.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 683         |\n",
      "|    mean_reward          | -0.0257     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 121000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010578131 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    value_loss           | 0.0321      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 683.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 683      |\n",
      "|    mean_reward     | -0.0257  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 0.719    |\n",
      "| time/              |          |\n",
      "|    fps             | 13       |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 9427     |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 636.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 636         |\n",
      "|    mean_reward          | -0.0779     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018306741 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 636.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 636      |\n",
      "|    mean_reward     | -0.0779  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 0.719    |\n",
      "| time/              |          |\n",
      "|    fps             | 13       |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 9466     |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | -0.867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 125000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01341713 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.395     |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.16e+03 |\n",
      "|    ep_rew_mean     | 0.719    |\n",
      "| time/              |          |\n",
      "|    fps             | 13       |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 9514     |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=20.46 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015275874 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=20.46 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=20.46 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 11       |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 10896    |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=19.87 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 19.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034935367 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0205       |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    value_loss           | 0.269        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=19.87 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 19.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 11       |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 11803    |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 673.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 673         |\n",
      "|    mean_reward          | -0.00783    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008469816 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.294      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    value_loss           | 0.0372      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 673.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 673      |\n",
      "|    mean_reward     | -0.00783 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 11       |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 11832    |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=20.49 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 134000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014914695 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00663    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.00172     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=20.49 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 12764    |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 136000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015638132 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.000586    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.26e+03 |\n",
      "|    ep_rew_mean     | 0.923    |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 12813    |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 138000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017902352 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | -0.0968     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.0137      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.36e+03 |\n",
      "|    ep_rew_mean     | 1.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 12846    |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=19.31 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 19.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009792915 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0419      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.327       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=19.31 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 19.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.36e+03 |\n",
      "|    ep_rew_mean     | 1.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 13755    |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=18.79 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 18.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 142000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015413039 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.36        |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00787     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=18.79 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 18.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.36e+03 |\n",
      "|    ep_rew_mean     | 1.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 14676    |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=20.57 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015443353 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0286     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.001       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=20.57 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 20.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.36e+03 |\n",
      "|    ep_rew_mean     | 1.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 15609    |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 146000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009240815 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.000678    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.36e+03 |\n",
      "|    ep_rew_mean     | 1.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 15659    |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2           |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008226169 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.000775    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-0.90 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2        |\n",
      "|    mean_reward     | -0.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.46e+03 |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 15686    |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=19.23 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+04        |\n",
      "|    mean_reward          | 19.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041715587 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.211        |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00695     |\n",
      "|    value_loss           | 0.182        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=19.23 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 19.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.46e+03 |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 16596    |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=18.58 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 18.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002647069 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    value_loss           | 0.00232     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=18.58 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 18.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.46e+03 |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 8        |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 17529    |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=21.27 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+04       |\n",
      "|    mean_reward          | 21.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 154000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016547836 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=155000, episode_reward=21.27 +/- 0.00\n",
      "Episode length: 10000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+04    |\n",
      "|    mean_reward     | 21.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.46e+03 |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 8        |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 18468    |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | -0.867      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012441408 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.000588    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-0.87 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1        |\n",
      "|    mean_reward     | -0.867   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.46e+03 |\n",
      "|    ep_rew_mean     | 1.33     |\n",
      "| time/              |          |\n",
      "|    fps             | 8        |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 18520    |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl-models-sde2/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                              n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      3\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rl-logs/\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m,\n\u001b[1;32m      4\u001b[0m                              deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                              )\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: Monitor(env)]), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./rl-logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m             device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;66;03m# n_steps=128,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m# learning_rate=0.0015,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m# batch_size=16,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:201\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    200\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:460\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 460\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/miniconda3/envs/rlenv/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_env.py:41\u001b[0m, in \u001b[0;36mCellEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m     res_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     t, tot, N, R, cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_population\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     res_fraction \u001b[38;5;241m=\u001b[39m R \u001b[38;5;241m/\u001b[39m tot\n\u001b[1;32m     43\u001b[0m     tot \u001b[38;5;241m=\u001b[39m tot[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# most recent cell count in entire simulation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:186\u001b[0m, in \u001b[0;36mCell_Population.simulate_population\u001b[0;34m(self, b, delta_t, plot)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_stop \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimulation call will exceed total time allotted and thus can not be accepted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m t, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFDE_PI12_PC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_stop\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# solving for values at next timestep using fractional integration\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# upacking\u001b[39;00m\n\u001b[1;32m    189\u001b[0m N \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:219\u001b[0m, in \u001b[0;36mCell_Population.FDE_PI12_PC\u001b[0;34m(self, T_stop)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFDE_PI12_PC\u001b[39m(\u001b[38;5;28mself\u001b[39m, T_stop):\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Main process of computation by means of the FFT algorithm\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil((T_stop \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh))\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTriangolo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzn_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzn_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProbl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeg \u001b[38;5;241m=\u001b[39m N\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Evaluation solution in T when T is not in the mesh\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Github/iaifi24-hackathon/cell_model_pop_fde_slow_sde.py:245\u001b[0m, in \u001b[0;36mCell_Population.Triangolo\u001b[0;34m(self, nxi, nxf, t, y, fy, zn_pred, zn_corr, N, METH, Probl)\u001b[0m\n\u001b[1;32m    242\u001b[0m j_beg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j_beg, n):\n\u001b[0;32m--> 245\u001b[0m     Phi \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m METH[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m][:Probl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_length\u001b[39m\u001b[38;5;124m'\u001b[39m], n \u001b[38;5;241m-\u001b[39m j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m fy[:, j]\n\u001b[1;32m    247\u001b[0m St \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mStartingTerm(t[n], Probl[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# if no corrector application, add noise now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models-sde2/',\n",
    "                             n_eval_episodes=3,\n",
    "                             log_path='./rl-logs/', eval_freq=1_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = PPO(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            device=\"cpu\",\n",
    "            # n_steps=128,\n",
    "            # learning_rate=0.0015,\n",
    "            # batch_size=16,\n",
    ")\n",
    "model.learn(total_timesteps=5_000_000, tb_log_name=\"ppo\",\n",
    "            callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import evaluate_model\n",
    "env_args = {\n",
    "    'dt': 0.01,\n",
    "    'alpha_mem': 0.7,\n",
    "    'max_timesteps': 10000,\n",
    "    'frame_stack': 20\n",
    "}    \n",
    "obs, act, frac = evaluate_model(env_args, num_episodes=1, model_str='./rl-models-sde2/best_model.zip', multiprocess=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cell_env.CellEnv at 0x7f811cba3b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CellEnv(**env_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f79db2a3790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t = np.arange(0, obs.shape[1]*env_args['dt'], env_args['dt'])\n",
    "plt.plot(t, obs.mean(axis=0), label='PPO')\n",
    "# import from data npy:\n",
    "t, tots  = np.load('data.npy')\n",
    "plt.plot(t, tots, label='hardcoded')\n",
    "plt.grid()\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(700,1000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f79db3dfdf0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(frac.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
