{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cell_env import CellEnv\n",
    "# Use sb3 env checker:\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    'dt': 0.1,\n",
    "    'alpha_mem': 0.7,\n",
    "    'max_timesteps': 1000\n",
    "}    \n",
    "\n",
    "env = CellEnv(**env_args)\n",
    "\n",
    "# use the monitor wrapper to log the results:\n",
    "env = Monitor(env)\n",
    "eval_env = CellEnv(**env_args)\n",
    "eval_env = Monitor(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./rl-logs/ppo_7\n",
      "Eval num_timesteps=1000, episode_reward=-11311.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.13e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1000      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-11311.72 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.13e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2000      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -361     |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-6612.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -6.61e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008719908 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0592     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.37        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    value_loss           | 50.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-6612.63 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -6.61e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 4000      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-38107.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -3.81e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018376071 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 24.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-38107.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -3.81e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 6000      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12143.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -1.21e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011051403 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-12143.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -1.21e+04 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 8000      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -79.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-4872.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -4.87e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001169075 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000802   |\n",
      "|    value_loss           | 193         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-4872.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -4.87e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 10000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -45      |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-13273.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -1.33e+04   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480287 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 85          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00821    |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models-sde/',\n",
    "                             n_eval_episodes=3,\n",
    "                             log_path='./rl-logs/', eval_freq=1_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = PPO(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            device=\"cpu\",\n",
    "            # n_steps=128,\n",
    "            # learning_rate=0.0015,\n",
    "            # batch_size=16,\n",
    ")\n",
    "model.learn(total_timesteps=100_000, tb_log_name=\"ppo\",\n",
    "            callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./rl-models-sde/',\n",
    "                             n_eval_episodes=3,\n",
    "                             log_path='./rl-logs/', eval_freq=1_000,\n",
    "                             deterministic=True, render=False,\n",
    "                             )\n",
    "\n",
    "model = PPO(\"MlpPolicy\", DummyVecEnv([lambda: Monitor(env)]), verbose=4, tensorboard_log=\"./rl-logs/\",\n",
    "            device=\"cpu\",\n",
    "            # n_steps=128,\n",
    "            # learning_rate=0.0015,\n",
    "            # batch_size=16,\n",
    ")\n",
    "model.learn(total_timesteps=5_000_000, tb_log_name=\"ppo\",\n",
    "            callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
